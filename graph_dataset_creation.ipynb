{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ykffzr4H-YMP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 18:41:53.642812: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-09 18:41:54.557076: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-09 18:41:54.916084: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-09 18:41:55.532807: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-09 18:41:55.536346: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-09 18:41:56.507282: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-09 18:42:00.123761: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9cqGPenz1TVh"
   },
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "file_path = r\"/workspaces/rumour-detection-pheme/Dataset creation/charliehebdo-all-rnr-threads.csv\"\n",
    "df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3bAzR4p52Rf8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rumour\n",
       "0    1555\n",
       "1     447\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('rumour')['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nq86ae2_7l0H"
   },
   "outputs": [],
   "source": [
    "df.time = pd.to_datetime(df.time, format='%a %b %d %H:%M:%S +0000 %Y')\n",
    "df.reply_time = pd.to_datetime(df.reply_time, format='%a %b %d %H:%M:%S +0000 %Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kiU1-cVGPzC"
   },
   "source": [
    "### Time for replies and Number of replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "B0y9FosJ8eU2"
   },
   "outputs": [],
   "source": [
    " df['time_diff']=(df.reply_time - df.time).dt.total_seconds()/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jVIvMkty9eSs"
   },
   "outputs": [],
   "source": [
    "df['reply_number'] = df.groupby('id')['time_diff'].rank(method='dense')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDFus9zKLhBg"
   },
   "source": [
    "#### Number of replies x Retweet counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AKxtFOLASzUP"
   },
   "outputs": [],
   "source": [
    "df_posts = df[['id','text','followers','favorite_count','retweet_count','verified',\\\n",
    "  'rumour','user_id']].drop_duplicates().merge(df.groupby(['id']).agg(replies=(\\\n",
    "  'time_diff','count'),first_time_diff=('time_diff','first')).reset_index(),\\\n",
    "  on=\"id\",how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH68TYoeMgYm"
   },
   "source": [
    "#### Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-wXsNYeNuLs",
    "outputId": "540fcdf9-482d-4e1e-ccc5-83d87c85df67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# function for cleaning data\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt\n",
    "\n",
    "\n",
    "def clean_text(\n",
    "    string: str,\n",
    "    punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "    stop_words=english_stopwords) -> str:\n",
    "    \"\"\"\n",
    "    A method to clean text\n",
    "    \"\"\"\n",
    "    # Cleaning the urls\n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "\n",
    "    # Cleaning the html elements\n",
    "    string = re.sub(r'<.*?>', '', string)\n",
    "\n",
    "    # Removing the punctuations\n",
    "    for x in string.lower():\n",
    "        if x in punctuations:\n",
    "            string = string.replace(x, \"\")\n",
    "\n",
    "    # Converting the text to lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Removing stop words\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IrvGTGi9M2K2"
   },
   "outputs": [],
   "source": [
    "df_posts['clean_text'] = np.vectorize(remove_pattern)(df_posts['text'], \"@[\\w]*\")\n",
    "df_posts['clean_text'] = df_posts['clean_text'].str.replace(\"[^a-zA-Z#]\", \" \").apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DUqjXD9nZyhH"
   },
   "outputs": [],
   "source": [
    "# create the dictionary\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_posts['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HnpNJdMFZ7OW"
   },
   "outputs": [],
   "source": [
    "def embedding_vocab(filepath, word_index,embedding_dim):\n",
    "    vocab_size = len(word_index) + 1\n",
    "\n",
    "\n",
    "    embedding_matrix_vocab = np.zeros((vocab_size,\n",
    "                                       embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                embedding_matrix_vocab[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "    return embedding_matrix_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUF6Y9hCcv1M",
    "outputId": "99084bf5-0137-44a0-b8b0-7584a21d87c2"
   },
   "outputs": [],
   "source": [
    "#!wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
    "#!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "gaWny-EfcIcN"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix_vocab = embedding_vocab(\n",
    "    'glove.6B.100d.txt', tokenizer.word_index,\n",
    "embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "sequences = tokenizer.texts_to_sequences(df_posts['clean_text'])\n",
    "\n",
    "# Padding sequences if necessary\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Function to get embeddings for a sequence\n",
    "def get_embeddings(sequence, embedding_matrix):\n",
    "    embeddings = []\n",
    "    for idx in sequence:\n",
    "        embeddings.append(embedding_matrix[idx])\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Apply function to get embeddings for each sequence\n",
    "df_posts['embeddings'] = [get_embeddings(seq, embedding_matrix_vocab) for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "pycnHmE3zX_o"
   },
   "outputs": [],
   "source": [
    "array_avg = []\n",
    "for i in df_posts.embeddings:\n",
    "  array_avg.append(np.mean(i,axis=0))\n",
    "df_posts['embeddings_avg'] = array_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reply embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_reply_text'] = np.vectorize(remove_pattern)(df['reply_text'], \"@[\\w]*\")\n",
    "df['clean_reply_text'] = df['clean_reply_text'].str.replace(\"[^a-zA-Z#]\", \" \").apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dictionary\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['clean_reply_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix_vocab = embedding_vocab(\n",
    "    'glove.6B.100d.txt', tokenizer.word_index,\n",
    "embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "sequences = tokenizer.texts_to_sequences(df['clean_reply_text'])\n",
    "\n",
    "# Padding sequences if necessary\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Apply function to get embeddings for each sequence\n",
    "df['reply_embeddings'] = [get_embeddings(seq, embedding_matrix_vocab) for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/codespace/.local/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "array_avg = []\n",
    "for i in df.reply_embeddings:\n",
    "  array_avg.append(np.mean(i,axis=0))\n",
    "df['reply_embeddings_avg'] = array_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['reply_embeddings_avg'] = df['reply_embeddings_avg'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving cleaned csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('replies_charlie_hebdo.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.to_pickle(\"posts_charlie_hebdo.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44ZNwTqtKovw"
   },
   "source": [
    "#### Creat Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_replies=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_replies = pd.read_csv(\"replies_charlie_hebdo.csv\")\n",
    "#df_posts = pd.read_csv(\"posts_charlie_hebdo.csv\")\n",
    "#df_replies = df_replies[df_replies.reply_verified.isnull()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_post_not_rumour = df_posts[df_posts.rumour==0]\n",
    "#total_samples = int(df_posts[df_posts.rumour==1].count()[0]*1.1)\n",
    "\n",
    "# Get the proportion of each value in the 'replies' column\n",
    "#value_counts = df_post_not_rumour['replies'].value_counts(normalize=True)\n",
    "\n",
    "# Calculate the number of samples for each value\n",
    "#samples_per_value = (value_counts * total_samples).round().astype(int)\n",
    "\n",
    "# Sample the required number of rows for each value in 'replies'\n",
    "#sampled_dfs = []\n",
    "#for value, num_samples in samples_per_value.items():\n",
    "#    sampled_dfs.append(df_post_not_rumour[df_post_not_rumour['replies'] \\\n",
    "#                       == value].sample(num_samples, random_state=42))\n",
    "\n",
    "# Concatenate all the samples into a single dataframe\n",
    "#sampled_df = pd.concat(sampled_dfs)\n",
    "\n",
    "# Shuffle the resulting dataframe\n",
    "#sampled_df = sampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "#df_posts = pd.concat([sampled_df,df_posts[df_posts.rumour==1]])\\\n",
    "#             .sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "\n",
    "#df_replies = df_replies[df_replies.id.isin(df_posts.id.values)][['id','reply_id',\\\n",
    "#             'reply_followers','reply_user_id','reply_verified',\"rumour\",\\\n",
    "#              \"user_id\",\"reply_embeddings_avg\",\"time_diff\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yY1jnj9m3ElI"
   },
   "source": [
    "#### Torch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "xY9kjy-URKKx"
   },
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "df_posts['verified'] = df_posts['verified'].astype('str').str.\\\n",
    "             replace(' ', '').replace('True', '1').replace('False', '0')\\\n",
    "             .astype('int64')\n",
    "\n",
    "df_posts = pd.concat([df_posts, pd.get_dummies(\\\n",
    "                          df_posts[\"verified\"],dtype=int)], axis=1, join='inner')\n",
    "df_posts.drop([\"verified\"], axis=1, inplace=True)\n",
    "df_posts.rename(columns={1:'verified',0:'no_verified'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "fDw9YB9qRKB_"
   },
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "df_replies['reply_verified'] = df_replies['reply_verified'].astype('str').str.\\\n",
    "             replace(' ', '').replace('True', '1').replace('False', '0')\\\n",
    "             .astype('int64')\n",
    "\n",
    "df_replies = pd.concat([df_replies, pd.get_dummies(\\\n",
    "                          df_replies[\"reply_verified\"],dtype=int)], axis=1, join='inner')\n",
    "df_replies.drop([\"reply_verified\"], axis=1, inplace=True)\n",
    "df_replies.rename(columns={1:'reply_verified',0:'reply_no_verified'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Wa1NAPktPoB8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get unique values from the column\n",
    "unique_values = df_posts['user_id'].unique()\n",
    "\n",
    "# Create a dictionary to map values to IDs\n",
    "value_to_id = {value: i for i, value in enumerate(unique_values)}\n",
    "\n",
    "# Map the values to IDs using the dictionary\n",
    "#retweets_node_features['index'] = retweets_node_features['reply_user_id'].map(value_to_id)\n",
    "post_map  = value_to_id\n",
    "\n",
    "#Only keep features\n",
    "post_features = df_posts[[\"followers\", \"favorite_count\",\"retweet_count\",\"no_verified\",\"verified\",\\\n",
    "                          \"first_time_diff\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "i3VSO_R8xKgy"
   },
   "outputs": [],
   "source": [
    "embeddings_avg = []\n",
    "for embeddings in df_posts['embeddings_avg']:\n",
    "  #embeddings= i.replace('[','').replace(']','').replace('\\n','').split(' ')\n",
    "  #embeddings =  [float(item) for item in embeddings if item != '']\n",
    "  embeddings_avg.append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize the Robust Scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Assuming data is a DataFrame containing your dataset\n",
    "scaled_features = scaler.fit_transform(post_features[['followers', 'favorite_count', 'retweet_count', 'first_time_diff']])\n",
    "\n",
    "# Convert the scaled features back to a DataFrame\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=['followers', 'favorite_count', 'retweet_count', 'first_time_diff'])\n",
    "\n",
    "# Add the binary features back to the scaled data\n",
    "scaled_data['no_verified'] = post_features['no_verified']\n",
    "scaled_data['verified'] = post_features['verified']\n",
    "post_features = scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QnaMBT0yylBn",
    "outputId": "31b11cf1-5402-4ca1-c567-f1c5abfeea86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2002, 106)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to numpy\n",
    "x1 = np.concatenate((post_features.to_numpy(),np.array(embeddings_avg)),axis=1)\n",
    "#x1 = post_features.to_numpy()\n",
    "x1.shape # [num_movie_nodes x movie_node_feature_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "dNbQMYlKzCUi"
   },
   "outputs": [],
   "source": [
    "# Select node features\n",
    "retweets_node_features = df_replies[[\"reply_followers\", \"reply_no_verified\", \"reply_verified\",\"reply_user_id\",\"time_diff\"]]\n",
    "\n",
    "# Get unique values from the column\n",
    "unique_values = retweets_node_features['reply_user_id'].unique()\n",
    "\n",
    "# Create a dictionary to map values to IDs\n",
    "value_to_id = {value: i for i, value in enumerate(unique_values)}\n",
    "\n",
    "# Map the values to IDs using the dictionary\n",
    "#retweets_node_features['index'] = retweets_node_features['reply_user_id'].map(value_to_id)\n",
    "retweets_id_mapping  = value_to_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Only keep features\n",
    "retweets_node_features = retweets_node_features[[\"reply_followers\", \"reply_no_verified\", \"reply_verified\",\"time_diff\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_avg = []\n",
    "for embeddings in df_replies['reply_embeddings_avg']:\n",
    "  #embeddings= i.replace('[','').replace(']','').replace('\\n','').split(' ')\n",
    "  #embeddings =  [float(item) for item in embeddings if item != '']\n",
    "  embeddings_avg.append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Robust Scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Assuming data is a DataFrame containing your dataset\n",
    "scaled_features = scaler.fit_transform(retweets_node_features[['reply_followers','time_diff']])\n",
    "\n",
    "# Convert the scaled features back to a DataFrame\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=['reply_followers','time_diff'])\n",
    "\n",
    "# Add the binary features back to the scaled data\n",
    "scaled_data['reply_no_verified'] = retweets_node_features['reply_no_verified']\n",
    "scaled_data['reply_verified'] = retweets_node_features['reply_verified']\n",
    "retweets_node_features = scaled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19050, 4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to numpy\n",
    "#x2 = np.concatenate((retweets_node_features.to_numpy(),np.array(embeddings_avg)),axis=1)\n",
    "x2 = retweets_node_features.to_numpy()\n",
    "x2.shape # [num_movie_nodes x movie_node_feature_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYMUpme80rwi",
    "outputId": "fad85d16-324a-47ee-a7e4-a2afd472dffa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2002,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract labels\n",
    "labels = df_posts.rumour\n",
    "y = labels.to_numpy()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "rsb7ZYVI04R6"
   },
   "outputs": [],
   "source": [
    "# Map post IDs\n",
    "#post_map = posts_id_mapping.reset_index().set_index(\"id\").to_dict()\n",
    "df_replies[\"user_id\"] = df_replies['user_id'].map(post_map).astype(int)\n",
    "# Map user IDs\n",
    "#retweets_map = retweets_id_mapping #retweets_id_mapping.reset_index().set_index(\"reply_user_id\").to_dict()\n",
    "df_replies[\"reply_id\"] = df_replies[\"reply_user_id\"].map(retweets_id_mapping)#.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zov-Q9429v1",
    "outputId": "a1a9b871-c403-434b-e5b3-45e4feea6544"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,    25,    25,    25],\n",
       "       [    0,     1,     2, ...,   284,  1810, 14464]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = df_replies[[\"user_id\", \"reply_id\"]].values.transpose()\n",
    "edge_index # [2 x num_edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "I8VTjazz6G7U"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Number of rows\n",
    "num_rows = x1.shape[0]\n",
    "\n",
    "# Desired proportions\n",
    "train_proportion = 0.70\n",
    "val_proportion = 0.15\n",
    "test_proportion = 0.15\n",
    "\n",
    "# Generate a list of indices and shuffle them\n",
    "indices = np.arange(num_rows)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Calculate the split indices\n",
    "train_end = int(train_proportion * num_rows)\n",
    "val_end = train_end + int(val_proportion * num_rows)\n",
    "\n",
    "# Split the indices into train, validation, and test\n",
    "train_indices = indices[:train_end]\n",
    "val_indices = indices[train_end:val_end]\n",
    "test_indices = indices[val_end:]\n",
    "\n",
    "# Create masks\n",
    "train_mask = np.zeros(num_rows, dtype=bool)\n",
    "val_mask = np.zeros(num_rows, dtype=bool)\n",
    "test_mask = np.zeros(num_rows, dtype=bool)\n",
    "\n",
    "# Assign True to the corresponding indices\n",
    "train_mask[train_indices] = True\n",
    "val_mask[val_indices] = True\n",
    "test_mask[test_indices] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XyvzeR7IymQ",
    "outputId": "b26eb871-555d-42fd-caff-ad906a7ccf7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  user_id={\n",
      "    x=[2002, 106],\n",
      "    y=[2002],\n",
      "    train_mask=[2002],\n",
      "    val_mask=[2002],\n",
      "    test_mask=[2002],\n",
      "  },\n",
      "  reply_id={ x=[19050, 4] },\n",
      "  (user_id, retweet, reply_id)={ edge_index=[2, 19050] },\n",
      "  (reply_id, rev_retweet, user_id)={ edge_index=[2, 19050] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the HeteroData object\n",
    "data = HeteroData()\n",
    "\n",
    "# Add node features and labels for the 'paper' node type\n",
    "data['user_id'].x = torch.tensor(x1,dtype=torch.float32)  # Node features dtype=torch.float32\n",
    "data['user_id'].y = torch.from_numpy(y)  # Node labels (for example, binary classification)\n",
    "data['user_id'].train_mask = torch.tensor(train_mask)  # Training mask\n",
    "data['user_id'].val_mask =torch.tensor(val_mask)  # Validation mask\n",
    "data['user_id'].test_mask = torch.tensor(test_mask)  # Test mask\n",
    "\n",
    "# Add node features for the 'author' node type\n",
    "data['reply_id'].x = torch.tensor(x2,dtype=torch.float32) #torch.float32\n",
    "\n",
    "# Add edges for the (id, retweet, reply_D) relation\n",
    "data['user_id', 'retweet', 'reply_id'].edge_index = torch.from_numpy(edge_index.reshape(2,len(x2)))\n",
    "data = T.ToUndirected()(data)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtOzUtI7frmN",
    "outputId": "cedbbb5a-d3a3-46b2-adea-f6ed97e206cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: tensor(306) torch.Size([1401])\n"
     ]
    }
   ],
   "source": [
    "print(\"train:\",data['user_id'].y[train_mask].sum(),data['user_id'].y[train_mask].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0hD33th-gFnO",
    "outputId": "0e0f7739-7d7e-49dd-cf0f-8c8221739a32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: tensor(66) torch.Size([300])\n"
     ]
    }
   ],
   "source": [
    "print(\"val:\", data['user_id'].y[val_mask].sum(),data['user_id'].y[val_mask].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5_aX0ZogGHn",
    "outputId": "5f4934da-8242-429e-9a90-fa8f256d2b2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: tensor(75) torch.Size([301])\n"
     ]
    }
   ],
   "source": [
    "print(\"test:\", data['user_id'].y[test_mask].sum(),data['user_id'].y[test_mask].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('charlie_hebdo_graph_dataset_node_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
