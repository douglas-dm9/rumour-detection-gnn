{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ykffzr4H-YMP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import pickle\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9cqGPenz1TVh"
   },
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "file_path = r\"/home/azureuser/rumour-detection-pheme/charliehebdo-all-rnr-threads.csv\"\n",
    "df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nq86ae2_7l0H"
   },
   "outputs": [],
   "source": [
    "df.time = pd.to_datetime(df.time, format='%a %b %d %H:%M:%S +0000 %Y')\n",
    "df.reply_time = pd.to_datetime(df.reply_time, format='%a %b %d %H:%M:%S +0000 %Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kiU1-cVGPzC"
   },
   "source": [
    "### Time for replies and Number of replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "B0y9FosJ8eU2"
   },
   "outputs": [],
   "source": [
    " df['time_diff']=(df.reply_time - df.time).dt.total_seconds()/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jVIvMkty9eSs"
   },
   "outputs": [],
   "source": [
    "df['reply_number'] = df.groupby('id')['time_diff'].rank(method='dense')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDFus9zKLhBg"
   },
   "source": [
    "#### Number of replies x Retweet counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AKxtFOLASzUP"
   },
   "outputs": [],
   "source": [
    "df_posts = df[['id','text','followers','favorite_count','retweet_count','verified',\\\n",
    "  'rumour','user_id']].drop_duplicates().merge(df.groupby(['id']).agg(replies=(\\\n",
    "  'time_diff','count'),first_time_diff=('time_diff','first')).reset_index(),\\\n",
    "  on=\"id\",how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH68TYoeMgYm"
   },
   "source": [
    "#### Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-wXsNYeNuLs",
    "outputId": "540fcdf9-482d-4e1e-ccc5-83d87c85df67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# function for cleaning data\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt\n",
    "\n",
    "\n",
    "def clean_text(\n",
    "    string: str,\n",
    "    punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "    stop_words=english_stopwords) -> str:\n",
    "    \"\"\"\n",
    "    A method to clean text\n",
    "    \"\"\"\n",
    "    # Cleaning the urls\n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "\n",
    "    # Cleaning the html elements\n",
    "    string = re.sub(r'<.*?>', '', string)\n",
    "\n",
    "    # Removing the punctuations\n",
    "    for x in string.lower():\n",
    "        if x in punctuations:\n",
    "            string = string.replace(x, \"\")\n",
    "\n",
    "    # Converting the text to lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Removing stop words\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IrvGTGi9M2K2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/tmp/ipykernel_50599/1199893929.py:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  df_posts['clean_text'] = np.vectorize(remove_pattern)(df_posts['text'], \"@[\\w]*\")\n"
     ]
    }
   ],
   "source": [
    "df_posts['clean_text'] = np.vectorize(remove_pattern)(df_posts['text'], \"@[\\w]*\")\n",
    "df_posts['clean_text'] = df_posts['clean_text'].str.replace(\"[^a-zA-Z#]\", \" \").apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HnpNJdMFZ7OW"
   },
   "outputs": [],
   "source": [
    "def embedding_vocab(filepath, word_index,embedding_dim):\n",
    "    vocab_size = len(word_index) + 1\n",
    "\n",
    "\n",
    "    embedding_matrix_vocab = np.zeros((vocab_size,\n",
    "                                       embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                embedding_matrix_vocab[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "    return embedding_matrix_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "#from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df_posts['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUF6Y9hCcv1M",
    "outputId": "99084bf5-0137-44a0-b8b0-7584a21d87c2"
   },
   "outputs": [],
   "source": [
    "#import zipfile\n",
    "\n",
    "#with zipfile.ZipFile('glove.6B.zip', 'r') as zip_ref:\n",
    "#    zip_ref.extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gaWny-EfcIcN"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix_vocab = embedding_vocab(\n",
    "    'glove.6B.100d.txt', tokenizer.word_index,\n",
    "embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "# create the dictionary\n",
    "sequences = tokenizer.texts_to_sequences(df_posts['clean_text'])\n",
    "\n",
    "# Padding sequences if necessary\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Function to get embeddings for a sequence\n",
    "def get_embeddings(sequence, embedding_matrix):\n",
    "    embeddings = []\n",
    "    for idx in sequence:\n",
    "        embeddings.append(embedding_matrix[idx])\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Apply function to get embeddings for each sequence\n",
    "df_posts['embeddings'] = [get_embeddings(seq, embedding_matrix_vocab) for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "pycnHmE3zX_o"
   },
   "outputs": [],
   "source": [
    "array_avg = []\n",
    "for i in df_posts.embeddings:\n",
    "  array_avg.append(np.mean(i,axis=0))\n",
    "df_posts['embeddings_avg'] = array_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reply embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/tmp/ipykernel_50599/2579928606.py:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  df['clean_reply_text'] = np.vectorize(remove_pattern)(df['reply_text'], \"@[\\w]*\")\n"
     ]
    }
   ],
   "source": [
    "df['clean_reply_text'] = np.vectorize(remove_pattern)(df['reply_text'], \"@[\\w]*\")\n",
    "df['clean_reply_text'] = df['clean_reply_text'].str.replace(\"[^a-zA-Z#]\", \" \").apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dictionary\n",
    "tokenizer =  keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df['clean_reply_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix_vocab = embedding_vocab(\n",
    "    'glove.6B.100d.txt', tokenizer.word_index,\n",
    "embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "sequences = tokenizer.texts_to_sequences(df['clean_reply_text'])\n",
    "\n",
    "# Padding sequences if necessary\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Apply function to get embeddings for each sequence\n",
    "df['reply_embeddings'] = [get_embeddings(seq, embedding_matrix_vocab) for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/miniconda3/envs/mastering/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/azureuser/miniconda3/envs/mastering/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "array_avg = []\n",
    "for i in df.reply_embeddings:\n",
    "  array_avg.append(np.mean(i,axis=0))\n",
    "df['reply_embeddings_avg'] = array_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['reply_embeddings_avg'] = df['reply_embeddings_avg'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving cleaned csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>location</th>\n",
       "      <th>followers</th>\n",
       "      <th>user_id</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>verified</th>\n",
       "      <th>rumour</th>\n",
       "      <th>...</th>\n",
       "      <th>reply_time</th>\n",
       "      <th>reply_location</th>\n",
       "      <th>reply_followers</th>\n",
       "      <th>reply_user_id</th>\n",
       "      <th>reply_verified</th>\n",
       "      <th>time_diff</th>\n",
       "      <th>reply_number</th>\n",
       "      <th>clean_reply_text</th>\n",
       "      <th>reply_embeddings</th>\n",
       "      <th>reply_embeddings_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>552783238415265792</td>\n",
       "      <td>Breaking: At least 10 dead, 5 injured after tO...</td>\n",
       "      <td>2015-01-07 11:06:08</td>\n",
       "      <td>Paris</td>\n",
       "      <td>1628</td>\n",
       "      <td>384779793</td>\n",
       "      <td>14</td>\n",
       "      <td>159</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2015-01-07 11:24:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>202572421</td>\n",
       "      <td>False</td>\n",
       "      <td>18.116667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>religion peace strikes</td>\n",
       "      <td>[[0.38767001032829285, 0.5266600251197815, 0.3...</td>\n",
       "      <td>[-0.1889099975426992, 0.3738733381032944, -0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>552783238415265792</td>\n",
       "      <td>Breaking: At least 10 dead, 5 injured after tO...</td>\n",
       "      <td>2015-01-07 11:06:08</td>\n",
       "      <td>Paris</td>\n",
       "      <td>1628</td>\n",
       "      <td>384779793</td>\n",
       "      <td>14</td>\n",
       "      <td>159</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2015-01-07 11:31:37</td>\n",
       "      <td>Zimbabwe-London</td>\n",
       "      <td>375</td>\n",
       "      <td>239943362</td>\n",
       "      <td>False</td>\n",
       "      <td>25.483333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>hi henry would willing give itv news phone int...</td>\n",
       "      <td>[[0.1444000005722046, 0.23978999257087708, 0.9...</td>\n",
       "      <td>[-0.12313784658908844, -0.18030815256329683, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>552783238415265792</td>\n",
       "      <td>Breaking: At least 10 dead, 5 injured after tO...</td>\n",
       "      <td>2015-01-07 11:06:08</td>\n",
       "      <td>Paris</td>\n",
       "      <td>1628</td>\n",
       "      <td>384779793</td>\n",
       "      <td>14</td>\n",
       "      <td>159</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2015-01-07 11:38:37</td>\n",
       "      <td>delhi</td>\n",
       "      <td>17</td>\n",
       "      <td>2903715212</td>\n",
       "      <td>False</td>\n",
       "      <td>32.483333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>please call terrorists gunmen dont dilute news...</td>\n",
       "      <td>[[-0.9112600088119507, 0.3958500027656555, 1.2...</td>\n",
       "      <td>[-0.19128870833665132, 0.15544349609408528, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>552783238415265792</td>\n",
       "      <td>Breaking: At least 10 dead, 5 injured after tO...</td>\n",
       "      <td>2015-01-07 11:06:08</td>\n",
       "      <td>Paris</td>\n",
       "      <td>1628</td>\n",
       "      <td>384779793</td>\n",
       "      <td>14</td>\n",
       "      <td>159</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2015-01-07 11:45:32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54</td>\n",
       "      <td>2911191121</td>\n",
       "      <td>False</td>\n",
       "      <td>39.400000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>french govt needs take strict action</td>\n",
       "      <td>[[0.02704799920320511, -0.0538330003619194, 0....</td>\n",
       "      <td>[-0.06885033225019772, 0.2138711006846279, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>552783238415265792</td>\n",
       "      <td>Breaking: At least 10 dead, 5 injured after tO...</td>\n",
       "      <td>2015-01-07 11:06:08</td>\n",
       "      <td>Paris</td>\n",
       "      <td>1628</td>\n",
       "      <td>384779793</td>\n",
       "      <td>14</td>\n",
       "      <td>159</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2015-01-07 12:32:50</td>\n",
       "      <td>Shredsville</td>\n",
       "      <td>683</td>\n",
       "      <td>1348798826</td>\n",
       "      <td>False</td>\n",
       "      <td>86.700000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>people didnt hand guns jeffrey epstein also cf...</td>\n",
       "      <td>[[0.2901900112628937, 0.8049700260162354, 0.31...</td>\n",
       "      <td>[0.014819002151489258, 0.014486800134181976, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19045</th>\n",
       "      <td>553592195786506240</td>\n",
       "      <td>#BREAKING - Both #CharlieHebdo suspects killed...</td>\n",
       "      <td>2015-01-09 16:40:39</td>\n",
       "      <td>Paris, France</td>\n",
       "      <td>1261930</td>\n",
       "      <td>1994321</td>\n",
       "      <td>41</td>\n",
       "      <td>133</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2015-01-09 16:42:42</td>\n",
       "      <td>Montreal, Canada</td>\n",
       "      <td>14085</td>\n",
       "      <td>7207042</td>\n",
       "      <td>False</td>\n",
       "      <td>2.050000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>rt breaking charliehebdo suspects killed polic...</td>\n",
       "      <td>[[-0.34817999601364136, -0.10100000351667404, ...</td>\n",
       "      <td>[-0.04054700657725334, -0.028997698239982127, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19046</th>\n",
       "      <td>553592195786506240</td>\n",
       "      <td>#BREAKING - Both #CharlieHebdo suspects killed...</td>\n",
       "      <td>2015-01-09 16:40:39</td>\n",
       "      <td>Paris, France</td>\n",
       "      <td>1261930</td>\n",
       "      <td>1994321</td>\n",
       "      <td>41</td>\n",
       "      <td>133</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2015-01-09 16:42:54</td>\n",
       "      <td>قبرص(اليونان)</td>\n",
       "      <td>189</td>\n",
       "      <td>2814170438</td>\n",
       "      <td>False</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>coming</td>\n",
       "      <td>[[0.0667089968919754, 0.2744799852371216, 0.81...</td>\n",
       "      <td>[0.0667089968919754, 0.2744799852371216, 0.814...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19047</th>\n",
       "      <td>553592195786506240</td>\n",
       "      <td>#BREAKING - Both #CharlieHebdo suspects killed...</td>\n",
       "      <td>2015-01-09 16:40:39</td>\n",
       "      <td>Paris, France</td>\n",
       "      <td>1261930</td>\n",
       "      <td>1994321</td>\n",
       "      <td>41</td>\n",
       "      <td>133</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2015-01-09 16:43:11</td>\n",
       "      <td>Planet Earth</td>\n",
       "      <td>281</td>\n",
       "      <td>16573662</td>\n",
       "      <td>False</td>\n",
       "      <td>2.533333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>happy hear two terrorists gone doom praise inv...</td>\n",
       "      <td>[[-0.09043599665164948, 0.1963600069284439, 0....</td>\n",
       "      <td>[-0.10175399528816342, 0.3584062494337559, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19048</th>\n",
       "      <td>553592195786506240</td>\n",
       "      <td>#BREAKING - Both #CharlieHebdo suspects killed...</td>\n",
       "      <td>2015-01-09 16:40:39</td>\n",
       "      <td>Paris, France</td>\n",
       "      <td>1261930</td>\n",
       "      <td>1994321</td>\n",
       "      <td>41</td>\n",
       "      <td>133</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2015-01-09 16:46:50</td>\n",
       "      <td>UK</td>\n",
       "      <td>793</td>\n",
       "      <td>1067999394</td>\n",
       "      <td>False</td>\n",
       "      <td>6.183333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>well done french ending chapter terror terrori...</td>\n",
       "      <td>[[-0.5308600068092346, 0.5140399932861328, 0.0...</td>\n",
       "      <td>[0.047132634981112045, 0.16482700068842282, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19049</th>\n",
       "      <td>553592195786506240</td>\n",
       "      <td>#BREAKING - Both #CharlieHebdo suspects killed...</td>\n",
       "      <td>2015-01-09 16:40:39</td>\n",
       "      <td>Paris, France</td>\n",
       "      <td>1261930</td>\n",
       "      <td>1994321</td>\n",
       "      <td>41</td>\n",
       "      <td>133</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2015-01-09 16:58:09</td>\n",
       "      <td>Sunny California</td>\n",
       "      <td>2236</td>\n",
       "      <td>37651316</td>\n",
       "      <td>False</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>good swift justice</td>\n",
       "      <td>[[-0.030768999829888344, 0.11992999911308289, ...</td>\n",
       "      <td>[0.10588399755458038, -0.15457333127657572, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19050 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                               text  \\\n",
       "0      552783238415265792  Breaking: At least 10 dead, 5 injured after tO...   \n",
       "1      552783238415265792  Breaking: At least 10 dead, 5 injured after tO...   \n",
       "2      552783238415265792  Breaking: At least 10 dead, 5 injured after tO...   \n",
       "3      552783238415265792  Breaking: At least 10 dead, 5 injured after tO...   \n",
       "4      552783238415265792  Breaking: At least 10 dead, 5 injured after tO...   \n",
       "...                   ...                                                ...   \n",
       "19045  553592195786506240  #BREAKING - Both #CharlieHebdo suspects killed...   \n",
       "19046  553592195786506240  #BREAKING - Both #CharlieHebdo suspects killed...   \n",
       "19047  553592195786506240  #BREAKING - Both #CharlieHebdo suspects killed...   \n",
       "19048  553592195786506240  #BREAKING - Both #CharlieHebdo suspects killed...   \n",
       "19049  553592195786506240  #BREAKING - Both #CharlieHebdo suspects killed...   \n",
       "\n",
       "                     time       location  followers    user_id  \\\n",
       "0     2015-01-07 11:06:08          Paris       1628  384779793   \n",
       "1     2015-01-07 11:06:08          Paris       1628  384779793   \n",
       "2     2015-01-07 11:06:08          Paris       1628  384779793   \n",
       "3     2015-01-07 11:06:08          Paris       1628  384779793   \n",
       "4     2015-01-07 11:06:08          Paris       1628  384779793   \n",
       "...                   ...            ...        ...        ...   \n",
       "19045 2015-01-09 16:40:39  Paris, France    1261930    1994321   \n",
       "19046 2015-01-09 16:40:39  Paris, France    1261930    1994321   \n",
       "19047 2015-01-09 16:40:39  Paris, France    1261930    1994321   \n",
       "19048 2015-01-09 16:40:39  Paris, France    1261930    1994321   \n",
       "19049 2015-01-09 16:40:39  Paris, France    1261930    1994321   \n",
       "\n",
       "       favorite_count  retweet_count  verified  rumour  ...  \\\n",
       "0                  14            159     False       1  ...   \n",
       "1                  14            159     False       1  ...   \n",
       "2                  14            159     False       1  ...   \n",
       "3                  14            159     False       1  ...   \n",
       "4                  14            159     False       1  ...   \n",
       "...               ...            ...       ...     ...  ...   \n",
       "19045              41            133      True       0  ...   \n",
       "19046              41            133      True       0  ...   \n",
       "19047              41            133      True       0  ...   \n",
       "19048              41            133      True       0  ...   \n",
       "19049              41            133      True       0  ...   \n",
       "\n",
       "               reply_time    reply_location reply_followers reply_user_id  \\\n",
       "0     2015-01-07 11:24:15               NaN              40     202572421   \n",
       "1     2015-01-07 11:31:37   Zimbabwe-London             375     239943362   \n",
       "2     2015-01-07 11:38:37             delhi              17    2903715212   \n",
       "3     2015-01-07 11:45:32               NaN              54    2911191121   \n",
       "4     2015-01-07 12:32:50       Shredsville             683    1348798826   \n",
       "...                   ...               ...             ...           ...   \n",
       "19045 2015-01-09 16:42:42  Montreal, Canada           14085       7207042   \n",
       "19046 2015-01-09 16:42:54     قبرص(اليونان)             189    2814170438   \n",
       "19047 2015-01-09 16:43:11      Planet Earth             281      16573662   \n",
       "19048 2015-01-09 16:46:50                UK             793    1067999394   \n",
       "19049 2015-01-09 16:58:09  Sunny California            2236      37651316   \n",
       "\n",
       "       reply_verified  time_diff  reply_number  \\\n",
       "0               False  18.116667           1.0   \n",
       "1               False  25.483333           2.0   \n",
       "2               False  32.483333           3.0   \n",
       "3               False  39.400000           4.0   \n",
       "4               False  86.700000           5.0   \n",
       "...               ...        ...           ...   \n",
       "19045           False   2.050000           1.0   \n",
       "19046           False   2.250000           2.0   \n",
       "19047           False   2.533333           3.0   \n",
       "19048           False   6.183333           4.0   \n",
       "19049           False  17.500000           5.0   \n",
       "\n",
       "                                        clean_reply_text  \\\n",
       "0                                 religion peace strikes   \n",
       "1      hi henry would willing give itv news phone int...   \n",
       "2      please call terrorists gunmen dont dilute news...   \n",
       "3                   french govt needs take strict action   \n",
       "4      people didnt hand guns jeffrey epstein also cf...   \n",
       "...                                                  ...   \n",
       "19045  rt breaking charliehebdo suspects killed polic...   \n",
       "19046                                             coming   \n",
       "19047  happy hear two terrorists gone doom praise inv...   \n",
       "19048  well done french ending chapter terror terrori...   \n",
       "19049                                 good swift justice   \n",
       "\n",
       "                                        reply_embeddings  \\\n",
       "0      [[0.38767001032829285, 0.5266600251197815, 0.3...   \n",
       "1      [[0.1444000005722046, 0.23978999257087708, 0.9...   \n",
       "2      [[-0.9112600088119507, 0.3958500027656555, 1.2...   \n",
       "3      [[0.02704799920320511, -0.0538330003619194, 0....   \n",
       "4      [[0.2901900112628937, 0.8049700260162354, 0.31...   \n",
       "...                                                  ...   \n",
       "19045  [[-0.34817999601364136, -0.10100000351667404, ...   \n",
       "19046  [[0.0667089968919754, 0.2744799852371216, 0.81...   \n",
       "19047  [[-0.09043599665164948, 0.1963600069284439, 0....   \n",
       "19048  [[-0.5308600068092346, 0.5140399932861328, 0.0...   \n",
       "19049  [[-0.030768999829888344, 0.11992999911308289, ...   \n",
       "\n",
       "                                    reply_embeddings_avg  \n",
       "0      [-0.1889099975426992, 0.3738733381032944, -0.2...  \n",
       "1      [-0.12313784658908844, -0.18030815256329683, 0...  \n",
       "2      [-0.19128870833665132, 0.15544349609408528, 0....  \n",
       "3      [-0.06885033225019772, 0.2138711006846279, 0.1...  \n",
       "4      [0.014819002151489258, 0.014486800134181976, -...  \n",
       "...                                                  ...  \n",
       "19045  [-0.04054700657725334, -0.028997698239982127, ...  \n",
       "19046  [0.0667089968919754, 0.2744799852371216, 0.814...  \n",
       "19047  [-0.10175399528816342, 0.3584062494337559, 0.2...  \n",
       "19048  [0.047132634981112045, 0.16482700068842282, 0....  \n",
       "19049  [0.10588399755458038, -0.15457333127657572, 0....  \n",
       "\n",
       "[19050 rows x 22 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('replies_charlie_hebdo.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.to_pickle(\"posts_charlie_hebdo.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44ZNwTqtKovw"
   },
   "source": [
    "#### Creat Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts = pd.read_pickle(r\"/workspaces/rumour-detection-pheme/posts_charlie_hebdo.pkl\")\n",
    "df_replies = pd.read_pickle(r\"/workspaces/rumour-detection-pheme/replies_charlie_hebdo.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_post_not_rumour = df_posts[df_posts.rumour==0]\n",
    "#total_samples = int(df_posts[df_posts.rumour==1].count()[0]*1.1)\n",
    "\n",
    "# Get the proportion of each value in the 'replies' column\n",
    "#value_counts = df_post_not_rumour['replies'].value_counts(normalize=True)\n",
    "\n",
    "# Calculate the number of samples for each value\n",
    "#samples_per_value = (value_counts * total_samples).round().astype(int)\n",
    "\n",
    "# Sample the required number of rows for each value in 'replies'\n",
    "#sampled_dfs = []\n",
    "#for value, num_samples in samples_per_value.items():\n",
    "#    sampled_dfs.append(df_post_not_rumour[df_post_not_rumour['replies'] \\\n",
    "#                       == value].sample(num_samples, random_state=42))\n",
    "\n",
    "# Concatenate all the samples into a single dataframe\n",
    "#sampled_df = pd.concat(sampled_dfs)\n",
    "# Shuffle the resulting dataframe\n",
    "#sampled_df = sampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "#df_posts = pd.concat([sampled_df,df_posts[df_posts.rumour==1]])\\\n",
    "#             .sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "\n",
    "#df_replies = df_replies[df_replies.id.isin(df_posts.id.values)][['id','reply_id',\\\n",
    "#             'reply_followers','reply_user_id','reply_verified',\"rumour\",\\\n",
    "#              \"user_id\",\"reply_embeddings_avg\",\"time_diff\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yY1jnj9m3ElI"
   },
   "source": [
    "#### Torch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "xY9kjy-URKKx"
   },
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "df_posts['verified'] = df_posts['verified'].astype('str').str.\\\n",
    "             replace(' ', '').replace('True', '1').replace('False', '0')\\\n",
    "             .astype('int64')\n",
    "\n",
    "df_posts = pd.concat([df_posts, pd.get_dummies(\\\n",
    "                          df_posts[\"verified\"],dtype=int)], axis=1, join='inner')\n",
    "df_posts.drop([\"verified\"], axis=1, inplace=True)\n",
    "df_posts.rename(columns={1:'verified',0:'no_verified'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "fDw9YB9qRKB_"
   },
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "df_replies['reply_verified'] = df_replies['reply_verified'].astype('str').str.\\\n",
    "             replace(' ', '').replace('True', '1').replace('False', '0')\\\n",
    "             .astype('int64')\n",
    "\n",
    "df_replies = pd.concat([df_replies, pd.get_dummies(\\\n",
    "                          df_replies[\"reply_verified\"],dtype=int)], axis=1, join='inner')\n",
    "df_replies.drop([\"reply_verified\"], axis=1, inplace=True)\n",
    "df_replies.rename(columns={1:'reply_verified',0:'reply_no_verified'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "Wa1NAPktPoB8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get unique values from the column\n",
    "unique_values = df_posts['id'].unique()\n",
    "\n",
    "# Create a dictionary to map values to IDs\n",
    "value_to_id = {value: i for i, value in enumerate(unique_values)}\n",
    "\n",
    "# Map the values to IDs using the dictionary\n",
    "#retweets_node_features['index'] = retweets_node_features['reply_user_id'].map(value_to_id)\n",
    "post_map  = value_to_id\n",
    "\n",
    "#Only keep features\n",
    "post_features = df_posts[[\"followers\", \"favorite_count\",\"retweet_count\",\"no_verified\",\"verified\",\\\n",
    "                          \"first_time_diff\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "i3VSO_R8xKgy"
   },
   "outputs": [],
   "source": [
    "embeddings_avg = []\n",
    "for embeddings in df_posts['embeddings_avg']:\n",
    "  #embeddings= i.replace('[','').replace(']','').replace('\\n','').split(' ')\n",
    "  #embeddings =  [float(item) for item in embeddings if item != '']\n",
    "  embeddings_avg.append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize the Robust Scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Assuming data is a DataFrame containing your dataset\n",
    "scaled_features = scaler.fit_transform(post_features[['followers', 'favorite_count', 'retweet_count', 'first_time_diff']])\n",
    "\n",
    "# Convert the scaled features back to a DataFrame\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=['followers', 'favorite_count', 'retweet_count', 'first_time_diff'])\n",
    "\n",
    "# Add the binary features back to the scaled data\n",
    "scaled_data['no_verified'] = post_features['no_verified']\n",
    "scaled_data['verified'] = post_features['verified']\n",
    "post_features = scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QnaMBT0yylBn",
    "outputId": "31b11cf1-5402-4ca1-c567-f1c5abfeea86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2002, 106)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to numpy\n",
    "x1 = np.concatenate((post_features.to_numpy(),np.array(embeddings_avg)),axis=1)\n",
    "#x1 = post_features.to_numpy()\n",
    "x1.shape # [num_movie_nodes x movie_node_feature_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "dNbQMYlKzCUi"
   },
   "outputs": [],
   "source": [
    "# Select node features\n",
    "retweets_node_features = df_replies[[\"reply_followers\", \"reply_no_verified\", \"reply_verified\",\"reply_user_id\",\"time_diff\"]]\n",
    "\n",
    "# Get unique values from the column\n",
    "unique_values = retweets_node_features['reply_user_id'].unique()\n",
    "\n",
    "# Create a dictionary to map values to IDs\n",
    "value_to_id = {value: i for i, value in enumerate(unique_values)}\n",
    "\n",
    "# Map the values to IDs using the dictionary\n",
    "#retweets_node_features['index'] = retweets_node_features['reply_user_id'].map(value_to_id)\n",
    "retweets_id_mapping  = value_to_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Only keep features\n",
    "retweets_node_features = retweets_node_features[[\"reply_followers\", \"reply_no_verified\", \"reply_verified\",\"time_diff\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_avg = []\n",
    "for embeddings in df_replies['reply_embeddings_avg']:\n",
    "  #embeddings= i.replace('[','').replace(']','').replace('\\n','').split(' ')\n",
    "  #embeddings =  [float(item) for item in embeddings if item != '']\n",
    "  embeddings_avg.append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Robust Scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Assuming data is a DataFrame containing your dataset\n",
    "scaled_features = scaler.fit_transform(retweets_node_features[['reply_followers','time_diff']])\n",
    "\n",
    "# Convert the scaled features back to a DataFrame\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=['reply_followers','time_diff'])\n",
    "\n",
    "# Add the binary features back to the scaled data\n",
    "scaled_data['reply_no_verified'] = retweets_node_features['reply_no_verified']\n",
    "scaled_data['reply_verified'] = retweets_node_features['reply_verified']\n",
    "retweets_node_features = scaled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19050, 4)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to numpy\n",
    "#x2 = np.concatenate((retweets_node_features.to_numpy(),np.array(embeddings_avg)),axis=1)\n",
    "x2 = retweets_node_features.to_numpy()\n",
    "x2.shape # [num_movie_nodes x movie_node_feature_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYMUpme80rwi",
    "outputId": "fad85d16-324a-47ee-a7e4-a2afd472dffa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2002,)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract labels\n",
    "labels = df_posts.rumour\n",
    "y = labels.to_numpy()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "rsb7ZYVI04R6"
   },
   "outputs": [],
   "source": [
    "# Map post IDs\n",
    "#post_map = posts_id_mapping.reset_index().set_index(\"id\").to_dict()\n",
    "df_replies[\"id\"] = df_replies['id'].map(post_map).astype(int)\n",
    "# Map user IDs\n",
    "#retweets_map = retweets_id_mapping #retweets_id_mapping.reset_index().set_index(\"reply_user_id\").to_dict()\n",
    "df_replies[\"reply_user_id\"] = df_replies[\"reply_user_id\"].map(retweets_id_mapping)#.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zov-Q9429v1",
    "outputId": "a1a9b871-c403-434b-e5b3-45e4feea6544"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,  2001,  2001,  2001],\n",
       "       [    0,     1,     2, ...,   284,  1810, 14464]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = df_replies[[\"id\", \"reply_user_id\"]].values.transpose()\n",
    "edge_index # [2 x num_edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "I8VTjazz6G7U"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Number of rows\n",
    "num_rows = x1.shape[0]\n",
    "\n",
    "# Desired proportions\n",
    "train_proportion = 0.70\n",
    "val_proportion = 0.15\n",
    "test_proportion = 0.15\n",
    "\n",
    "# Generate a list of indices and shuffle them\n",
    "indices = np.arange(num_rows)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Calculate the split indices\n",
    "train_end = int(train_proportion * num_rows)\n",
    "val_end = train_end + int(val_proportion * num_rows)\n",
    "\n",
    "# Split the indices into train, validation, and test\n",
    "train_indices = indices[:train_end]\n",
    "val_indices = indices[train_end:val_end]\n",
    "test_indices = indices[val_end:]\n",
    "\n",
    "# Create masks\n",
    "train_mask = np.zeros(num_rows, dtype=bool)\n",
    "val_mask = np.zeros(num_rows, dtype=bool)\n",
    "test_mask = np.zeros(num_rows, dtype=bool)\n",
    "\n",
    "# Assign True to the corresponding indices\n",
    "train_mask[train_indices] = True\n",
    "val_mask[val_indices] = True\n",
    "test_mask[test_indices] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XyvzeR7IymQ",
    "outputId": "b26eb871-555d-42fd-caff-ad906a7ccf7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  id={\n",
      "    x=[2002, 106],\n",
      "    y=[2002],\n",
      "    train_mask=[2002],\n",
      "    val_mask=[2002],\n",
      "    test_mask=[2002],\n",
      "  },\n",
      "  reply_user_id={ x=[19050, 4] },\n",
      "  (id, retweet, reply_user_id)={ edge_index=[2, 19050] },\n",
      "  (reply_user_id, rev_retweet, id)={ edge_index=[2, 19050] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the HeteroData object\n",
    "data = HeteroData()\n",
    "\n",
    "# Add node features and labels for the 'paper' node type\n",
    "data['id'].x = torch.tensor(x1,dtype=torch.float32)  # Node features dtype=torch.float32\n",
    "data['id'].y = torch.from_numpy(y)  # Node labels (for example, binary classification)\n",
    "data['id'].train_mask = torch.tensor(train_mask)  # Training mask\n",
    "data['id'].val_mask =torch.tensor(val_mask)  # Validation mask\n",
    "data['id'].test_mask = torch.tensor(test_mask)  # Test mask\n",
    "\n",
    "# Add node features for the 'author' node type\n",
    "data['reply_user_id'].x = torch.tensor(x2,dtype=torch.float32) #torch.float32\n",
    "\n",
    "# Add edges for the (id, retweet, reply_D) relation\n",
    "data['id', 'retweet', 'reply_user_id'].edge_index = torch.from_numpy(edge_index.reshape(2,len(x2)))\n",
    "data = T.ToUndirected()(data)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtOzUtI7frmN",
    "outputId": "cedbbb5a-d3a3-46b2-adea-f6ed97e206cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: tensor(307) torch.Size([1401])\n"
     ]
    }
   ],
   "source": [
    "print(\"train:\",data['id'].y[train_mask].sum(),data['id'].y[train_mask].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0hD33th-gFnO",
    "outputId": "0e0f7739-7d7e-49dd-cf0f-8c8221739a32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: tensor(65) torch.Size([300])\n"
     ]
    }
   ],
   "source": [
    "print(\"val:\", data['id'].y[val_mask].sum(),data['id'].y[val_mask].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5_aX0ZogGHn",
    "outputId": "5f4934da-8242-429e-9a90-fa8f256d2b2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: tensor(75) torch.Size([301])\n"
     ]
    }
   ],
   "source": [
    "print(\"test:\", data['id'].y[test_mask].sum(),data['id'].y[test_mask].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('charlie_hebdo_graph_dataset_node_embeddings_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HeteroData object from the pkl file\n",
    "with open('charlie_hebdo_graph_dataset_node_embeddings_v2.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mastering",
   "language": "python",
   "name": "mastering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
