{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbee25c8-853d-412a-8677-fdd4a9eb0cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "import pickle\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, to_hetero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf21648f-ad1a-4e0f-9261-32816face4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usage\n",
    "file_path_replies = r\"/home/azureuser/rumour-detection-pheme/replies_charlie_hebdo.pkl\"\n",
    "file_path_posts = r\"/home/azureuser/rumour-detection-pheme/posts_charlie_hebdo.pkl\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a34ddbd4-c12d-4de9-9650-b4b044d561ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HeteroDataProcessorFilterNodeonTest:\n",
    "    def __init__(self, file_path_replies, file_path_posts, time_cut=15):\n",
    "        self.file_path_replies = file_path_replies\n",
    "        self.file_path_posts = file_path_posts\n",
    "        self.time_cut = time_cut\n",
    "        self.df_replies = None\n",
    "        self.df_posts = None\n",
    "\n",
    "    def load_data(self):\n",
    "        self.df_replies = pd.read_pickle(self.file_path_replies)\n",
    "        self.df_posts = pd.read_pickle(self.file_path_posts)\n",
    "\n",
    "    def process_data(self):\n",
    "        # Define post and reply features\n",
    "        post_features = ['followers', 'favorite_count', 'retweet_count', 'verified', 'rumour', 'id', 'embeddings_avg']\n",
    "        reply_features = ['reply_followers', 'reply_user_id', 'reply_verified', 'time_diff', 'reply_embeddings_avg', 'id']\n",
    "\n",
    "        # Filter and group replies\n",
    "        self.df_replies['min_since_fst_post'] = round(\n",
    "            (self.df_replies['time'] - self.df_replies['time'].min()).dt.total_seconds() / 60, 2)\n",
    "        grouped_replies = self.df_replies.groupby(['id']).agg(\n",
    "            replies=('time_diff', 'count'),\n",
    "            first_time_diff=('time_diff', 'first')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Merge posts and replies\n",
    "        self.df_posts = self.df_posts[post_features].merge(grouped_replies, on=\"id\", how=\"inner\")\n",
    "        self.df_posts['replies'] = self.df_posts['replies'].fillna(0)\n",
    "        self.df_posts['first_time_diff'] = self.df_posts['first_time_diff'].fillna(0)\n",
    "\n",
    "        # One-hot encoding for verified columns\n",
    "        self.df_posts['verified'] = self.df_posts['verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        self.df_posts = pd.concat([self.df_posts, pd.get_dummies(self.df_posts[\"verified\"], dtype=int)], axis=1)\n",
    "        self.df_posts.drop([\"verified\"], axis=1, inplace=True)\n",
    "        self.df_posts.rename(columns={1: 'verified', 0: 'no_verified'}, inplace=True)\n",
    "\n",
    "        self.df_replies['reply_verified'] = self.df_replies['reply_verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        self.df_replies = pd.concat([self.df_replies, pd.get_dummies(self.df_replies[\"reply_verified\"], dtype=int)], axis=1)\n",
    "        self.df_replies.drop([\"reply_verified\"], axis=1, inplace=True)\n",
    "        self.df_replies.rename(columns={1: 'reply_verified', 0: 'reply_no_verified'}, inplace=True)\n",
    "\n",
    "        # Train/test split\n",
    "        train, not_train = train_test_split(self.df_posts, test_size=0.3, random_state=42, stratify=self.df_posts['rumour'])\n",
    "        val, test = train_test_split(not_train, test_size=0.5, random_state=42, stratify=not_train['rumour'])\n",
    "\n",
    "        # Post features processing\n",
    "        post_features = train[[\"followers\", \"favorite_count\", \"retweet_count\", \"no_verified\", \"verified\", \"first_time_diff\"]]\n",
    "        scaler_posts = RobustScaler()\n",
    "        scaled_features =scaler_posts.fit_transform(post_features[['followers', 'favorite_count', 'retweet_count', 'first_time_diff']])\n",
    "        scaled_data = pd.DataFrame(scaled_features, columns=['followers', 'favorite_count', 'retweet_count', 'first_time_diff'])\n",
    "        scaled_data['no_verified'] = np.array(train['no_verified'])\n",
    "        scaled_data['verified'] = np.array(train['verified'])\n",
    "        post_features = scaled_data\n",
    "        post_embeddings = np.array(train['embeddings_avg'].tolist())\n",
    "        x1 = np.concatenate((post_features, post_embeddings), axis=1)\n",
    "\n",
    "        # Reply features processing\n",
    "        scaler_replies = RobustScaler()\n",
    "        reply_features = self.df_replies[self.df_replies.id.isin(np.array(train.id))][[\"reply_followers\", \"reply_no_verified\", \"reply_verified\", \"time_diff\"]]\n",
    "        reply_features[['reply_followers', 'time_diff']] = scaler_replies.fit_transform(reply_features[['reply_followers', 'time_diff']])\n",
    "        reply_embeddings = np.array(self.df_replies[self.df_replies.id.isin(np.array(train.id))]['reply_embeddings_avg'].tolist())\n",
    "        x2 = np.concatenate((reply_features, reply_embeddings), axis=1)\n",
    "\n",
    "        # Test/validation data preparation\n",
    "        test_val_df_replies = pd.read_pickle(self.file_path_replies)\n",
    "        test_val_df_posts = pd.read_pickle(self.file_path_posts)\n",
    "        test_val_df_posts = test_val_df_posts[~test_val_df_posts.id.isin(train.id)]\n",
    "        test_val_df_replies = test_val_df_replies[~test_val_df_replies.id.isin(train.id)]\n",
    "\n",
    "        post_features = ['followers', 'favorite_count', 'retweet_count', 'verified', 'rumour', 'id', 'embeddings_avg']\n",
    "        test_val_reply_features = ['reply_followers', 'reply_user_id', 'reply_verified', 'time_diff', 'reply_embeddings_avg', 'id']\n",
    "        \n",
    "        test_val_df_replies['min_since_fst_post'] = round((test_val_df_replies['time'] - test_val_df_replies['time'].min())\\\n",
    "        .dt.total_seconds() / 60,2)\n",
    "        \n",
    "        test_val_df_replies = test_val_df_replies[test_val_reply_features][(test_val_df_replies.time_diff <= time_cut)&\\\n",
    "           (test_val_df_replies.min_since_fst_post <= time_cut)]\n",
    "        grouped_replies = test_val_df_replies.groupby(['id']).agg(\n",
    "            replies=('time_diff', 'count'),\n",
    "            first_time_diff=('time_diff', 'first')\n",
    "        ).reset_index()\n",
    "        \n",
    "        test_val_df_posts = test_val_df_posts[post_features].merge(grouped_replies, on=\"id\", how=\"inner\")\n",
    "        test_val_df_posts['replies'] = test_val_df_posts['replies'].fillna(0)\n",
    "        test_val_df_posts['first_time_diff'] = test_val_df_posts['first_time_diff'].fillna(0)\n",
    "        \n",
    "        # One-hot encoding for verified columns\n",
    "        test_val_df_posts['verified'] = test_val_df_posts['verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        test_val_df_posts = pd.concat([test_val_df_posts, pd.get_dummies(test_val_df_posts[\"verified\"], dtype=int)], axis=1)\n",
    "        test_val_df_posts.drop([\"verified\"], axis=1, inplace=True)\n",
    "        test_val_df_posts.rename(columns={1: 'verified', 0: 'no_verified'}, inplace=True)\n",
    "        \n",
    "        test_val_df_replies['reply_verified'] = test_val_df_replies['reply_verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        test_val_df_replies = pd.concat([test_val_df_replies, pd.get_dummies(test_val_df_replies[\"reply_verified\"], dtype=int)], axis=1)\n",
    "        test_val_df_replies.drop([\"reply_verified\"], axis=1, inplace=True)\n",
    "        test_val_df_replies.rename(columns={1: 'reply_verified', 0: 'reply_no_verified'}, inplace=True)\n",
    "        \n",
    "        test_val_df_posts = test_val_df_posts.merge(pd.concat([val,test])[['id']].reset_index(),on='id',how='left')\n",
    "        test_val_df_posts.set_index('index',drop=True,inplace=True)\n",
    "        \n",
    "        post_features = test_val_df_posts[[\"followers\", \"favorite_count\", \"retweet_count\", \"no_verified\", \"verified\", \"first_time_diff\"]]\n",
    "        \n",
    "        \n",
    "        scaled_features = scaler_posts.transform(post_features[['followers', 'favorite_count', 'retweet_count', 'first_time_diff']])\n",
    "        \n",
    "        # Convert the scaled features back to a DataFrame\n",
    "        scaled_data = pd.DataFrame(scaled_features, columns=['followers', 'favorite_count', 'retweet_count', 'first_time_diff'])\n",
    "        \n",
    "        # Add the binary features back to the scaled data\n",
    "        scaled_data['no_verified'] = np.array(post_features['no_verified'])\n",
    "        scaled_data['verified'] = np.array(post_features['verified'])\n",
    "        post_features = scaled_data\n",
    "        \n",
    "        post_embeddings = np.array(test_val_df_posts['embeddings_avg'].tolist())\n",
    "        #post_features = scaler.fit_transform(post_features)\n",
    "        x3 = np.concatenate((post_features, post_embeddings), axis=1)\n",
    "        \n",
    "        test_val_reply_features =  test_val_df_replies[[\"reply_followers\", \"reply_no_verified\",\"reply_verified\",\"time_diff\"]]\n",
    "        test_val_reply_features[['reply_followers','time_diff']] = scaler_replies.transform(test_val_reply_features[['reply_followers','time_diff']])\n",
    "        \n",
    "        test_val_reply_embeddings = np.array(test_val_df_replies['reply_embeddings_avg'].tolist())\n",
    "        x4 = np.concatenate((test_val_reply_features, test_val_reply_embeddings), axis=1)\n",
    "        \n",
    "        \n",
    "        # Mapping post ids\n",
    "        post_map = {value: i for i, value in enumerate(pd.concat([train[['id']],test_val_df_posts[['id']]])['id'].unique())}\n",
    "        df_replies_edges = pd.concat([self.df_replies[self.df_replies.id.isin(np.array(train.id))][[\"id\", \"reply_user_id\"]],\\\n",
    "                                test_val_df_replies[[\"id\", \"reply_user_id\"]]])\n",
    "        \n",
    "        df_replies_edges[\"id\"] = df_replies_edges['id'].map(post_map).astype(int)\n",
    "        \n",
    "        # Mapping reply user ids\n",
    "        reply_user_map = {value: i for i, value in enumerate(df_replies_edges['reply_user_id'].unique())}\n",
    "        df_replies_edges[\"reply_user_id\"] = df_replies_edges[\"reply_user_id\"].map(reply_user_map)\n",
    "\n",
    "        return train,test_val_df_posts,df_replies_edges,x1,x2,x3,x4\n",
    "\n",
    "\n",
    "    def create_heterodata(self,train,test_val_df_posts,df_replies_edges,x1,x2,x3,x4):\n",
    "\n",
    "        y = pd.concat([train['rumour'],test_val_df_posts['rumour']]).to_numpy()\n",
    "        edge_index = df_replies_edges.values.transpose()\n",
    "        x = np.concatenate((x1,x3))\n",
    "        x_reply = np.concatenate((x2,x4))\n",
    "            \n",
    "        num_rows = x.shape[0]\n",
    "        train_mask = np.zeros(num_rows, dtype=bool)\n",
    "        val_mask = np.zeros(num_rows, dtype=bool)\n",
    "        test_mask = np.zeros(num_rows, dtype=bool)\n",
    "        train_mask[:-x3.shape[0]]=True\n",
    "        val_mask[-x3.shape[0]:-int(x3.shape[0]/2)]=True\n",
    "        test_mask[-int(x3.shape[0]/2):]=True\n",
    "            \n",
    "        data = HeteroData()\n",
    "        data['id'].x = torch.tensor(x, dtype=torch.float32)\n",
    "        data['id'].y =  torch.from_numpy(y)\n",
    "        data['id'].train_mask = torch.tensor(train_mask)\n",
    "        data['id'].val_mask = torch.tensor(val_mask) \n",
    "        data['id'].test_mask = torch.tensor(test_mask)\n",
    "        data['reply_user_id'].x = torch.tensor(x_reply, dtype=torch.float32)\n",
    "        data['id', 'retweet', 'reply_user_id'].edge_index = torch.from_numpy(edge_index.reshape(2,len(x_reply)))\n",
    "        data = T.ToUndirected()(data)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def process(self):\n",
    "            \n",
    "        self.load_data()\n",
    "        train,test_val_df_posts,df_replies_edges,x1,x2,x3,x4 = self.process_data()\n",
    "        return self.create_heterodata(train,test_val_df_posts,df_replies_edges,x1,x2,x3,x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "102693a4-a5ad-4f77-80b8-f3fbd33ec322",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HeteroDataProcessorFilterNodeonTestV2:\n",
    "    #Different time cuts \n",
    "    def __init__(self, file_path_replies, file_path_posts, time_cut_replies=15,time_cut_posts=15):\n",
    "        self.file_path_replies = file_path_replies\n",
    "        self.file_path_posts = file_path_posts\n",
    "        self.time_cut_replies = time_cut_replies\n",
    "        self.time_cut_posts = time_cut_posts\n",
    "        self.df_replies = None\n",
    "        self.df_posts = None\n",
    "\n",
    "    def load_data(self):\n",
    "        self.df_replies = pd.read_pickle(self.file_path_replies)\n",
    "        self.df_posts = pd.read_pickle(self.file_path_posts)\n",
    "\n",
    "    def process_data(self):\n",
    "        # Define post and reply features\n",
    "        post_features = ['followers', 'favorite_count', 'retweet_count', 'verified', 'rumour', 'id', 'embeddings_avg']\n",
    "        reply_features = ['reply_followers', 'reply_user_id', 'reply_verified', 'time_diff', 'reply_embeddings_avg', 'id']\n",
    "\n",
    "        # Filter and group replies\n",
    "        self.df_replies['min_since_fst_post'] = round(\n",
    "            (self.df_replies['time'] - self.df_replies['time'].min()).dt.total_seconds() / 60, 2)\n",
    "        grouped_replies = self.df_replies.groupby(['id']).agg(\n",
    "            replies=('time_diff', 'count'),\n",
    "            first_time_diff=('time_diff', 'first')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Merge posts and replies\n",
    "        self.df_posts = self.df_posts[post_features].merge(grouped_replies, on=\"id\", how=\"inner\")\n",
    "        self.df_posts['replies'] = self.df_posts['replies'].fillna(0)\n",
    "        self.df_posts['first_time_diff'] = self.df_posts['first_time_diff'].fillna(0)\n",
    "\n",
    "        # One-hot encoding for verified columns\n",
    "        self.df_posts['verified'] = self.df_posts['verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        self.df_posts = pd.concat([self.df_posts, pd.get_dummies(self.df_posts[\"verified\"], dtype=int)], axis=1)\n",
    "        self.df_posts.drop([\"verified\"], axis=1, inplace=True)\n",
    "        self.df_posts.rename(columns={1: 'verified', 0: 'no_verified'}, inplace=True)\n",
    "\n",
    "        self.df_replies['reply_verified'] = self.df_replies['reply_verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        self.df_replies = pd.concat([self.df_replies, pd.get_dummies(self.df_replies[\"reply_verified\"], dtype=int)], axis=1)\n",
    "        self.df_replies.drop([\"reply_verified\"], axis=1, inplace=True)\n",
    "        self.df_replies.rename(columns={1: 'reply_verified', 0: 'reply_no_verified'}, inplace=True)\n",
    "\n",
    "        # Train/test split\n",
    "        train, not_train = train_test_split(self.df_posts, test_size=0.3, random_state=42, stratify=self.df_posts['rumour'])\n",
    "        val, test = train_test_split(not_train, test_size=0.5, random_state=42, stratify=not_train['rumour'])\n",
    "\n",
    "        # Post features processing\n",
    "        post_features = train[[\"followers\", \"favorite_count\", \"retweet_count\", \"no_verified\", \"verified\", \"first_time_diff\"]]\n",
    "        scaler_posts = RobustScaler()\n",
    "        scaled_features =scaler_posts.fit_transform(post_features[['followers', 'favorite_count', 'retweet_count', 'first_time_diff']])\n",
    "        scaled_data = pd.DataFrame(scaled_features, columns=['followers', 'favorite_count', 'retweet_count', 'first_time_diff'])\n",
    "        scaled_data['no_verified'] = np.array(train['no_verified'])\n",
    "        scaled_data['verified'] = np.array(train['verified'])\n",
    "        post_features = scaled_data\n",
    "        post_embeddings = np.array(train['embeddings_avg'].tolist())\n",
    "        x1 = np.concatenate((post_features, post_embeddings), axis=1)\n",
    "\n",
    "        # Reply features processing\n",
    "        scaler_replies = RobustScaler()\n",
    "        reply_features = self.df_replies[self.df_replies.id.isin(np.array(train.id))][[\"reply_followers\", \"reply_no_verified\", \"reply_verified\", \"time_diff\"]]\n",
    "        reply_features[['reply_followers', 'time_diff']] = scaler_replies.fit_transform(reply_features[['reply_followers', 'time_diff']])\n",
    "        reply_embeddings = np.array(self.df_replies[self.df_replies.id.isin(np.array(train.id))]['reply_embeddings_avg'].tolist())\n",
    "        x2 = np.concatenate((reply_features, reply_embeddings), axis=1)\n",
    "\n",
    "        # Test/validation data preparation\n",
    "        test_val_df_replies = pd.read_pickle(self.file_path_replies)\n",
    "        test_val_df_posts = pd.read_pickle(self.file_path_posts)\n",
    "        test_val_df_posts = test_val_df_posts[~test_val_df_posts.id.isin(train.id)]\n",
    "        test_val_df_replies = test_val_df_replies[~test_val_df_replies.id.isin(train.id)]\n",
    "\n",
    "        post_features = ['followers', 'favorite_count', 'retweet_count', 'verified', 'rumour', 'id', 'embeddings_avg']\n",
    "        test_val_reply_features = ['reply_followers', 'reply_user_id', 'reply_verified', 'time_diff', 'reply_embeddings_avg', 'id']\n",
    "        \n",
    "        test_val_df_replies['min_since_fst_post'] = round((test_val_df_replies['time'] - test_val_df_replies['time'].min())\\\n",
    "        .dt.total_seconds() / 60,2)\n",
    "        \n",
    "        test_val_df_replies = test_val_df_replies[test_val_reply_features][(test_val_df_replies.time_diff <= self.time_cut_replies)&\\\n",
    "           (test_val_df_replies.min_since_fst_post <= self.time_cut_posts)]\n",
    "        grouped_replies = test_val_df_replies.groupby(['id']).agg(\n",
    "            replies=('time_diff', 'count'),\n",
    "            first_time_diff=('time_diff', 'first')\n",
    "        ).reset_index()\n",
    "        \n",
    "        test_val_df_posts = test_val_df_posts[post_features].merge(grouped_replies, on=\"id\", how=\"inner\")\n",
    "        test_val_df_posts['replies'] = test_val_df_posts['replies'].fillna(0)\n",
    "        test_val_df_posts['first_time_diff'] = test_val_df_posts['first_time_diff'].fillna(0)\n",
    "        \n",
    "        # One-hot encoding for verified columns\n",
    "        test_val_df_posts['verified'] = test_val_df_posts['verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        test_val_df_posts = pd.concat([test_val_df_posts, pd.get_dummies(test_val_df_posts[\"verified\"], dtype=int)], axis=1)\n",
    "        test_val_df_posts.drop([\"verified\"], axis=1, inplace=True)\n",
    "        test_val_df_posts.rename(columns={1: 'verified', 0: 'no_verified'}, inplace=True)\n",
    "        \n",
    "        test_val_df_replies['reply_verified'] = test_val_df_replies['reply_verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        test_val_df_replies = pd.concat([test_val_df_replies, pd.get_dummies(test_val_df_replies[\"reply_verified\"], dtype=int)], axis=1)\n",
    "        test_val_df_replies.drop([\"reply_verified\"], axis=1, inplace=True)\n",
    "        test_val_df_replies.rename(columns={1: 'reply_verified', 0: 'reply_no_verified'}, inplace=True)\n",
    "        \n",
    "        test_val_df_posts = test_val_df_posts.merge(pd.concat([val,test])[['id']].reset_index(),on='id',how='left')\n",
    "        test_val_df_posts.set_index('index',drop=True,inplace=True)\n",
    "        \n",
    "        post_features = test_val_df_posts[[\"followers\", \"favorite_count\", \"retweet_count\", \"no_verified\", \"verified\", \"first_time_diff\"]]\n",
    "        \n",
    "        \n",
    "        scaled_features = scaler_posts.transform(post_features[['followers', 'favorite_count', 'retweet_count', 'first_time_diff']])\n",
    "        \n",
    "        # Convert the scaled features back to a DataFrame\n",
    "        scaled_data = pd.DataFrame(scaled_features, columns=['followers', 'favorite_count', 'retweet_count', 'first_time_diff'])\n",
    "        \n",
    "        # Add the binary features back to the scaled data\n",
    "        scaled_data['no_verified'] = np.array(post_features['no_verified'])\n",
    "        scaled_data['verified'] = np.array(post_features['verified'])\n",
    "        post_features = scaled_data\n",
    "        \n",
    "        post_embeddings = np.array(test_val_df_posts['embeddings_avg'].tolist())\n",
    "        #post_features = scaler.fit_transform(post_features)\n",
    "        x3 = np.concatenate((post_features, post_embeddings), axis=1)\n",
    "        \n",
    "        test_val_reply_features =  test_val_df_replies[[\"reply_followers\", \"reply_no_verified\",\"reply_verified\",\"time_diff\"]]\n",
    "        test_val_reply_features[['reply_followers','time_diff']] = scaler_replies.transform(test_val_reply_features[['reply_followers','time_diff']])\n",
    "        \n",
    "        test_val_reply_embeddings = np.array(test_val_df_replies['reply_embeddings_avg'].tolist())\n",
    "        x4 = np.concatenate((test_val_reply_features, test_val_reply_embeddings), axis=1)\n",
    "        \n",
    "        \n",
    "        # Mapping post ids\n",
    "        post_map = {value: i for i, value in enumerate(pd.concat([train[['id']],test_val_df_posts[['id']]])['id'].unique())}\n",
    "        df_replies_edges = pd.concat([self.df_replies[self.df_replies.id.isin(np.array(train.id))][[\"id\", \"reply_user_id\"]],\\\n",
    "                                test_val_df_replies[[\"id\", \"reply_user_id\"]]])\n",
    "        \n",
    "        df_replies_edges[\"id\"] = df_replies_edges['id'].map(post_map).astype(int)\n",
    "        \n",
    "        # Mapping reply user ids\n",
    "        reply_user_map = {value: i for i, value in enumerate(df_replies_edges['reply_user_id'].unique())}\n",
    "        df_replies_edges[\"reply_user_id\"] = df_replies_edges[\"reply_user_id\"].map(reply_user_map)\n",
    "\n",
    "        return train,test_val_df_posts,df_replies_edges,x1,x2,x3,x4\n",
    "\n",
    "\n",
    "    def create_heterodata(self,train,test_val_df_posts,df_replies_edges,x1,x2,x3,x4):\n",
    "\n",
    "        y = pd.concat([train['rumour'],test_val_df_posts['rumour']]).to_numpy()\n",
    "        edge_index = df_replies_edges.values.transpose()\n",
    "        x = np.concatenate((x1,x3))\n",
    "        x_reply = np.concatenate((x2,x4))\n",
    "            \n",
    "        num_rows = x.shape[0]\n",
    "        train_mask = np.zeros(num_rows, dtype=bool)\n",
    "        val_mask = np.zeros(num_rows, dtype=bool)\n",
    "        test_mask = np.zeros(num_rows, dtype=bool)\n",
    "        train_mask[:-x3.shape[0]]=True\n",
    "        val_mask[-x3.shape[0]:-int(x3.shape[0]/2)]=True\n",
    "        test_mask[-int(x3.shape[0]/2):]=True\n",
    "            \n",
    "        data = HeteroData()\n",
    "        data['id'].x = torch.tensor(x, dtype=torch.float32)\n",
    "        data['id'].y =  torch.from_numpy(y)\n",
    "        data['id'].train_mask = torch.tensor(train_mask)\n",
    "        data['id'].val_mask = torch.tensor(val_mask) \n",
    "        data['id'].test_mask = torch.tensor(test_mask)\n",
    "        data['reply_user_id'].x = torch.tensor(x_reply, dtype=torch.float32)\n",
    "        data['id', 'retweet', 'reply_user_id'].edge_index = torch.from_numpy(edge_index.reshape(2,len(x_reply)))\n",
    "        data = T.ToUndirected()(data)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def process(self):\n",
    "            \n",
    "        self.load_data()\n",
    "        train,test_val_df_posts,df_replies_edges,x1,x2,x3,x4 = self.process_data()\n",
    "        return self.create_heterodata(train,test_val_df_posts,df_replies_edges,x1,x2,x3,x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37248a22-d94c-4ee9-8cc9-14c7ee47df1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5639/450977190.py:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_val_reply_features[['reply_followers','time_diff']] = scaler_replies.transform(test_val_reply_features[['reply_followers','time_diff']])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usage\n",
    "file_path_replies = r\"/home/azureuser/rumour-detection-pheme/replies_charlie_hebdo.pkl\"\n",
    "file_path_posts = r\"/home/azureuser/rumour-detection-pheme/posts_charlie_hebdo.pkl\"\n",
    "time_cut =12\n",
    "\n",
    "processor = HeteroDataProcessorFilterNodeonTestV2(file_path_replies, file_path_posts, time_cut_replies=10,time_cut_posts=50)\n",
    "data = processor.process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e91c489-6070-4f7a-8903-4d9ae0884310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  id={\n",
       "    x=[1430, 106],\n",
       "    y=[1430],\n",
       "    train_mask=[1430],\n",
       "    val_mask=[1430],\n",
       "    test_mask=[1430],\n",
       "  },\n",
       "  reply_user_id={ x=[13286, 104] },\n",
       "  (id, retweet, reply_user_id)={ edge_index=[2, 13286] },\n",
       "  (reply_user_id, rev_retweet, id)={ edge_index=[2, 13286] }\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7633d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadRumoursDatasetFilterNodeV2:\n",
    "    def __init__(self, file_path_replies, file_path_posts, time_cut_replies=15,time_cut_posts=15):\n",
    "        self.file_path_replies = file_path_replies\n",
    "        self.file_path_posts = file_path_posts\n",
    "        self.time_cut_replies = time_cut_replies\n",
    "        self.time_cut_posts = time_cut_posts\n",
    "        self.df_replies = None\n",
    "        self.df_posts = None\n",
    "        self.df_final = None\n",
    "\n",
    "    def load_data(self):\n",
    "        self.df_replies = pd.read_pickle(self.file_path_replies)\n",
    "        self.df_posts = pd.read_pickle(self.file_path_posts)\n",
    "\n",
    "    def process_data(self):\n",
    "        post_features = ['followers','favorite_count','retweet_count','verified','rumour','id','embeddings_avg']\n",
    "        \n",
    "        \n",
    "        self.df_replies['min_since_fst_post'] = round((self.df_replies['time'] - self.df_replies['time'].min())\\\n",
    "                        .dt.total_seconds() / 60,2)\n",
    "        \n",
    "        reply_features = ['reply_followers','reply_user_id','reply_verified','time_diff','reply_embeddings_avg',\\\n",
    "                          'min_since_fst_post','id','time']\n",
    "\n",
    "        filtered_replies = self.df_replies[reply_features][(self.df_replies.time_diff <= self.time_cut_replies)&\\\n",
    "                                                           (self.df_replies.min_since_fst_post <= self.time_cut_posts)]\n",
    "        \n",
    "        grouped_replies = filtered_replies.groupby(['id']).agg(\n",
    "            replies=('time_diff', 'count'),\n",
    "            first_time_diff=('time_diff', 'first')\n",
    "        ).reset_index()\n",
    "\n",
    "        self.df_posts = self.df_posts[post_features]\n",
    "        self.df_final = self.df_posts.merge(grouped_replies, on=\"id\", how=\"inner\")\n",
    "        self.df_final['replies'] = self.df_final['replies'].fillna(0)\n",
    "        self.df_final['first_time_diff'] = self.df_final['first_time_diff'].fillna(0)\n",
    "        #self.df_final = self.df_final.drop(columns=['id'])\n",
    "\n",
    "        \n",
    "        # Initialize the Robust Scaler\n",
    "        scaler = RobustScaler()\n",
    "        \n",
    "        # Assuming data is a DataFrame containing your dataset\n",
    "        scaled_features = ['followers', 'favorite_count', 'retweet_count', 'first_time_diff']\n",
    "        # Convert the scaled features back to a DataFrame\n",
    "        scaled_data = pd.DataFrame(scaler.fit_transform(self.df_final [scaled_features]),columns=scaled_features)    \n",
    "        \n",
    "        self.df_final [scaled_features] = scaled_data\n",
    "\n",
    "\n",
    "        # One-hot encoding\n",
    "        self.df_final ['verified'] = self.df_final ['verified'].astype('str').str.\\\n",
    "                     replace(' ', '').replace('True', '1').replace('False', '0')\\\n",
    "                     .astype('int64')\n",
    "        \n",
    "        self.df_final  = pd.concat([self.df_final , pd.get_dummies(\\\n",
    "                                  self.df_final [\"verified\"],dtype=int)], axis=1, join='inner')\n",
    "        self.df_final .drop([\"verified\"], axis=1, inplace=True)\n",
    "        self.df_final .rename(columns={1:'verified',0:'no_verified'},inplace=True)\n",
    "\n",
    "    def get_final_dataframe(self):\n",
    "        return self.df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c30feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "file_path_replies = r\"/home/azureuser/rumour-detection-pheme/replies_charlie_hebdo.pkl\"\n",
    "file_path_posts = r\"/home/azureuser/rumour-detection-pheme/posts_charlie_hebdo.pkl\"\n",
    "time_cut = 1e10\n",
    "\n",
    "processor = LoadRumoursDatasetFilterNodeV2(file_path_replies, file_path_posts,  time_cut_replies=10,time_cut_posts=50)\n",
    "processor.load_data()\n",
    "processor.process_data()\n",
    "df_final = processor.get_final_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20997651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>followers</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>rumour</th>\n",
       "      <th>id</th>\n",
       "      <th>embeddings_avg</th>\n",
       "      <th>replies</th>\n",
       "      <th>first_time_diff</th>\n",
       "      <th>no_verified</th>\n",
       "      <th>verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.007584</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>1.648649</td>\n",
       "      <td>1</td>\n",
       "      <td>552783667052167168</td>\n",
       "      <td>[-0.12335950043052435, -0.055849663292368255, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.189091</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.287235</td>\n",
       "      <td>-0.531646</td>\n",
       "      <td>-0.291892</td>\n",
       "      <td>1</td>\n",
       "      <td>552783745565347840</td>\n",
       "      <td>[-0.1364929385483265, -0.07159566258390744, -0...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.680000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.265037</td>\n",
       "      <td>-0.531646</td>\n",
       "      <td>-0.410811</td>\n",
       "      <td>1</td>\n",
       "      <td>552784168849907712</td>\n",
       "      <td>[-0.045377860377941816, -0.20127306692302227, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.101818</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.911401</td>\n",
       "      <td>-0.101266</td>\n",
       "      <td>1.248649</td>\n",
       "      <td>1</td>\n",
       "      <td>552784526955806720</td>\n",
       "      <td>[-0.03706469060853124, -0.1309182441327721, -0...</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.472727</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.213457</td>\n",
       "      <td>0.303797</td>\n",
       "      <td>1.778378</td>\n",
       "      <td>1</td>\n",
       "      <td>552784882687299584</td>\n",
       "      <td>[-0.003344586119055748, 0.11169316650678714, 0...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>-0.098331</td>\n",
       "      <td>4.329114</td>\n",
       "      <td>1.205405</td>\n",
       "      <td>0</td>\n",
       "      <td>552793409438904321</td>\n",
       "      <td>[-0.1536131378961727, 0.25495246875410277, 0.0...</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.298182</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>9.406947</td>\n",
       "      <td>2.835443</td>\n",
       "      <td>2.783784</td>\n",
       "      <td>0</td>\n",
       "      <td>552794286849536001</td>\n",
       "      <td>[-0.21957818283276123, -0.17643718150528995, 0...</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.378182</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1.212691</td>\n",
       "      <td>0.151899</td>\n",
       "      <td>0.356757</td>\n",
       "      <td>0</td>\n",
       "      <td>552794514424102913</td>\n",
       "      <td>[0.21425410360097885, -0.1142502959817648, 0.1...</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.341818</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>-0.096839</td>\n",
       "      <td>1.772152</td>\n",
       "      <td>0.681081</td>\n",
       "      <td>0</td>\n",
       "      <td>552794995942772736</td>\n",
       "      <td>[0.10138966764012973, -0.06978333741426468, 0....</td>\n",
       "      <td>1</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.517320</td>\n",
       "      <td>-0.050633</td>\n",
       "      <td>0.118919</td>\n",
       "      <td>0</td>\n",
       "      <td>552795056080691200</td>\n",
       "      <td>[0.2523706691960494, -0.2678595017641783, 0.05...</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.058182</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    followers  favorite_count  retweet_count  rumour                  id  \\\n",
       "0   -0.007584        0.050633       1.648649       1  552783667052167168   \n",
       "1    0.287235       -0.531646      -0.291892       1  552783745565347840   \n",
       "2    0.265037       -0.531646      -0.410811       1  552784168849907712   \n",
       "3    0.911401       -0.101266       1.248649       1  552784526955806720   \n",
       "4    1.213457        0.303797       1.778378       1  552784882687299584   \n",
       "..        ...             ...            ...     ...                 ...   \n",
       "74  -0.098331        4.329114       1.205405       0  552793409438904321   \n",
       "75   9.406947        2.835443       2.783784       0  552794286849536001   \n",
       "76   1.212691        0.151899       0.356757       0  552794514424102913   \n",
       "77  -0.096839        1.772152       0.681081       0  552794995942772736   \n",
       "78   0.517320       -0.050633       0.118919       0  552795056080691200   \n",
       "\n",
       "                                       embeddings_avg  replies  \\\n",
       "0   [-0.12335950043052435, -0.055849663292368255, ...        1   \n",
       "1   [-0.1364929385483265, -0.07159566258390744, -0...        2   \n",
       "2   [-0.045377860377941816, -0.20127306692302227, ...        2   \n",
       "3   [-0.03706469060853124, -0.1309182441327721, -0...        4   \n",
       "4   [-0.003344586119055748, 0.11169316650678714, 0...        8   \n",
       "..                                                ...      ...   \n",
       "74  [-0.1536131378961727, 0.25495246875410277, 0.0...        9   \n",
       "75  [-0.21957818283276123, -0.17643718150528995, 0...       16   \n",
       "76  [0.21425410360097885, -0.1142502959817648, 0.1...        5   \n",
       "77  [0.10138966764012973, -0.06978333741426468, 0....        1   \n",
       "78  [0.2523706691960494, -0.2678595017641783, 0.05...        3   \n",
       "\n",
       "    first_time_diff  no_verified  verified  \n",
       "0          2.189091            0         1  \n",
       "1          1.680000            0         1  \n",
       "2         -0.101818            0         1  \n",
       "3         -0.472727            0         1  \n",
       "4          0.000000            0         1  \n",
       "..              ...          ...       ...  \n",
       "74        -0.298182            1         0  \n",
       "75        -0.378182            0         1  \n",
       "76        -0.341818            0         1  \n",
       "77         1.818182            1         0  \n",
       "78        -0.058182            0         1  \n",
       "\n",
       "[79 rows x 10 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mastering",
   "language": "python",
   "name": "mastering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
