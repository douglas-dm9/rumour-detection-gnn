{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec2cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "import pickle\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9df83fc-2853-428d-9e7c-6633bd8ddf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadRumoursDataset:\n",
    "    def __init__(self, file_path_replies, file_path_posts, time_cut):\n",
    "        self.file_path_replies = file_path_replies\n",
    "        self.file_path_posts = file_path_posts\n",
    "        self.time_cut = time_cut\n",
    "        self.df_replies = None\n",
    "        self.df_posts = None\n",
    "        self.df_final = None\n",
    "\n",
    "    def load_data(self):\n",
    "        self.df_replies = pd.read_pickle(self.file_path_replies)\n",
    "        self.df_posts = pd.read_pickle(self.file_path_posts)\n",
    "\n",
    "    def process_data(self):\n",
    "        post_features = ['followers','favorite_count','retweet_count','verified','rumour','id','embeddings_avg']\n",
    "        \n",
    "        reply_features = ['reply_followers','reply_user_id','reply_verified','time_diff','reply_embeddings_avg','id']\n",
    "\n",
    "        filtered_replies = self.df_replies[reply_features][self.df_replies.time_diff < self.time_cut]\n",
    "        grouped_replies = filtered_replies.groupby(['id']).agg(\n",
    "            replies=('time_diff', 'count'),\n",
    "            first_time_diff=('time_diff', 'first')\n",
    "        ).reset_index()\n",
    "\n",
    "        self.df_posts = self.df_posts[post_features]\n",
    "        self.df_final = self.df_posts.merge(grouped_replies, on=\"id\", how=\"left\")\n",
    "        self.df_final['replies'] = self.df_final['replies'].fillna(0)\n",
    "        self.df_final['first_time_diff'] = self.df_final['first_time_diff'].fillna(0)\n",
    "        self.df_final = self.df_final.drop(columns=['id'])\n",
    "\n",
    "        \n",
    "        # Initialize the Robust Scaler\n",
    "        scaler = RobustScaler()\n",
    "        \n",
    "        # Assuming data is a DataFrame containing your dataset\n",
    "        scaled_features = ['followers', 'favorite_count', 'retweet_count', 'first_time_diff']\n",
    "        # Convert the scaled features back to a DataFrame\n",
    "        scaled_data = pd.DataFrame(scaler.fit_transform(self.df_final [scaled_features]),columns=scaled_features)    \n",
    "        \n",
    "        self.df_final [scaled_features] = scaled_data\n",
    "\n",
    "\n",
    "        # One-hot encoding\n",
    "        self.df_final ['verified'] = self.df_final ['verified'].astype('str').str.\\\n",
    "                     replace(' ', '').replace('True', '1').replace('False', '0')\\\n",
    "                     .astype('int64')\n",
    "        \n",
    "        self.df_final  = pd.concat([self.df_final , pd.get_dummies(\\\n",
    "                                  self.df_final [\"verified\"],dtype=int)], axis=1, join='inner')\n",
    "        self.df_final .drop([\"verified\"], axis=1, inplace=True)\n",
    "        self.df_final .rename(columns={1:'verified',0:'no_verified'},inplace=True)\n",
    "\n",
    "    def get_final_dataframe(self):\n",
    "        return self.df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6308c8a-a7f3-4f8a-8eac-9964ad649b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroDataProcessor:\n",
    "    def __init__(self, file_path_replies, file_path_posts, time_cut=15):\n",
    "        self.file_path_replies = file_path_replies\n",
    "        self.file_path_posts = file_path_posts\n",
    "        self.time_cut = time_cut\n",
    "        self.df_replies = None\n",
    "        self.df_posts = None\n",
    "        self.post_map = None\n",
    "        self.reply_user_map = None\n",
    "\n",
    "    def load_data(self):\n",
    "        self.df_replies = pd.read_pickle(self.file_path_replies)\n",
    "        self.df_posts = pd.read_pickle(self.file_path_posts)\n",
    "\n",
    "    def process_data(self):\n",
    "        post_features = ['followers', 'favorite_count', 'retweet_count', 'verified', 'rumour', 'id', 'embeddings_avg']\n",
    "        reply_features = ['reply_followers', 'reply_user_id', 'reply_verified', 'time_diff', 'reply_embeddings_avg', 'id']\n",
    "\n",
    "        # Filter and group replies\n",
    "        self.df_replies = self.df_replies[reply_features][self.df_replies.time_diff < self.time_cut]\n",
    "        grouped_replies = self.df_replies.groupby(['id']).agg(\n",
    "            replies=('time_diff', 'count'),\n",
    "            first_time_diff=('time_diff', 'first')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Merge posts and replies\n",
    "        self.df_posts = self.df_posts[post_features].merge(grouped_replies, on=\"id\", how=\"left\")\n",
    "        self.df_posts['replies'] = self.df_posts['replies'].fillna(0)\n",
    "        self.df_posts['first_time_diff'] = self.df_posts['first_time_diff'].fillna(0)\n",
    "\n",
    "        # One-hot encoding for verified columns\n",
    "        self.df_posts['verified'] = self.df_posts['verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        self.df_posts = pd.concat([self.df_posts, pd.get_dummies(self.df_posts[\"verified\"], dtype=int)], axis=1)\n",
    "        self.df_posts.drop([\"verified\"], axis=1, inplace=True)\n",
    "        self.df_posts.rename(columns={1: 'verified', 0: 'no_verified'}, inplace=True)\n",
    "\n",
    "        self.df_replies['reply_verified'] = self.df_replies['reply_verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        self.df_replies = pd.concat([self.df_replies, pd.get_dummies(self.df_replies[\"reply_verified\"], dtype=int)], axis=1)\n",
    "        self.df_replies.drop([\"reply_verified\"], axis=1, inplace=True)\n",
    "        self.df_replies.rename(columns={1: 'reply_verified', 0: 'reply_no_verified'}, inplace=True)\n",
    "\n",
    "        # Mapping post ids\n",
    "        self.post_map = {value: i for i, value in enumerate(self.df_posts['id'].unique())}\n",
    "        self.df_replies[\"id\"] = self.df_replies['id'].map(self.post_map).astype(int)\n",
    "\n",
    "        # Mapping reply user ids\n",
    "        self.reply_user_map = {value: i for i, value in enumerate(self.df_replies['reply_user_id'].unique())}\n",
    "        self.df_replies[\"reply_user_id\"] = self.df_replies[\"reply_user_id\"].map(self.reply_user_map)\n",
    "\n",
    "    def create_features(self):\n",
    "        post_features = self.df_posts[[\"followers\", \"favorite_count\", \"retweet_count\", \"no_verified\", \"verified\", \"first_time_diff\"]]\n",
    "\n",
    "\n",
    "        # Initialize the Robust Scaler\n",
    "        scaler = RobustScaler()\n",
    "        \n",
    "        # Assuming data is a DataFrame containing your dataset\n",
    "        scaled_features = scaler.fit_transform(post_features[['followers', 'favorite_count', 'retweet_count', 'first_time_diff']])\n",
    "        \n",
    "        # Convert the scaled features back to a DataFrame\n",
    "        scaled_data = pd.DataFrame(scaled_features, columns=['followers', 'favorite_count', 'retweet_count', 'first_time_diff'])\n",
    "        \n",
    "        # Add the binary features back to the scaled data\n",
    "        scaled_data['no_verified'] = post_features['no_verified']\n",
    "        scaled_data['verified'] = post_features['verified']\n",
    "        post_features = scaled_data\n",
    "\n",
    "        post_embeddings = np.array(self.df_posts['embeddings_avg'].tolist())\n",
    "        #post_features = self.scaler.fit_transform(post_features)\n",
    "        x1 = np.concatenate((post_features, post_embeddings), axis=1)\n",
    "\n",
    "        scaler = RobustScaler()\n",
    "        reply_features = self.df_replies[[\"reply_followers\", \"reply_no_verified\", \"reply_verified\",\"time_diff\"]]\n",
    "        reply_features[['reply_followers','time_diff']] = scaler.fit_transform(reply_features[['reply_followers','time_diff']])\n",
    "\n",
    "        reply_embeddings = np.array(self.df_replies['reply_embeddings_avg'].tolist())\n",
    "        #reply_features = self.scaler.transform(reply_features)\n",
    "        x2 = np.concatenate((reply_features, reply_embeddings), axis=1)\n",
    "\n",
    "        return x1, x2\n",
    "\n",
    "    def create_heterodata(self, x1, x2):\n",
    "        y = self.df_posts['rumour'].to_numpy()\n",
    "        edge_index = self.df_replies[[\"id\", \"reply_user_id\"]].values.transpose()\n",
    "\n",
    "        num_rows = x1.shape[0]\n",
    "        indices = np.arange(num_rows)\n",
    "        np.random.shuffle(indices)\n",
    "        train_end = int(0.70 * num_rows)\n",
    "        val_end = train_end + int(0.15 * num_rows)\n",
    "        train_indices, val_indices, test_indices = indices[:train_end], indices[train_end:val_end], indices[val_end:]\n",
    "\n",
    "        train_mask = np.zeros(num_rows, dtype=bool)\n",
    "        val_mask = np.zeros(num_rows, dtype=bool)\n",
    "        test_mask = np.zeros(num_rows, dtype=bool)\n",
    "        train_mask[train_indices], val_mask[val_indices], test_mask[test_indices] = True, True, True\n",
    "\n",
    "        data = HeteroData()\n",
    "        data['id'].x = torch.tensor(x1, dtype=torch.float32)\n",
    "        data['id'].y =  torch.from_numpy(y)\n",
    "        data['id'].train_mask = torch.tensor(train_mask)\n",
    "        data['id'].val_mask = torch.tensor(val_mask) \n",
    "        data['id'].test_mask = torch.tensor(test_mask)\n",
    "        data['reply_user_id'].x = torch.tensor(x2, dtype=torch.float32)\n",
    "        data['id', 'retweet', 'reply_user_id'].edge_index = torch.from_numpy(edge_index.reshape(2,len(x2)))\n",
    "        data = T.ToUndirected()(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def process(self):\n",
    "        self.load_data()\n",
    "        self.process_data()\n",
    "        x1, x2 = self.create_features()\n",
    "        return self.create_heterodata(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9391a49b-b84d-46dc-8373-d1ab1cdb358c",
   "metadata": {},
   "source": [
    "#### Tests with ML Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab31227a-b2c0-47e2-aad4-87f6974c90c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "file_path_replies = r\"/workspaces/rumour-detection-pheme/replies_charlie_hebdo.pkl\"\n",
    "file_path_posts = r\"/workspaces/rumour-detection-pheme/posts_charlie_hebdo.pkl\"\n",
    "time_cut = 1e10\n",
    "\n",
    "processor = LoadRumoursDataset(file_path_replies, file_path_posts, time_cut)\n",
    "processor.load_data()\n",
    "processor.process_data()\n",
    "df_final = processor.get_final_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58b9d37a-6426-4f78-a1ec-706617e6f19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>followers</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>rumour</th>\n",
       "      <th>embeddings_avg</th>\n",
       "      <th>replies</th>\n",
       "      <th>first_time_diff</th>\n",
       "      <th>no_verified</th>\n",
       "      <th>verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.073004</td>\n",
       "      <td>-0.532290</td>\n",
       "      <td>-0.160000</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.12270056130364537, 0.01583862374536693, -0...</td>\n",
       "      <td>5</td>\n",
       "      <td>3.510619</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.031065</td>\n",
       "      <td>-0.344423</td>\n",
       "      <td>1.293333</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.12335950043052435, -0.055849663292368255, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999077</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.356672</td>\n",
       "      <td>-0.524462</td>\n",
       "      <td>-0.302222</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.1364929385483265, -0.07159566258390744, -0...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.740536</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.332156</td>\n",
       "      <td>-0.524462</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.045377860377941816, -0.20127306692302227, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.164358</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.046022</td>\n",
       "      <td>-0.391389</td>\n",
       "      <td>0.964444</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.03706469060853124, -0.1309182441327721, -0...</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.352724</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>3.895910</td>\n",
       "      <td>-0.140900</td>\n",
       "      <td>-0.124444</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.21622000262141228, -0.15450449846684933, -0...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.204986</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>10.452911</td>\n",
       "      <td>3.859100</td>\n",
       "      <td>5.213333</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.21485890651291067, 0.03315381561829285, -0....</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.245614</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.012322</td>\n",
       "      <td>1.189824</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.08846200071275234, -0.1485882457345724, 0.1...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.500462</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>3.631498</td>\n",
       "      <td>0.477495</td>\n",
       "      <td>1.075556</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.021962551607025996, -0.019428667094972398, ...</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.271468</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>0.952113</td>\n",
       "      <td>-0.320939</td>\n",
       "      <td>-0.275556</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.11712110787630081, -0.1469299958811866, -0....</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.049861</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2002 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      followers  favorite_count  retweet_count  rumour  \\\n",
       "0     -0.073004       -0.532290      -0.160000       1   \n",
       "1      0.031065       -0.344423       1.293333       1   \n",
       "2      0.356672       -0.524462      -0.302222       1   \n",
       "3      0.332156       -0.524462      -0.400000       1   \n",
       "4      1.046022       -0.391389       0.964444       1   \n",
       "...         ...             ...            ...     ...   \n",
       "1997   3.895910       -0.140900      -0.124444       0   \n",
       "1998  10.452911        3.859100       5.213333       0   \n",
       "1999   0.012322        1.189824       0.955556       0   \n",
       "2000   3.631498        0.477495       1.075556       0   \n",
       "2001   0.952113       -0.320939      -0.275556       0   \n",
       "\n",
       "                                         embeddings_avg  replies  \\\n",
       "0     [-0.12270056130364537, 0.01583862374536693, -0...        5   \n",
       "1     [-0.12335950043052435, -0.055849663292368255, ...        5   \n",
       "2     [-0.1364929385483265, -0.07159566258390744, -0...        5   \n",
       "3     [-0.045377860377941816, -0.20127306692302227, ...        3   \n",
       "4     [-0.03706469060853124, -0.1309182441327721, -0...       10   \n",
       "...                                                 ...      ...   \n",
       "1997  [0.21622000262141228, -0.15450449846684933, -0...        8   \n",
       "1998  [0.21485890651291067, 0.03315381561829285, -0....        9   \n",
       "1999  [0.08846200071275234, -0.1485882457345724, 0.1...        9   \n",
       "2000  [0.021962551607025996, -0.019428667094972398, ...       18   \n",
       "2001  [0.11712110787630081, -0.1469299958811866, -0....        5   \n",
       "\n",
       "      first_time_diff  no_verified  verified  \n",
       "0            3.510619            1         0  \n",
       "1            0.999077            0         1  \n",
       "2            0.740536            0         1  \n",
       "3           -0.164358            0         1  \n",
       "4           -0.352724            0         1  \n",
       "...               ...          ...       ...  \n",
       "1997         0.204986            0         1  \n",
       "1998        -0.245614            0         1  \n",
       "1999         0.500462            1         0  \n",
       "2000        -0.271468            0         1  \n",
       "2001        -0.049861            0         1  \n",
       "\n",
       "[2002 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7cfbc8-476c-4634-ada2-88f1a0949532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "#mlflow.set_experiment(\"spyder-experiment\")\n",
    "import mlflow.pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c14c5974-5e2a-4b75-bd19-a81d8ed46957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/09 01:40:35 INFO mlflow.tracking.fluent: Experiment with name 'GAT_test' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/workspaces/rumour-detection-pheme/mlruns/3', creation_time=1723167635045, experiment_id='3', last_update_time=1723167635045, lifecycle_stage='active', name='GAT_test', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"GAT_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9863bea5-b581-4148-9a88-22916271258b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26501/2019559920.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reply_features[['reply_followers','time_diff']] = scaler.fit_transform(reply_features[['reply_followers','time_diff']])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usage\n",
    "file_path_replies = r\"/workspaces/rumour-detection-pheme/replies_charlie_hebdo.pkl\"\n",
    "file_path_posts = r\"/workspaces/rumour-detection-pheme/posts_charlie_hebdo.pkl\"\n",
    "time_cut =60\n",
    "\n",
    "processor = HeteroDataProcessor(file_path_replies, file_path_posts, time_cut)\n",
    "data = processor.process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff5d4cbf-bdd3-4f53-9416-4aa5b1b06865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  id={\n",
       "    x=[2002, 106],\n",
       "    y=[2002],\n",
       "    train_mask=[2002],\n",
       "    val_mask=[2002],\n",
       "    test_mask=[2002],\n",
       "  },\n",
       "  reply_user_id={ x=[14440, 104] },\n",
       "  (id, retweet, reply_user_id)={ edge_index=[2, 14440] },\n",
       "  (reply_user_id, rev_retweet, id)={ edge_index=[2, 14440] }\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a63b9204-0fbe-41ce-abf2-16bbb67429b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 | Train Loss: 0.7454 | Train Acc: 74.95% | Val Acc: 76.33%\n",
      "Epoch:  50 | Train Loss: 0.4059 | Train Acc: 81.87% | Val Acc: 80.67%\n",
      "Epoch: 100 | Train Loss: 0.3335 | Train Acc: 86.22% | Val Acc: 84.67%\n",
      "Epoch: 150 | Train Loss: 0.2636 | Train Acc: 89.22% | Val Acc: 85.67%\n",
      "Epoch: 200 | Train Loss: 0.2193 | Train Acc: 92.36% | Val Acc: 85.00%\n",
      "Epoch: 250 | Train Loss: 0.1708 | Train Acc: 94.36% | Val Acc: 85.33%\n",
      "Epoch: 300 | Train Loss: 0.1442 | Train Acc: 95.93% | Val Acc: 85.67%\n",
      "Epoch: 350 | Train Loss: 0.1172 | Train Acc: 96.79% | Val Acc: 85.67%\n",
      "Epoch: 400 | Train Loss: 0.0940 | Train Acc: 97.57% | Val Acc: 85.67%\n",
      "Epoch: 450 | Train Loss: 0.0785 | Train Acc: 97.79% | Val Acc: 86.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/09 01:52:48 WARNING mlflow.utils.requirements_utils: Found torch version (2.3.1+cpu) contains a local version label (+cpu). MLflow logged a pip requirement for this package as 'torch==2.3.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 86.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/09 01:52:52 WARNING mlflow.utils.requirements_utils: Found torch version (2.3.1+cpu) contains a local version label (+cpu). MLflow logged a pip requirement for this package as 'torch==2.3.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/08/09 01:52:52 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, to_hetero\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv((-1, -1), dim_h, add_self_loops=False)\n",
    "        self.conv2 = GATConv(dim_h, dim_h, add_self_loops=False)  # Added second GATConv layer\n",
    "        self.linear = nn.Linear(dim_h, dim_out)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index).relu()\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h, edge_index).relu()  # Pass through the second GATConv layer\n",
    "        h = self.dropout(h)\n",
    "        h = self.linear(h)\n",
    "        return h\n",
    "\n",
    "model = GAT(dim_h=64, dim_out=2)\n",
    "model = to_hetero(model, data.metadata(), aggr='sum')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data, model = data.to(device), model.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(mask):\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict)['id'].argmax(dim=-1)\n",
    "    acc = (pred[mask] == data['id'].y[mask]).sum() / mask.sum()\n",
    "    return float(acc)\n",
    "    \n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "    \n",
    "with mlflow.start_run():\n",
    "\n",
    "    for epoch in range(500):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x_dict, data.edge_index_dict)['id']\n",
    "        mask = data['id'].train_mask\n",
    "        loss = F.cross_entropy(out[mask], data['id'].y[mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if epoch % 50 == 0:\n",
    "            train_acc = test(data['id'].train_mask)\n",
    "            val_acc = test(data['id'].val_mask)\n",
    "            print(f'Epoch: {epoch:>3} | Train Loss: {loss:.4f} | Train Acc: {train_acc*100:.2f}% | Val Acc: {val_acc*100:.2f}%')\n",
    "    \n",
    "         # Log metrics\n",
    "        mlflow.log_metric(\"train_loss\", loss.item(), step=epoch)\n",
    "        mlflow.log_metric(\"train_acc\", train_acc, step=epoch)\n",
    "        mlflow.log_metric(\"val_acc\", val_acc, step=epoch)\n",
    "    \n",
    "    test_acc = test(data['id'].test_mask)\n",
    "    print(f'Test accuracy: {test_acc*100:.2f}%')\n",
    "    \n",
    "    mlflow.log_metric(\"test_acc\", test_acc)\n",
    "    \n",
    "    mlflow.log_param(\"dim_h\", 64)\n",
    "    mlflow.log_param(\"dim_out\", 2)\n",
    "    mlflow.log_param(\"learning_rate\", 0.001)\n",
    "    mlflow.log_param(\"epochs\", 500)\n",
    "    \n",
    "    mlflow.pytorch.log_model(model, \"GAT_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4546081b-0d7b-41d9-af6d-f24956ec73e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8090855457227139"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mask = data['id'].test_mask\n",
    "pred = model(data.x_dict, data.edge_index_dict)['id'].argmax(dim=-1)\n",
    "true_labels = data['id'].y[test_mask]\n",
    "pred_labels = pred[test_mask]\n",
    "precision_score(true_labels, pred_labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87211ef8-9152-47e7-b737-3b3e10e87d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8147680845950493"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(true_labels, pred_labels, average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
