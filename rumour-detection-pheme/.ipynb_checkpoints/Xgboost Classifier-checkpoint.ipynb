{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cf3cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b0137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,StratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from tqdm import tqdm\n",
    "from utils import LoadRumoursDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import matplotlib.pylab as plt\n",
    "import uuid\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1759fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "#mlflow.set_experiment(\"spyder-experiment\")\n",
    "import mlflow.pytorch\n",
    "mlflow.set_experiment(\"Xgb experiment 2024-08-24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d32ca9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "file_path_replies = r\"/home/azureuser/rumour-detection-pheme/replies_charlie_hebdo.pkl\"\n",
    "file_path_posts = r\"/home/azureuser/rumour-detection-pheme/posts_charlie_hebdo.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a70429a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cut =1e6\n",
    "processor = LoadRumoursDataset(file_path_replies, file_path_posts, time_cut)\n",
    "processor.load_data()\n",
    "processor.process_data()\n",
    "df_final = processor.get_final_dataframe()\n",
    "df_final['id'] = [uuid.uuid4() for _ in range(len(df_final))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa891cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not_rumours = df_final[df_final.rumour==0]\n",
    "\n",
    "n = int((df_final.rumour.sum())*1) // df_not_rumours['replies'].nunique()  # Unique number of values in 'replies' column\n",
    "\n",
    "# Group by 'replies' and sample 'n' rows from each group\n",
    "subset_df_not_rumours= df_not_rumours.sample(n= df_final[df_final.rumour==1].shape[0])\n",
    "\n",
    "#subset_df_not_rumours= df_not_rumours.groupby('replies',group_keys=False).apply(lambda x: x.sample(n=min(len(x), n))).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20f791a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_balanced =pd.concat([subset_df_not_rumours,df_final[df_final.rumour==1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61afe71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_remaining = df_final[(~df_final.id.isin(subset_df_not_rumours.id))&(df_final.rumour==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2c477b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X  = df_final_balanced.drop(columns=['rumour','id'])\n",
    "X = np.hstack([X.drop(columns=['embeddings_avg']).values, np.array(pd.DataFrame(X.embeddings_avg.tolist()))])\n",
    "y =df_final_balanced['rumour']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42,stratify=y,shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "772305fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_all  = df_final_remaining.drop(columns=['rumour','id'])\n",
    "X_test_all = np.hstack([X_test_all.drop(columns=['embeddings_avg']).values, np.array(pd.DataFrame(X_test_all.embeddings_avg.tolist()))])\n",
    "y_test_all =df_final_remaining['rumour']\n",
    "X_test_all = np.concatenate((X_test_all, X_test), axis=0)\n",
    "y_test_all = pd.concat([y_test_all, y_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe2258d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomizedSearchCV\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mstats\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Define the hyperparameter distributions\n",
    "param_dist = {\n",
    "    'max_depth': stats.randint(3, 10),\n",
    "    'learning_rate': stats.uniform(0.01, 0.1),\n",
    "    'subsample': stats.uniform(0.5, 0.5),\n",
    "    'n_estimators':stats.randint(50, 200)\n",
    "}\n",
    "\n",
    "# Create the XGBoost model object\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "# Create the RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(xgb_model, param_distributions=param_dist, n_iter=10, cv=5, scoring='recall')\n",
    "\n",
    "# Fit the RandomizedSearchCV object to the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best set of hyperparameters and the corresponding score\n",
    "print(\"Best set of hyperparameters: \", random_search.best_params_)\n",
    "print(\"Best score: \", random_search.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mastering] *",
   "language": "python",
   "name": "conda-env-mastering-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
