{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ykffzr4H-YMP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 15:21:59.699827: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-13 15:22:00.771231: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-13 15:22:01.083524: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-13 15:22:01.681381: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-13 15:22:01.840997: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-13 15:22:02.980570: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-13 15:22:06.751021: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Create_dataset_classes_with_time_cut.ipynb\n",
      "'Graph Neural Networks.ipynb'\n",
      " LICENSE\n",
      " README.md\n",
      " charlie_hebdo_graph_dataset_node_embeddings.pkl\n",
      " charlie_hebdo_graph_dataset_node_embeddings_id_user_id.pkl\n",
      " charlie_hebdo_graph_dataset_node_embeddings_v2.pkl\n",
      " charlie_hebdo_graph_dataset_reply_node_embeddings.pkl\n",
      " charliehebdo-all-rnr-threads.csv\n",
      " eda.ipynb\n",
      " glove.6B.100d.txt\n",
      " graph_dataset_creation.ipynb\n",
      " posts_charlie_hebdo.pkl\n",
      " replies_charlie_hebdo.pkl\n",
      " test_ml_models.ipynb\n",
      " utils.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9cqGPenz1TVh"
   },
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "file_path = r\"/workspaces/rumour-detection-pheme/charliehebdo-all-rnr-threads.csv\"\n",
    "df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>reply_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>552783238415265792</td>\n",
       "      <td>Breaking: At least 10 dead, 5 injured after tO...</td>\n",
       "      <td>@H_E_Samuel @George_Berridge @michael_taggart ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>552783238415265792</td>\n",
       "      <td>Breaking: At least 10 dead, 5 injured after tO...</td>\n",
       "      <td>@H_E_Samuel Hi Henry would you be willing to g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>552783238415265792</td>\n",
       "      <td>Breaking: At least 10 dead, 5 injured after tO...</td>\n",
       "      <td>@H_E_Samuel @H_E_Samuel please call them terro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>552783238415265792</td>\n",
       "      <td>Breaking: At least 10 dead, 5 injured after tO...</td>\n",
       "      <td>@H_E_Samuel French govt needs to take strict a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>552783238415265792</td>\n",
       "      <td>Breaking: At least 10 dead, 5 injured after tO...</td>\n",
       "      <td>@H_E_Samuel @terrychristian  if only people di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19045</th>\n",
       "      <td>553592195786506240</td>\n",
       "      <td>#BREAKING - Both #CharlieHebdo suspects killed...</td>\n",
       "      <td>RT @FRANCE24: #BREAKING - Both #CharlieHebdo s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19046</th>\n",
       "      <td>553592195786506240</td>\n",
       "      <td>#BREAKING - Both #CharlieHebdo suspects killed...</td>\n",
       "      <td>@FRANCE24 we are coming http://t.co/ilsRarZOUi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19047</th>\n",
       "      <td>553592195786506240</td>\n",
       "      <td>#BREAKING - Both #CharlieHebdo suspects killed...</td>\n",
       "      <td>@FRANCE24: I am  very  happy  to hear  those  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19048</th>\n",
       "      <td>553592195786506240</td>\n",
       "      <td>#BREAKING - Both #CharlieHebdo suspects killed...</td>\n",
       "      <td>@FRANCE24 @France24_en Well done to the French...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19049</th>\n",
       "      <td>553592195786506240</td>\n",
       "      <td>#BREAKING - Both #CharlieHebdo suspects killed...</td>\n",
       "      <td>@FRANCE24 @jenndogg1 Good.  Swift justice!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19050 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                               text  \\\n",
       "0      552783238415265792  Breaking: At least 10 dead, 5 injured after tO...   \n",
       "1      552783238415265792  Breaking: At least 10 dead, 5 injured after tO...   \n",
       "2      552783238415265792  Breaking: At least 10 dead, 5 injured after tO...   \n",
       "3      552783238415265792  Breaking: At least 10 dead, 5 injured after tO...   \n",
       "4      552783238415265792  Breaking: At least 10 dead, 5 injured after tO...   \n",
       "...                   ...                                                ...   \n",
       "19045  553592195786506240  #BREAKING - Both #CharlieHebdo suspects killed...   \n",
       "19046  553592195786506240  #BREAKING - Both #CharlieHebdo suspects killed...   \n",
       "19047  553592195786506240  #BREAKING - Both #CharlieHebdo suspects killed...   \n",
       "19048  553592195786506240  #BREAKING - Both #CharlieHebdo suspects killed...   \n",
       "19049  553592195786506240  #BREAKING - Both #CharlieHebdo suspects killed...   \n",
       "\n",
       "                                              reply_text  \n",
       "0      @H_E_Samuel @George_Berridge @michael_taggart ...  \n",
       "1      @H_E_Samuel Hi Henry would you be willing to g...  \n",
       "2      @H_E_Samuel @H_E_Samuel please call them terro...  \n",
       "3      @H_E_Samuel French govt needs to take strict a...  \n",
       "4      @H_E_Samuel @terrychristian  if only people di...  \n",
       "...                                                  ...  \n",
       "19045  RT @FRANCE24: #BREAKING - Both #CharlieHebdo s...  \n",
       "19046     @FRANCE24 we are coming http://t.co/ilsRarZOUi  \n",
       "19047  @FRANCE24: I am  very  happy  to hear  those  ...  \n",
       "19048  @FRANCE24 @France24_en Well done to the French...  \n",
       "19049         @FRANCE24 @jenndogg1 Good.  Swift justice!  \n",
       "\n",
       "[19050 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['id','text','reply_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "3bAzR4p52Rf8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rumour\n",
       "0    1555\n",
       "1     447\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('rumour')['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "nq86ae2_7l0H"
   },
   "outputs": [],
   "source": [
    "df.time = pd.to_datetime(df.time, format='%a %b %d %H:%M:%S +0000 %Y')\n",
    "df.reply_time = pd.to_datetime(df.reply_time, format='%a %b %d %H:%M:%S +0000 %Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kiU1-cVGPzC"
   },
   "source": [
    "### Time for replies and Number of replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "id": "B0y9FosJ8eU2"
   },
   "outputs": [],
   "source": [
    " df['time_diff']=(df.reply_time - df.time).dt.total_seconds()/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "jVIvMkty9eSs"
   },
   "outputs": [],
   "source": [
    "df['reply_number'] = df.groupby('id')['time_diff'].rank(method='dense')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDFus9zKLhBg"
   },
   "source": [
    "#### Number of replies x Retweet counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "AKxtFOLASzUP"
   },
   "outputs": [],
   "source": [
    "df_posts = df[['id','text','followers','favorite_count','retweet_count','verified',\\\n",
    "  'rumour','user_id']].drop_duplicates().merge(df.groupby(['id']).agg(replies=(\\\n",
    "  'time_diff','count'),first_time_diff=('time_diff','first')).reset_index(),\\\n",
    "  on=\"id\",how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH68TYoeMgYm"
   },
   "source": [
    "#### Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-wXsNYeNuLs",
    "outputId": "540fcdf9-482d-4e1e-ccc5-83d87c85df67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# function for cleaning data\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt\n",
    "\n",
    "\n",
    "def clean_text(\n",
    "    string: str,\n",
    "    punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "    stop_words=english_stopwords) -> str:\n",
    "    \"\"\"\n",
    "    A method to clean text\n",
    "    \"\"\"\n",
    "    # Cleaning the urls\n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "\n",
    "    # Cleaning the html elements\n",
    "    string = re.sub(r'<.*?>', '', string)\n",
    "\n",
    "    # Removing the punctuations\n",
    "    for x in string.lower():\n",
    "        if x in punctuations:\n",
    "            string = string.replace(x, \"\")\n",
    "\n",
    "    # Converting the text to lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Removing stop words\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "IrvGTGi9M2K2"
   },
   "outputs": [],
   "source": [
    "df_posts['clean_text'] = np.vectorize(remove_pattern)(df_posts['text'], \"@[\\w]*\")\n",
    "df_posts['clean_text'] = df_posts['clean_text'].str.replace(\"[^a-zA-Z#]\", \" \").apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "DUqjXD9nZyhH"
   },
   "outputs": [],
   "source": [
    "# create the dictionary\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_posts['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "HnpNJdMFZ7OW"
   },
   "outputs": [],
   "source": [
    "def embedding_vocab(filepath, word_index,embedding_dim):\n",
    "    vocab_size = len(word_index) + 1\n",
    "\n",
    "\n",
    "    embedding_matrix_vocab = np.zeros((vocab_size,\n",
    "                                       embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                embedding_matrix_vocab[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "    return embedding_matrix_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUF6Y9hCcv1M",
    "outputId": "99084bf5-0137-44a0-b8b0-7584a21d87c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-07 00:46:57--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
      "\n",
      "2024-08-07 00:49:36 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
    "#unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "gaWny-EfcIcN"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix_vocab = embedding_vocab(\n",
    "    'glove.6B.100d.txt', tokenizer.word_index,\n",
    "embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "sequences = tokenizer.texts_to_sequences(df_posts['clean_text'])\n",
    "\n",
    "# Padding sequences if necessary\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Function to get embeddings for a sequence\n",
    "def get_embeddings(sequence, embedding_matrix):\n",
    "    embeddings = []\n",
    "    for idx in sequence:\n",
    "        embeddings.append(embedding_matrix[idx])\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Apply function to get embeddings for each sequence\n",
    "df_posts['embeddings'] = [get_embeddings(seq, embedding_matrix_vocab) for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "pycnHmE3zX_o"
   },
   "outputs": [],
   "source": [
    "array_avg = []\n",
    "for i in df_posts.embeddings:\n",
    "  array_avg.append(np.mean(i,axis=0))\n",
    "df_posts['embeddings_avg'] = array_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reply embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_reply_text'] = np.vectorize(remove_pattern)(df['reply_text'], \"@[\\w]*\")\n",
    "df['clean_reply_text'] = df['clean_reply_text'].str.replace(\"[^a-zA-Z#]\", \" \").apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dictionary\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['clean_reply_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix_vocab = embedding_vocab(\n",
    "    'glove.6B.100d.txt', tokenizer.word_index,\n",
    "embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "sequences = tokenizer.texts_to_sequences(df['clean_reply_text'])\n",
    "\n",
    "# Padding sequences if necessary\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Apply function to get embeddings for each sequence\n",
    "df['reply_embeddings'] = [get_embeddings(seq, embedding_matrix_vocab) for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "array_avg = []\n",
    "for i in df.reply_embeddings:\n",
    "  array_avg.append(np.mean(i,axis=0))\n",
    "df['reply_embeddings_avg'] = array_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['reply_embeddings_avg'] = df['reply_embeddings_avg'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving cleaned csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('replies_charlie_hebdo.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.to_pickle(\"posts_charlie_hebdo.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44ZNwTqtKovw"
   },
   "source": [
    "#### Creat Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts = pd.read_pickle(r\"/workspaces/rumour-detection-pheme/posts_charlie_hebdo.pkl\")\n",
    "df_replies = pd.read_pickle(r\"/workspaces/rumour-detection-pheme/replies_charlie_hebdo.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_post_not_rumour = df_posts[df_posts.rumour==0]\n",
    "#total_samples = int(df_posts[df_posts.rumour==1].count()[0]*1.1)\n",
    "\n",
    "# Get the proportion of each value in the 'replies' column\n",
    "#value_counts = df_post_not_rumour['replies'].value_counts(normalize=True)\n",
    "\n",
    "# Calculate the number of samples for each value\n",
    "#samples_per_value = (value_counts * total_samples).round().astype(int)\n",
    "\n",
    "# Sample the required number of rows for each value in 'replies'\n",
    "#sampled_dfs = []\n",
    "#for value, num_samples in samples_per_value.items():\n",
    "#    sampled_dfs.append(df_post_not_rumour[df_post_not_rumour['replies'] \\\n",
    "#                       == value].sample(num_samples, random_state=42))\n",
    "\n",
    "# Concatenate all the samples into a single dataframe\n",
    "#sampled_df = pd.concat(sampled_dfs)\n",
    "# Shuffle the resulting dataframe\n",
    "#sampled_df = sampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "#df_posts = pd.concat([sampled_df,df_posts[df_posts.rumour==1]])\\\n",
    "#             .sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "\n",
    "#df_replies = df_replies[df_replies.id.isin(df_posts.id.values)][['id','reply_id',\\\n",
    "#             'reply_followers','reply_user_id','reply_verified',\"rumour\",\\\n",
    "#              \"user_id\",\"reply_embeddings_avg\",\"time_diff\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yY1jnj9m3ElI"
   },
   "source": [
    "#### Torch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "xY9kjy-URKKx"
   },
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "df_posts['verified'] = df_posts['verified'].astype('str').str.\\\n",
    "             replace(' ', '').replace('True', '1').replace('False', '0')\\\n",
    "             .astype('int64')\n",
    "\n",
    "df_posts = pd.concat([df_posts, pd.get_dummies(\\\n",
    "                          df_posts[\"verified\"],dtype=int)], axis=1, join='inner')\n",
    "df_posts.drop([\"verified\"], axis=1, inplace=True)\n",
    "df_posts.rename(columns={1:'verified',0:'no_verified'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "fDw9YB9qRKB_"
   },
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "df_replies['reply_verified'] = df_replies['reply_verified'].astype('str').str.\\\n",
    "             replace(' ', '').replace('True', '1').replace('False', '0')\\\n",
    "             .astype('int64')\n",
    "\n",
    "df_replies = pd.concat([df_replies, pd.get_dummies(\\\n",
    "                          df_replies[\"reply_verified\"],dtype=int)], axis=1, join='inner')\n",
    "df_replies.drop([\"reply_verified\"], axis=1, inplace=True)\n",
    "df_replies.rename(columns={1:'reply_verified',0:'reply_no_verified'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "Wa1NAPktPoB8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get unique values from the column\n",
    "unique_values = df_posts['id'].unique()\n",
    "\n",
    "# Create a dictionary to map values to IDs\n",
    "value_to_id = {value: i for i, value in enumerate(unique_values)}\n",
    "\n",
    "# Map the values to IDs using the dictionary\n",
    "#retweets_node_features['index'] = retweets_node_features['reply_user_id'].map(value_to_id)\n",
    "post_map  = value_to_id\n",
    "\n",
    "#Only keep features\n",
    "post_features = df_posts[[\"followers\", \"favorite_count\",\"retweet_count\",\"no_verified\",\"verified\",\\\n",
    "                          \"first_time_diff\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "i3VSO_R8xKgy"
   },
   "outputs": [],
   "source": [
    "embeddings_avg = []\n",
    "for embeddings in df_posts['embeddings_avg']:\n",
    "  #embeddings= i.replace('[','').replace(']','').replace('\\n','').split(' ')\n",
    "  #embeddings =  [float(item) for item in embeddings if item != '']\n",
    "  embeddings_avg.append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize the Robust Scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Assuming data is a DataFrame containing your dataset\n",
    "scaled_features = scaler.fit_transform(post_features[['followers', 'favorite_count', 'retweet_count', 'first_time_diff']])\n",
    "\n",
    "# Convert the scaled features back to a DataFrame\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=['followers', 'favorite_count', 'retweet_count', 'first_time_diff'])\n",
    "\n",
    "# Add the binary features back to the scaled data\n",
    "scaled_data['no_verified'] = post_features['no_verified']\n",
    "scaled_data['verified'] = post_features['verified']\n",
    "post_features = scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QnaMBT0yylBn",
    "outputId": "31b11cf1-5402-4ca1-c567-f1c5abfeea86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2002, 106)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to numpy\n",
    "x1 = np.concatenate((post_features.to_numpy(),np.array(embeddings_avg)),axis=1)\n",
    "#x1 = post_features.to_numpy()\n",
    "x1.shape # [num_movie_nodes x movie_node_feature_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "dNbQMYlKzCUi"
   },
   "outputs": [],
   "source": [
    "# Select node features\n",
    "retweets_node_features = df_replies[[\"reply_followers\", \"reply_no_verified\", \"reply_verified\",\"reply_user_id\",\"time_diff\"]]\n",
    "\n",
    "# Get unique values from the column\n",
    "unique_values = retweets_node_features['reply_user_id'].unique()\n",
    "\n",
    "# Create a dictionary to map values to IDs\n",
    "value_to_id = {value: i for i, value in enumerate(unique_values)}\n",
    "\n",
    "# Map the values to IDs using the dictionary\n",
    "#retweets_node_features['index'] = retweets_node_features['reply_user_id'].map(value_to_id)\n",
    "retweets_id_mapping  = value_to_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Only keep features\n",
    "retweets_node_features = retweets_node_features[[\"reply_followers\", \"reply_no_verified\", \"reply_verified\",\"time_diff\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_avg = []\n",
    "for embeddings in df_replies['reply_embeddings_avg']:\n",
    "  #embeddings= i.replace('[','').replace(']','').replace('\\n','').split(' ')\n",
    "  #embeddings =  [float(item) for item in embeddings if item != '']\n",
    "  embeddings_avg.append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Robust Scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Assuming data is a DataFrame containing your dataset\n",
    "scaled_features = scaler.fit_transform(retweets_node_features[['reply_followers','time_diff']])\n",
    "\n",
    "# Convert the scaled features back to a DataFrame\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=['reply_followers','time_diff'])\n",
    "\n",
    "# Add the binary features back to the scaled data\n",
    "scaled_data['reply_no_verified'] = retweets_node_features['reply_no_verified']\n",
    "scaled_data['reply_verified'] = retweets_node_features['reply_verified']\n",
    "retweets_node_features = scaled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19050, 4)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to numpy\n",
    "#x2 = np.concatenate((retweets_node_features.to_numpy(),np.array(embeddings_avg)),axis=1)\n",
    "x2 = retweets_node_features.to_numpy()\n",
    "x2.shape # [num_movie_nodes x movie_node_feature_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYMUpme80rwi",
    "outputId": "fad85d16-324a-47ee-a7e4-a2afd472dffa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2002,)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract labels\n",
    "labels = df_posts.rumour\n",
    "y = labels.to_numpy()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "rsb7ZYVI04R6"
   },
   "outputs": [],
   "source": [
    "# Map post IDs\n",
    "#post_map = posts_id_mapping.reset_index().set_index(\"id\").to_dict()\n",
    "df_replies[\"id\"] = df_replies['id'].map(post_map).astype(int)\n",
    "# Map user IDs\n",
    "#retweets_map = retweets_id_mapping #retweets_id_mapping.reset_index().set_index(\"reply_user_id\").to_dict()\n",
    "df_replies[\"reply_user_id\"] = df_replies[\"reply_user_id\"].map(retweets_id_mapping)#.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zov-Q9429v1",
    "outputId": "a1a9b871-c403-434b-e5b3-45e4feea6544"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,  2001,  2001,  2001],\n",
       "       [    0,     1,     2, ...,   284,  1810, 14464]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = df_replies[[\"id\", \"reply_user_id\"]].values.transpose()\n",
    "edge_index # [2 x num_edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "I8VTjazz6G7U"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Number of rows\n",
    "num_rows = x1.shape[0]\n",
    "\n",
    "# Desired proportions\n",
    "train_proportion = 0.70\n",
    "val_proportion = 0.15\n",
    "test_proportion = 0.15\n",
    "\n",
    "# Generate a list of indices and shuffle them\n",
    "indices = np.arange(num_rows)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Calculate the split indices\n",
    "train_end = int(train_proportion * num_rows)\n",
    "val_end = train_end + int(val_proportion * num_rows)\n",
    "\n",
    "# Split the indices into train, validation, and test\n",
    "train_indices = indices[:train_end]\n",
    "val_indices = indices[train_end:val_end]\n",
    "test_indices = indices[val_end:]\n",
    "\n",
    "# Create masks\n",
    "train_mask = np.zeros(num_rows, dtype=bool)\n",
    "val_mask = np.zeros(num_rows, dtype=bool)\n",
    "test_mask = np.zeros(num_rows, dtype=bool)\n",
    "\n",
    "# Assign True to the corresponding indices\n",
    "train_mask[train_indices] = True\n",
    "val_mask[val_indices] = True\n",
    "test_mask[test_indices] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XyvzeR7IymQ",
    "outputId": "b26eb871-555d-42fd-caff-ad906a7ccf7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  id={\n",
      "    x=[2002, 106],\n",
      "    y=[2002],\n",
      "    train_mask=[2002],\n",
      "    val_mask=[2002],\n",
      "    test_mask=[2002],\n",
      "  },\n",
      "  reply_user_id={ x=[19050, 4] },\n",
      "  (id, retweet, reply_user_id)={ edge_index=[2, 19050] },\n",
      "  (reply_user_id, rev_retweet, id)={ edge_index=[2, 19050] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the HeteroData object\n",
    "data = HeteroData()\n",
    "\n",
    "# Add node features and labels for the 'paper' node type\n",
    "data['id'].x = torch.tensor(x1,dtype=torch.float32)  # Node features dtype=torch.float32\n",
    "data['id'].y = torch.from_numpy(y)  # Node labels (for example, binary classification)\n",
    "data['id'].train_mask = torch.tensor(train_mask)  # Training mask\n",
    "data['id'].val_mask =torch.tensor(val_mask)  # Validation mask\n",
    "data['id'].test_mask = torch.tensor(test_mask)  # Test mask\n",
    "\n",
    "# Add node features for the 'author' node type\n",
    "data['reply_user_id'].x = torch.tensor(x2,dtype=torch.float32) #torch.float32\n",
    "\n",
    "# Add edges for the (id, retweet, reply_D) relation\n",
    "data['id', 'retweet', 'reply_user_id'].edge_index = torch.from_numpy(edge_index.reshape(2,len(x2)))\n",
    "data = T.ToUndirected()(data)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtOzUtI7frmN",
    "outputId": "cedbbb5a-d3a3-46b2-adea-f6ed97e206cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: tensor(307) torch.Size([1401])\n"
     ]
    }
   ],
   "source": [
    "print(\"train:\",data['id'].y[train_mask].sum(),data['id'].y[train_mask].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0hD33th-gFnO",
    "outputId": "0e0f7739-7d7e-49dd-cf0f-8c8221739a32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: tensor(65) torch.Size([300])\n"
     ]
    }
   ],
   "source": [
    "print(\"val:\", data['id'].y[val_mask].sum(),data['id'].y[val_mask].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5_aX0ZogGHn",
    "outputId": "5f4934da-8242-429e-9a90-fa8f256d2b2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: tensor(75) torch.Size([301])\n"
     ]
    }
   ],
   "source": [
    "print(\"test:\", data['id'].y[test_mask].sum(),data['id'].y[test_mask].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('charlie_hebdo_graph_dataset_node_embeddings_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HeteroData object from the pkl file\n",
    "with open('charlie_hebdo_graph_dataset_node_embeddings_v2.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
