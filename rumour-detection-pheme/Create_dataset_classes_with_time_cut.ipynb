{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec2cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "import pickle\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9df83fc-2853-428d-9e7c-6633bd8ddf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadRumoursDatasetFilterNode:\n",
    "    def __init__(self, file_path_replies, file_path_posts, time_cut):\n",
    "        self.file_path_replies = file_path_replies\n",
    "        self.file_path_posts = file_path_posts\n",
    "        self.time_cut = time_cut\n",
    "        self.df_replies = None\n",
    "        self.df_posts = None\n",
    "        self.df_final = None\n",
    "\n",
    "    def load_data(self):\n",
    "        self.df_replies = pd.read_pickle(self.file_path_replies)\n",
    "        self.df_posts = pd.read_pickle(self.file_path_posts)\n",
    "\n",
    "    def process_data(self):\n",
    "        post_features = ['followers','favorite_count','retweet_count','verified','rumour','id','embeddings_avg']\n",
    "        \n",
    "        \n",
    "        self.df_replies['min_since_fst_post'] = round((self.df_replies['time'] - self.df_replies['time'].min())\\\n",
    "                        .dt.total_seconds() / 60,2)\n",
    "        \n",
    "        reply_features = ['reply_followers','reply_user_id','reply_verified','time_diff','reply_embeddings_avg',\\\n",
    "                          'min_since_fst_post','id','time']\n",
    "\n",
    "        filtered_replies = self.df_replies[reply_features][(self.df_replies.time_diff <= self.time_cut)&\\\n",
    "                                                           (self.df_replies.min_since_fst_post <= self.time_cut)]\n",
    "        \n",
    "        grouped_replies = filtered_replies.groupby(['id']).agg(\n",
    "            replies=('time_diff', 'count'),\n",
    "            first_time_diff=('time_diff', 'first')\n",
    "        ).reset_index()\n",
    "\n",
    "        self.df_posts = self.df_posts[post_features]\n",
    "        self.df_final = self.df_posts.merge(grouped_replies, on=\"id\", how=\"inner\")\n",
    "        self.df_final['replies'] = self.df_final['replies'].fillna(0)\n",
    "        self.df_final['first_time_diff'] = self.df_final['first_time_diff'].fillna(0)\n",
    "        #self.df_final = self.df_final.drop(columns=['id'])\n",
    "\n",
    "        \n",
    "        # Initialize the Robust Scaler\n",
    "        scaler = RobustScaler()\n",
    "        \n",
    "        # Assuming data is a DataFrame containing your dataset\n",
    "        scaled_features = ['followers', 'favorite_count', 'retweet_count', 'first_time_diff']\n",
    "        # Convert the scaled features back to a DataFrame\n",
    "        scaled_data = pd.DataFrame(scaler.fit_transform(self.df_final [scaled_features]),columns=scaled_features)    \n",
    "        \n",
    "        self.df_final [scaled_features] = scaled_data\n",
    "\n",
    "\n",
    "        # One-hot encoding\n",
    "        self.df_final ['verified'] = self.df_final ['verified'].astype('str').str.\\\n",
    "                     replace(' ', '').replace('True', '1').replace('False', '0')\\\n",
    "                     .astype('int64')\n",
    "        \n",
    "        self.df_final  = pd.concat([self.df_final , pd.get_dummies(\\\n",
    "                                  self.df_final [\"verified\"],dtype=int)], axis=1, join='inner')\n",
    "        self.df_final .drop([\"verified\"], axis=1, inplace=True)\n",
    "        self.df_final .rename(columns={1:'verified',0:'no_verified'},inplace=True)\n",
    "\n",
    "    def get_final_dataframe(self):\n",
    "        return self.df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ce79ab2-2b35-4d20-812f-44ad60af34dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "file_path_replies = r\"/home/azureuser/rumour-detection-pheme/replies_charlie_hebdo.pkl\"\n",
    "file_path_posts = r\"/home/azureuser/rumour-detection-pheme/posts_charlie_hebdo.pkl\"\n",
    "time_cut = 1e10\n",
    "\n",
    "processor = LoadRumoursDatasetFilterNode(file_path_replies, file_path_posts, time_cut)\n",
    "processor.load_data()\n",
    "processor.process_data()\n",
    "df_final = processor.get_final_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c04f8b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadRumoursDataset:\n",
    "    def __init__(self, file_path_replies, file_path_posts, time_cut):\n",
    "        self.file_path_replies = file_path_replies\n",
    "        self.file_path_posts = file_path_posts\n",
    "        self.time_cut = time_cut\n",
    "        self.df_replies = None\n",
    "        self.df_posts = None\n",
    "        self.df_final = None\n",
    "\n",
    "    def load_data(self):\n",
    "        self.df_replies = pd.read_pickle(self.file_path_replies)\n",
    "        self.df_posts = pd.read_pickle(self.file_path_posts)\n",
    "\n",
    "    def process_data(self):\n",
    "        post_features = ['followers','favorite_count','retweet_count','verified','rumour','id','embeddings_avg']\n",
    "        \n",
    "        reply_features = ['reply_followers','reply_user_id','reply_verified','time_diff','reply_embeddings_avg','id','time']\n",
    "\n",
    "        filtered_replies = self.df_replies[reply_features][self.df_replies.time_diff < self.time_cut]\n",
    "        grouped_replies = filtered_replies.groupby(['id']).agg(\n",
    "            replies=('time_diff', 'count'),\n",
    "            first_time_diff=('time_diff', 'first')\n",
    "        ).reset_index()\n",
    "\n",
    "        self.df_posts = self.df_posts[post_features]\n",
    "        self.df_final = self.df_posts.merge(grouped_replies, on=\"id\", how=\"left\")\n",
    "        self.df_final['replies'] = self.df_final['replies'].fillna(0)\n",
    "        self.df_final['first_time_diff'] = self.df_final['first_time_diff'].fillna(0)\n",
    "        self.df_final = self.df_final.drop(columns=['id'])\n",
    "\n",
    "        \n",
    "        # Initialize the Robust Scaler\n",
    "        scaler = RobustScaler()\n",
    "        \n",
    "        # Assuming data is a DataFrame containing your dataset\n",
    "        scaled_features = ['followers', 'favorite_count', 'retweet_count', 'first_time_diff']\n",
    "        # Convert the scaled features back to a DataFrame\n",
    "        scaled_data = pd.DataFrame(scaler.fit_transform(self.df_final [scaled_features]),columns=scaled_features)    \n",
    "        \n",
    "        self.df_final [scaled_features] = scaled_data\n",
    "\n",
    "\n",
    "        # One-hot encoding\n",
    "        self.df_final ['verified'] = self.df_final ['verified'].astype('str').str.\\\n",
    "                     replace(' ', '').replace('True', '1').replace('False', '0')\\\n",
    "                     .astype('int64')\n",
    "        \n",
    "        self.df_final  = pd.concat([self.df_final , pd.get_dummies(\\\n",
    "                                  self.df_final [\"verified\"],dtype=int)], axis=1, join='inner')\n",
    "        self.df_final .drop([\"verified\"], axis=1, inplace=True)\n",
    "        self.df_final .rename(columns={1:'verified',0:'no_verified'},inplace=True)\n",
    "\n",
    "    def get_final_dataframe(self):\n",
    "        return self.df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6308c8a-a7f3-4f8a-8eac-9964ad649b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroDataProcessorFilterNode:\n",
    "    def __init__(self, file_path_replies, file_path_posts, time_cut=15):\n",
    "        self.file_path_replies = file_path_replies\n",
    "        self.file_path_posts = file_path_posts\n",
    "        self.time_cut = time_cut\n",
    "        self.df_replies = None\n",
    "        self.df_posts = None\n",
    "        self.post_map = None\n",
    "        self.reply_user_map = None\n",
    "\n",
    "    def load_data(self):\n",
    "        self.df_replies = pd.read_pickle(self.file_path_replies)\n",
    "        self.df_posts = pd.read_pickle(self.file_path_posts)\n",
    "\n",
    "    def process_data(self):\n",
    "        post_features = ['followers', 'favorite_count', 'retweet_count', 'verified', 'rumour', 'id', 'embeddings_avg']\n",
    "        reply_features = ['reply_followers', 'reply_user_id', 'reply_verified', 'time_diff', 'reply_embeddings_avg', 'id']\n",
    "\n",
    "        # Filter and group replies\n",
    "        self.df_replies['min_since_fst_post'] = round((self.df_replies['time'] - self.df_replies['time'].min())\\\n",
    "                        .dt.total_seconds() / 60,2)\n",
    "        \n",
    "        self.df_replies = self.df_replies[reply_features][(self.df_replies.time_diff <= self.time_cut)&\\\n",
    "                                                           (self.df_replies.min_since_fst_post <= self.time_cut)]\n",
    "        grouped_replies = self.df_replies.groupby(['id']).agg(\n",
    "            replies=('time_diff', 'count'),\n",
    "            first_time_diff=('time_diff', 'first')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Merge posts and replies\n",
    "        self.df_posts = self.df_posts[post_features].merge(grouped_replies, on=\"id\", how=\"inner\")\n",
    "        self.df_posts['replies'] = self.df_posts['replies'].fillna(0)\n",
    "        self.df_posts['first_time_diff'] = self.df_posts['first_time_diff'].fillna(0)\n",
    "\n",
    "        # One-hot encoding for verified columns\n",
    "        self.df_posts['verified'] = self.df_posts['verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        self.df_posts = pd.concat([self.df_posts, pd.get_dummies(self.df_posts[\"verified\"], dtype=int)], axis=1)\n",
    "        self.df_posts.drop([\"verified\"], axis=1, inplace=True)\n",
    "        self.df_posts.rename(columns={1: 'verified', 0: 'no_verified'}, inplace=True)\n",
    "\n",
    "        self.df_replies['reply_verified'] = self.df_replies['reply_verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        self.df_replies = pd.concat([self.df_replies, pd.get_dummies(self.df_replies[\"reply_verified\"], dtype=int)], axis=1)\n",
    "        self.df_replies.drop([\"reply_verified\"], axis=1, inplace=True)\n",
    "        self.df_replies.rename(columns={1: 'reply_verified', 0: 'reply_no_verified'}, inplace=True)\n",
    "\n",
    "        # Mapping post ids\n",
    "        self.post_map = {value: i for i, value in enumerate(self.df_posts['id'].unique())}\n",
    "        self.df_replies[\"id\"] = self.df_replies['id'].map(self.post_map).astype(int)\n",
    "\n",
    "        # Mapping reply user ids\n",
    "        self.reply_user_map = {value: i for i, value in enumerate(self.df_replies['reply_user_id'].unique())}\n",
    "        self.df_replies[\"reply_user_id\"] = self.df_replies[\"reply_user_id\"].map(self.reply_user_map)\n",
    "\n",
    "    def create_features(self):\n",
    "        post_features = self.df_posts[[\"followers\", \"favorite_count\", \"retweet_count\", \"no_verified\", \"verified\", \"first_time_diff\"]]\n",
    "\n",
    "\n",
    "        # Initialize the Robust Scaler\n",
    "        scaler = RobustScaler()\n",
    "        \n",
    "        # Assuming data is a DataFrame containing your dataset\n",
    "        scaled_features = scaler.fit_transform(post_features[['followers', 'favorite_count', 'retweet_count', 'first_time_diff']])\n",
    "        \n",
    "        # Convert the scaled features back to a DataFrame\n",
    "        scaled_data = pd.DataFrame(scaled_features, columns=['followers', 'favorite_count', 'retweet_count', 'first_time_diff'])\n",
    "        \n",
    "        # Add the binary features back to the scaled data\n",
    "        scaled_data['no_verified'] = post_features['no_verified']\n",
    "        scaled_data['verified'] = post_features['verified']\n",
    "        post_features = scaled_data\n",
    "\n",
    "        post_embeddings = np.array(self.df_posts['embeddings_avg'].tolist())\n",
    "        #post_features = self.scaler.fit_transform(post_features)\n",
    "        x1 = np.concatenate((post_features, post_embeddings), axis=1)\n",
    "\n",
    "        scaler = RobustScaler()\n",
    "        reply_features = self.df_replies[[\"reply_followers\", \"reply_no_verified\", \"reply_verified\",\"time_diff\"]]\n",
    "        reply_features[['reply_followers','time_diff']] = scaler.fit_transform(reply_features[['reply_followers','time_diff']])\n",
    "\n",
    "        reply_embeddings = np.array(self.df_replies['reply_embeddings_avg'].tolist())\n",
    "        #reply_features = self.scaler.transform(reply_features)\n",
    "        x2 = np.concatenate((reply_features, reply_embeddings), axis=1)\n",
    "\n",
    "        return x1, x2\n",
    "\n",
    "    def create_heterodata(self, x1, x2):\n",
    "        y = self.df_posts['rumour'].to_numpy()\n",
    "        edge_index = self.df_replies[[\"id\", \"reply_user_id\"]].values.transpose()\n",
    "\n",
    "        num_rows = x1.shape[0]\n",
    "        indices = np.arange(num_rows)\n",
    "        np.random.shuffle(indices)\n",
    "        train_end = int(0.70 * num_rows)\n",
    "        val_end = train_end + int(0.15 * num_rows)\n",
    "        train_indices, val_indices, test_indices = indices[:train_end], indices[train_end:val_end], indices[val_end:]\n",
    "\n",
    "        train_mask = np.zeros(num_rows, dtype=bool)\n",
    "        val_mask = np.zeros(num_rows, dtype=bool)\n",
    "        test_mask = np.zeros(num_rows, dtype=bool)\n",
    "        train_mask[train_indices], val_mask[val_indices], test_mask[test_indices] = True, True, True\n",
    "\n",
    "        data = HeteroData()\n",
    "        data['id'].x = torch.tensor(x1, dtype=torch.float32)\n",
    "        data['id'].y =  torch.from_numpy(y)\n",
    "        data['id'].train_mask = torch.tensor(train_mask)\n",
    "        data['id'].val_mask = torch.tensor(val_mask) \n",
    "        data['id'].test_mask = torch.tensor(test_mask)\n",
    "        data['reply_user_id'].x = torch.tensor(x2, dtype=torch.float32)\n",
    "        data['id', 'retweet', 'reply_user_id'].edge_index = torch.from_numpy(edge_index.reshape(2,len(x2)))\n",
    "        data = T.ToUndirected()(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def process(self):\n",
    "        self.load_data()\n",
    "        self.process_data()\n",
    "        x1, x2 = self.create_features()\n",
    "        return self.create_heterodata(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9325a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroDataProcessor:\n",
    "    def __init__(self, file_path_replies, file_path_posts, time_cut=15):\n",
    "        self.file_path_replies = file_path_replies\n",
    "        self.file_path_posts = file_path_posts\n",
    "        self.time_cut = time_cut\n",
    "        self.df_replies = None\n",
    "        self.df_posts = None\n",
    "        self.post_map = None\n",
    "        self.reply_user_map = None\n",
    "\n",
    "    def load_data(self):\n",
    "        self.df_replies = pd.read_pickle(self.file_path_replies)\n",
    "        self.df_posts = pd.read_pickle(self.file_path_posts)\n",
    "\n",
    "    def process_data(self):\n",
    "        post_features = ['followers', 'favorite_count', 'retweet_count', 'verified', 'rumour', 'id', 'embeddings_avg']\n",
    "        reply_features = ['reply_followers', 'reply_user_id', 'reply_verified', 'time_diff', 'reply_embeddings_avg', 'id']\n",
    "\n",
    "        # Filter and group replies\n",
    "        self.df_replies = self.df_replies[reply_features][self.df_replies.time_diff < self.time_cut]\n",
    "        grouped_replies = self.df_replies.groupby(['id']).agg(\n",
    "            replies=('time_diff', 'count'),\n",
    "            first_time_diff=('time_diff', 'first')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Merge posts and replies\n",
    "        self.df_posts = self.df_posts[post_features].merge(grouped_replies, on=\"id\", how=\"left\")\n",
    "        self.df_posts['replies'] = self.df_posts['replies'].fillna(0)\n",
    "        self.df_posts['first_time_diff'] = self.df_posts['first_time_diff'].fillna(0)\n",
    "\n",
    "        # One-hot encoding for verified columns\n",
    "        self.df_posts['verified'] = self.df_posts['verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        self.df_posts = pd.concat([self.df_posts, pd.get_dummies(self.df_posts[\"verified\"], dtype=int)], axis=1)\n",
    "        self.df_posts.drop([\"verified\"], axis=1, inplace=True)\n",
    "        self.df_posts.rename(columns={1: 'verified', 0: 'no_verified'}, inplace=True)\n",
    "\n",
    "        self.df_replies['reply_verified'] = self.df_replies['reply_verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        self.df_replies = pd.concat([self.df_replies, pd.get_dummies(self.df_replies[\"reply_verified\"], dtype=int)], axis=1)\n",
    "        self.df_replies.drop([\"reply_verified\"], axis=1, inplace=True)\n",
    "        self.df_replies.rename(columns={1: 'reply_verified', 0: 'reply_no_verified'}, inplace=True)\n",
    "\n",
    "        # Mapping post ids\n",
    "        self.post_map = {value: i for i, value in enumerate(self.df_posts['id'].unique())}\n",
    "        self.df_replies[\"id\"] = self.df_replies['id'].map(self.post_map).astype(int)\n",
    "\n",
    "        # Mapping reply user ids\n",
    "        self.reply_user_map = {value: i for i, value in enumerate(self.df_replies['reply_user_id'].unique())}\n",
    "        self.df_replies[\"reply_user_id\"] = self.df_replies[\"reply_user_id\"].map(self.reply_user_map)\n",
    "\n",
    "    def create_features(self):\n",
    "        post_features = self.df_posts[[\"followers\", \"favorite_count\", \"retweet_count\", \"no_verified\", \"verified\", \"first_time_diff\"]]\n",
    "\n",
    "\n",
    "        # Initialize the Robust Scaler\n",
    "        scaler = RobustScaler()\n",
    "        \n",
    "        # Assuming data is a DataFrame containing your dataset\n",
    "        scaled_features = scaler.fit_transform(post_features[['followers', 'favorite_count', 'retweet_count', 'first_time_diff']])\n",
    "        \n",
    "        # Convert the scaled features back to a DataFrame\n",
    "        scaled_data = pd.DataFrame(scaled_features, columns=['followers', 'favorite_count', 'retweet_count', 'first_time_diff'])\n",
    "        \n",
    "        # Add the binary features back to the scaled data\n",
    "        scaled_data['no_verified'] = post_features['no_verified']\n",
    "        scaled_data['verified'] = post_features['verified']\n",
    "        post_features = scaled_data\n",
    "\n",
    "        post_embeddings = np.array(self.df_posts['embeddings_avg'].tolist())\n",
    "        #post_features = self.scaler.fit_transform(post_features)\n",
    "        x1 = np.concatenate((post_features, post_embeddings), axis=1)\n",
    "\n",
    "        scaler = RobustScaler()\n",
    "        reply_features = self.df_replies[[\"reply_followers\", \"reply_no_verified\", \"reply_verified\",\"time_diff\"]]\n",
    "        reply_features[['reply_followers','time_diff']] = scaler.fit_transform(reply_features[['reply_followers','time_diff']])\n",
    "\n",
    "        reply_embeddings = np.array(self.df_replies['reply_embeddings_avg'].tolist())\n",
    "        #reply_features = self.scaler.transform(reply_features)\n",
    "        x2 = np.concatenate((reply_features, reply_embeddings), axis=1)\n",
    "\n",
    "        return x1, x2\n",
    "\n",
    "    def create_heterodata(self, x1, x2):\n",
    "        y = self.df_posts['rumour'].to_numpy()\n",
    "        edge_index = self.df_replies[[\"id\", \"reply_user_id\"]].values.transpose()\n",
    "\n",
    "        num_rows = x1.shape[0]\n",
    "        indices = np.arange(num_rows)\n",
    "        np.random.shuffle(indices)\n",
    "        train_end = int(0.70 * num_rows)\n",
    "        val_end = train_end + int(0.15 * num_rows)\n",
    "        train_indices, val_indices, test_indices = indices[:train_end], indices[train_end:val_end], indices[val_end:]\n",
    "\n",
    "        train_mask = np.zeros(num_rows, dtype=bool)\n",
    "        val_mask = np.zeros(num_rows, dtype=bool)\n",
    "        test_mask = np.zeros(num_rows, dtype=bool)\n",
    "        train_mask[train_indices], val_mask[val_indices], test_mask[test_indices] = True, True, True\n",
    "\n",
    "        data = HeteroData()\n",
    "        data['id'].x = torch.tensor(x1, dtype=torch.float32)\n",
    "        data['id'].y =  torch.from_numpy(y)\n",
    "        data['id'].train_mask = torch.tensor(train_mask)\n",
    "        data['id'].val_mask = torch.tensor(val_mask) \n",
    "        data['id'].test_mask = torch.tensor(test_mask)\n",
    "        data['reply_user_id'].x = torch.tensor(x2, dtype=torch.float32)\n",
    "        data['id', 'retweet', 'reply_user_id'].edge_index = torch.from_numpy(edge_index.reshape(2,len(x2)))\n",
    "        data = T.ToUndirected()(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def process(self):\n",
    "        self.load_data()\n",
    "        self.process_data()\n",
    "        x1, x2 = self.create_features()\n",
    "        return self.create_heterodata(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9391a49b-b84d-46dc-8373-d1ab1cdb358c",
   "metadata": {},
   "source": [
    "#### Tests with ML Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ef112",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab31227a-b2c0-47e2-aad4-87f6974c90c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "file_path_replies = r\"/home/azureuser/rumour-detection-pheme/replies_charlie_hebdo.pkl\"\n",
    "file_path_posts = r\"/home/azureuser/rumour-detection-pheme/posts_charlie_hebdo.pkl\"\n",
    "time_cut = 7\n",
    "\n",
    "processor = LoadRumoursDatasetFilterNode(file_path_replies, file_path_posts, time_cut)\n",
    "processor.load_data()\n",
    "processor.process_data()\n",
    "df_final = processor.get_final_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58b9d37a-6426-4f78-a1ec-706617e6f19b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_final\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_final' is not defined"
     ]
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7cfbc8-476c-4634-ada2-88f1a0949532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "#mlflow.set_experiment(\"spyder-experiment\")\n",
    "import mlflow.pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c14c5974-5e2a-4b75-bd19-a81d8ed46957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/09 01:40:35 INFO mlflow.tracking.fluent: Experiment with name 'GAT_test' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/workspaces/rumour-detection-pheme/mlruns/3', creation_time=1723167635045, experiment_id='3', last_update_time=1723167635045, lifecycle_stage='active', name='GAT_test', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"GAT_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9863bea5-b581-4148-9a88-22916271258b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16828/3364976966.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reply_features[['reply_followers','time_diff']] = scaler.fit_transform(reply_features[['reply_followers','time_diff']])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usage\n",
    "file_path_replies = r\"/home/azureuser/rumour-detection-pheme/replies_charlie_hebdo.pkl\"\n",
    "file_path_posts = r\"/home/azureuser/rumour-detection-pheme/posts_charlie_hebdo.pkl\"\n",
    "time_cut =13\n",
    "\n",
    "processor = HeteroDataProcessorFilterNode(file_path_replies, file_path_posts, time_cut)\n",
    "data = processor.process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ff5d4cbf-bdd3-4f53-9416-4aa5b1b06865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  id={\n",
       "    x=[17, 106],\n",
       "    y=[17],\n",
       "    train_mask=[17],\n",
       "    val_mask=[17],\n",
       "    test_mask=[17],\n",
       "  },\n",
       "  reply_user_id={ x=[75, 104] },\n",
       "  (id, retweet, reply_user_id)={ edge_index=[2, 75] },\n",
       "  (reply_user_id, rev_retweet, id)={ edge_index=[2, 75] }\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a63b9204-0fbe-41ce-abf2-16bbb67429b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 | Train Loss: 0.7454 | Train Acc: 74.95% | Val Acc: 76.33%\n",
      "Epoch:  50 | Train Loss: 0.4059 | Train Acc: 81.87% | Val Acc: 80.67%\n",
      "Epoch: 100 | Train Loss: 0.3335 | Train Acc: 86.22% | Val Acc: 84.67%\n",
      "Epoch: 150 | Train Loss: 0.2636 | Train Acc: 89.22% | Val Acc: 85.67%\n",
      "Epoch: 200 | Train Loss: 0.2193 | Train Acc: 92.36% | Val Acc: 85.00%\n",
      "Epoch: 250 | Train Loss: 0.1708 | Train Acc: 94.36% | Val Acc: 85.33%\n",
      "Epoch: 300 | Train Loss: 0.1442 | Train Acc: 95.93% | Val Acc: 85.67%\n",
      "Epoch: 350 | Train Loss: 0.1172 | Train Acc: 96.79% | Val Acc: 85.67%\n",
      "Epoch: 400 | Train Loss: 0.0940 | Train Acc: 97.57% | Val Acc: 85.67%\n",
      "Epoch: 450 | Train Loss: 0.0785 | Train Acc: 97.79% | Val Acc: 86.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/09 01:52:48 WARNING mlflow.utils.requirements_utils: Found torch version (2.3.1+cpu) contains a local version label (+cpu). MLflow logged a pip requirement for this package as 'torch==2.3.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 86.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/09 01:52:52 WARNING mlflow.utils.requirements_utils: Found torch version (2.3.1+cpu) contains a local version label (+cpu). MLflow logged a pip requirement for this package as 'torch==2.3.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/08/09 01:52:52 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, to_hetero\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv((-1, -1), dim_h, add_self_loops=False)\n",
    "        self.conv2 = GATConv(dim_h, dim_h, add_self_loops=False)  # Added second GATConv layer\n",
    "        self.linear = nn.Linear(dim_h, dim_out)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index).relu()\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h, edge_index).relu()  # Pass through the second GATConv layer\n",
    "        h = self.dropout(h)\n",
    "        h = self.linear(h)\n",
    "        return h\n",
    "\n",
    "model = GAT(dim_h=64, dim_out=2)\n",
    "model = to_hetero(model, data.metadata(), aggr='sum')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data, model = data.to(device), model.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(mask):\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict)['id'].argmax(dim=-1)\n",
    "    acc = (pred[mask] == data['id'].y[mask]).sum() / mask.sum()\n",
    "    return float(acc)\n",
    "    \n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "    \n",
    "with mlflow.start_run():\n",
    "\n",
    "    for epoch in range(500):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x_dict, data.edge_index_dict)['id']\n",
    "        mask = data['id'].train_mask\n",
    "        loss = F.cross_entropy(out[mask], data['id'].y[mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if epoch % 50 == 0:\n",
    "            train_acc = test(data['id'].train_mask)\n",
    "            val_acc = test(data['id'].val_mask)\n",
    "            print(f'Epoch: {epoch:>3} | Train Loss: {loss:.4f} | Train Acc: {train_acc*100:.2f}% | Val Acc: {val_acc*100:.2f}%')\n",
    "    \n",
    "         # Log metrics\n",
    "        mlflow.log_metric(\"train_loss\", loss.item(), step=epoch)\n",
    "        mlflow.log_metric(\"train_acc\", train_acc, step=epoch)\n",
    "        mlflow.log_metric(\"val_acc\", val_acc, step=epoch)\n",
    "    \n",
    "    test_acc = test(data['id'].test_mask)\n",
    "    print(f'Test accuracy: {test_acc*100:.2f}%')\n",
    "    \n",
    "    mlflow.log_metric(\"test_acc\", test_acc)\n",
    "    \n",
    "    mlflow.log_param(\"dim_h\", 64)\n",
    "    mlflow.log_param(\"dim_out\", 2)\n",
    "    mlflow.log_param(\"learning_rate\", 0.001)\n",
    "    mlflow.log_param(\"epochs\", 500)\n",
    "    \n",
    "    mlflow.pytorch.log_model(model, \"GAT_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4546081b-0d7b-41d9-af6d-f24956ec73e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8090855457227139"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mask = data['id'].test_mask\n",
    "pred = model(data.x_dict, data.edge_index_dict)['id'].argmax(dim=-1)\n",
    "true_labels = data['id'].y[test_mask]\n",
    "pred_labels = pred[test_mask]\n",
    "precision_score(true_labels, pred_labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87211ef8-9152-47e7-b737-3b3e10e87d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8147680845950493"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(true_labels, pred_labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a86981-3934-4fce-9966-9f5cbb199fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103cefdc-408b-4dcf-a898-9432d7157f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa98247-0c57-4df0-9a6c-326ca4c7ee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd996e-c0e5-4c1e-8aa6-eb17aa7bc763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usage\n",
    "file_path_replies = r\"/home/azureuser/rumour-detection-pheme/replies_charlie_hebdo.pkl\"\n",
    "file_path_posts = r\"/home/azureuser/rumour-detection-pheme/posts_charlie_hebdo.pkl\"\n",
    "time_cut =180\n",
    "\n",
    "df_replies = pd.read_pickle(file_path_replies)\n",
    "df_posts = pd.read_pickle(file_path_posts)\n",
    "post_features = ['followers', 'favorite_count', 'retweet_count', 'verified', 'rumour', 'id', 'embeddings_avg']\n",
    "reply_features = ['reply_followers', 'reply_user_id', 'reply_verified', 'time_diff', 'reply_embeddings_avg', 'id']\n",
    "# Filter and group replies\n",
    "df_replies['min_since_fst_post'] = round((df_replies['time'] - df_replies['time'].min())\\\n",
    ".dt.total_seconds() / 60,2)\n",
    "\n",
    "grouped_replies = df_replies.groupby(['id']).agg(\n",
    "    replies=('time_diff', 'count'),\n",
    "    first_time_diff=('time_diff', 'first')\n",
    ").reset_index()\n",
    "\n",
    "# Merge posts and replies\n",
    "df_posts = df_posts[post_features].merge(grouped_replies, on=\"id\", how=\"inner\")\n",
    "df_posts['replies'] = df_posts['replies'].fillna(0)\n",
    "df_posts['first_time_diff'] = df_posts['first_time_diff'].fillna(0)\n",
    "\n",
    "# One-hot encoding for verified columns\n",
    "df_posts['verified'] = df_posts['verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "df_posts = pd.concat([df_posts, pd.get_dummies(df_posts[\"verified\"], dtype=int)], axis=1)\n",
    "df_posts.drop([\"verified\"], axis=1, inplace=True)\n",
    "df_posts.rename(columns={1: 'verified', 0: 'no_verified'}, inplace=True)\n",
    "\n",
    "df_replies['reply_verified'] = df_replies['reply_verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "df_replies = pd.concat([df_replies, pd.get_dummies(df_replies[\"reply_verified\"], dtype=int)], axis=1)\n",
    "df_replies.drop([\"reply_verified\"], axis=1, inplace=True)\n",
    "df_replies.rename(columns={1: 'reply_verified', 0: 'reply_no_verified'}, inplace=True)\n",
    "\n",
    "\n",
    "train, not_train = train_test_split(df_posts, test_size=0.3, random_state=42, stratify=df_posts['rumour'])\n",
    "\n",
    "# Then, split train + validation into train and validation\n",
    "val, test = train_test_split(not_train, test_size=0.5, random_state=42, stratify=not_train['rumour'])\n",
    "\n",
    "post_features = train[[\"followers\", \"favorite_count\", \"retweet_count\", \"no_verified\", \"verified\", \"first_time_diff\"]]\n",
    "\n",
    "\n",
    "# Initialize the Robust Scaler\n",
    "scaler_posts = RobustScaler()\n",
    "\n",
    "# Assuming data is a DataFrame containing your dataset\n",
    "scaled_features = scaler_posts.fit_transform(post_features[['followers', 'favorite_count', 'retweet_count', 'first_time_diff']])\n",
    "\n",
    "# Convert the scaled features back to a DataFrame\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=['followers', 'favorite_count', 'retweet_count', 'first_time_diff'])\n",
    "\n",
    "# Add the binary features back to the scaled data\n",
    "scaled_data['no_verified'] = np.array(train['no_verified'])\n",
    "scaled_data['verified'] = np.array(train['verified'])\n",
    "post_features = scaled_data\n",
    "\n",
    "post_embeddings = np.array(train['embeddings_avg'].tolist())\n",
    "#post_features = scaler.fit_transform(post_features)\n",
    "x1 = np.concatenate((post_features, post_embeddings), axis=1)\n",
    "\n",
    "scaler_replies = RobustScaler()\n",
    "reply_features =  df_replies[df_replies.id.isin(np.array(train.id))][[\"reply_followers\", \"reply_no_verified\",\"reply_verified\",\"time_diff\"]]\n",
    "reply_features[['reply_followers','time_diff']] = scaler_replies.fit_transform(reply_features[['reply_followers','time_diff']])\n",
    "\n",
    "reply_embeddings = np.array(df_replies[df_replies.id.isin(np.array(train.id))]['reply_embeddings_avg'].tolist())\n",
    "#reply_features = scaler.transform(reply_features)\n",
    "x2 = np.concatenate((reply_features, reply_embeddings), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "test_val_df_replies = pd.read_pickle(file_path_replies)\n",
    "test_val_df_posts = pd.read_pickle(file_path_posts)\n",
    "test_val_df_posts=test_val_df_posts[~test_val_df_posts.id.isin(train.id)]\n",
    "test_val_df_replies=test_val_df_replies[~test_val_df_replies.id.isin(train.id)]\n",
    "\n",
    "post_features = ['followers', 'favorite_count', 'retweet_count', 'verified', 'rumour', 'id', 'embeddings_avg']\n",
    "test_val_reply_features = ['reply_followers', 'reply_user_id', 'reply_verified', 'time_diff', 'reply_embeddings_avg', 'id']\n",
    "\n",
    "test_val_df_replies['min_since_fst_post'] = round((test_val_df_replies['time'] - test_val_df_replies['time'].min())\\\n",
    ".dt.total_seconds() / 60,2)\n",
    "\n",
    "test_val_df_replies = test_val_df_replies[test_val_reply_features][(test_val_df_replies.time_diff <= time_cut)&\\\n",
    "   (test_val_df_replies.min_since_fst_post <= time_cut)]\n",
    "grouped_replies = test_val_df_replies.groupby(['id']).agg(\n",
    "    replies=('time_diff', 'count'),\n",
    "    first_time_diff=('time_diff', 'first')\n",
    ").reset_index()\n",
    "\n",
    "test_val_df_posts = test_val_df_posts[post_features].merge(grouped_replies, on=\"id\", how=\"inner\")\n",
    "test_val_df_posts['replies'] = test_val_df_posts['replies'].fillna(0)\n",
    "test_val_df_posts['first_time_diff'] = test_val_df_posts['first_time_diff'].fillna(0)\n",
    "\n",
    "# One-hot encoding for verified columns\n",
    "test_val_df_posts['verified'] = test_val_df_posts['verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "test_val_df_posts = pd.concat([test_val_df_posts, pd.get_dummies(test_val_df_posts[\"verified\"], dtype=int)], axis=1)\n",
    "test_val_df_posts.drop([\"verified\"], axis=1, inplace=True)\n",
    "test_val_df_posts.rename(columns={1: 'verified', 0: 'no_verified'}, inplace=True)\n",
    "\n",
    "test_val_df_replies['reply_verified'] = test_val_df_replies['reply_verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "test_val_df_replies = pd.concat([test_val_df_replies, pd.get_dummies(test_val_df_replies[\"reply_verified\"], dtype=int)], axis=1)\n",
    "test_val_df_replies.drop([\"reply_verified\"], axis=1, inplace=True)\n",
    "test_val_df_replies.rename(columns={1: 'reply_verified', 0: 'reply_no_verified'}, inplace=True)\n",
    "\n",
    "test_val_df_posts = test_val_df_posts.merge(pd.concat([val,test])[['id']].reset_index(),on='id',how='left')\n",
    "test_val_df_posts.set_index('index',drop=True,inplace=True)\n",
    "\n",
    "post_features = test_val_df_posts[[\"followers\", \"favorite_count\", \"retweet_count\", \"no_verified\", \"verified\", \"first_time_diff\"]]\n",
    "\n",
    "\n",
    "scaled_features = scaler_posts.transform(post_features[['followers', 'favorite_count', 'retweet_count', 'first_time_diff']])\n",
    "\n",
    "# Convert the scaled features back to a DataFrame\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=['followers', 'favorite_count', 'retweet_count', 'first_time_diff'])\n",
    "\n",
    "# Add the binary features back to the scaled data\n",
    "scaled_data['no_verified'] = np.array(post_features['no_verified'])\n",
    "scaled_data['verified'] = np.array(post_features['verified'])\n",
    "post_features = scaled_data\n",
    "\n",
    "post_embeddings = np.array(test_val_df_posts['embeddings_avg'].tolist())\n",
    "#post_features = scaler.fit_transform(post_features)\n",
    "x3 = np.concatenate((post_features, post_embeddings), axis=1)\n",
    "\n",
    "test_val_reply_features =  test_val_df_replies[[\"reply_followers\", \"reply_no_verified\",\"reply_verified\",\"time_diff\"]]\n",
    "test_val_reply_features[['reply_followers','time_diff']] = scaler_replies.transform(test_val_reply_features[['reply_followers','time_diff']])\n",
    "\n",
    "test_val_reply_embeddings = np.array(test_val_df_replies['reply_embeddings_avg'].tolist())\n",
    "x4 = np.concatenate((test_val_reply_features, test_val_reply_embeddings), axis=1)\n",
    "\n",
    "\n",
    "# Mapping post ids\n",
    "post_map = {value: i for i, value in enumerate(pd.concat([train[['id']],test_val_df_posts[['id']]])['id'].unique())}\n",
    "df_replies_edges = pd.concat([df_replies[df_replies.id.isin(np.array(train.id))][[\"id\", \"reply_user_id\"]],\\\n",
    "                        test_val_df_replies[[\"id\", \"reply_user_id\"]]])\n",
    "\n",
    "df_replies_edges[\"id\"] = df_replies_edges['id'].map(post_map).astype(int)\n",
    "\n",
    "# Mapping reply user ids\n",
    "reply_user_map = {value: i for i, value in enumerate(df_replies_edges['reply_user_id'].unique())}\n",
    "df_replies_edges[\"reply_user_id\"] = df_replies_edges[\"reply_user_id\"].map(reply_user_map)\n",
    "\n",
    "\n",
    " y = pd.concat([train['rumour'],test_val_df_posts['rumour']]).to_numpy()\n",
    "edge_index = df_replies_edges.values.transpose()\n",
    "x = np.concatenate((x1,x3))\n",
    "x_reply = np.concatenate((x2,x4))\n",
    "\n",
    "num_rows = x.shape[0]\n",
    "train_mask = np.zeros(num_rows, dtype=bool)\n",
    "val_mask = np.zeros(num_rows, dtype=bool)\n",
    "test_mask = np.zeros(num_rows, dtype=bool)\n",
    "train_mask[:-x3.shape[0]]=True\n",
    "val_mask[-x3.shape[0]:-int(x3.shape[0]/2)]=True\n",
    "test_mask[-int(x3.shape[0]/2):]=True\n",
    "\n",
    "data = HeteroData()\n",
    "data['id'].x = torch.tensor(x, dtype=torch.float32)\n",
    "data['id'].y =  torch.from_numpy(y)\n",
    "data['id'].train_mask = torch.tensor(train_mask)\n",
    "data['id'].val_mask = torch.tensor(val_mask) \n",
    "data['id'].test_mask = torch.tensor(test_mask)\n",
    "data['reply_user_id'].x = torch.tensor(x_reply, dtype=torch.float32)\n",
    "data['id', 'retweet', 'reply_user_id'].edge_index = torch.from_numpy(edge_index.reshape(2,len(x_reply)))\n",
    "data = T.ToUndirected()(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mastering",
   "language": "python",
   "name": "mastering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
