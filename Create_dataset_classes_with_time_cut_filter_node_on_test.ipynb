{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbee25c8-853d-412a-8677-fdd4a9eb0cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "import pickle\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, to_hetero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf21648f-ad1a-4e0f-9261-32816face4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usage\n",
    "file_path_replies = r\"/home/azureuser/rumour-detection-pheme/replies_charlie_hebdo.pkl\"\n",
    "file_path_posts = r\"/home/azureuser/rumour-detection-pheme/posts_charlie_hebdo.pkl\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a34ddbd4-c12d-4de9-9650-b4b044d561ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HeteroDataProcessorFilterNodeonTest:\n",
    "    def __init__(self, file_path_replies, file_path_posts, time_cut=15):\n",
    "        self.file_path_replies = file_path_replies\n",
    "        self.file_path_posts = file_path_posts\n",
    "        self.time_cut = time_cut\n",
    "        self.df_replies = None\n",
    "        self.df_posts = None\n",
    "\n",
    "    def load_data(self):\n",
    "        self.df_replies = pd.read_pickle(self.file_path_replies)\n",
    "        self.df_posts = pd.read_pickle(self.file_path_posts)\n",
    "\n",
    "    def process_data(self):\n",
    "        # Define post and reply features\n",
    "        post_features = ['followers', 'favorite_count', 'retweet_count', 'verified', 'rumour', 'id', 'embeddings_avg']\n",
    "        reply_features = ['reply_followers', 'reply_user_id', 'reply_verified', 'time_diff', 'reply_embeddings_avg', 'id']\n",
    "\n",
    "        # Filter and group replies\n",
    "        self.df_replies['min_since_fst_post'] = round(\n",
    "            (self.df_replies['time'] - self.df_replies['time'].min()).dt.total_seconds() / 60, 2)\n",
    "        grouped_replies = self.df_replies.groupby(['id']).agg(\n",
    "            replies=('time_diff', 'count'),\n",
    "            first_time_diff=('time_diff', 'first')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Merge posts and replies\n",
    "        self.df_posts = self.df_posts[post_features].merge(grouped_replies, on=\"id\", how=\"inner\")\n",
    "        self.df_posts['replies'] = self.df_posts['replies'].fillna(0)\n",
    "        self.df_posts['first_time_diff'] = self.df_posts['first_time_diff'].fillna(0)\n",
    "\n",
    "        # One-hot encoding for verified columns\n",
    "        self.df_posts['verified'] = self.df_posts['verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        self.df_posts = pd.concat([self.df_posts, pd.get_dummies(self.df_posts[\"verified\"], dtype=int)], axis=1)\n",
    "        self.df_posts.drop([\"verified\"], axis=1, inplace=True)\n",
    "        self.df_posts.rename(columns={1: 'verified', 0: 'no_verified'}, inplace=True)\n",
    "\n",
    "        self.df_replies['reply_verified'] = self.df_replies['reply_verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        self.df_replies = pd.concat([self.df_replies, pd.get_dummies(self.df_replies[\"reply_verified\"], dtype=int)], axis=1)\n",
    "        self.df_replies.drop([\"reply_verified\"], axis=1, inplace=True)\n",
    "        self.df_replies.rename(columns={1: 'reply_verified', 0: 'reply_no_verified'}, inplace=True)\n",
    "\n",
    "        # Train/test split\n",
    "        train, not_train = train_test_split(self.df_posts, test_size=0.3, random_state=42, stratify=self.df_posts['rumour'])\n",
    "        val, test = train_test_split(not_train, test_size=0.5, random_state=42, stratify=not_train['rumour'])\n",
    "\n",
    "        # Post features processing\n",
    "        post_features = train[[\"followers\", \"favorite_count\", \"retweet_count\", \"no_verified\", \"verified\", \"first_time_diff\"]]\n",
    "        scaler_posts = RobustScaler()\n",
    "        scaled_features =scaler_posts.fit_transform(post_features[['followers', 'favorite_count', 'retweet_count', 'first_time_diff']])\n",
    "        scaled_data = pd.DataFrame(scaled_features, columns=['followers', 'favorite_count', 'retweet_count', 'first_time_diff'])\n",
    "        scaled_data['no_verified'] = np.array(train['no_verified'])\n",
    "        scaled_data['verified'] = np.array(train['verified'])\n",
    "        post_features = scaled_data\n",
    "        post_embeddings = np.array(train['embeddings_avg'].tolist())\n",
    "        x1 = np.concatenate((post_features, post_embeddings), axis=1)\n",
    "\n",
    "        # Reply features processing\n",
    "        scaler_replies = RobustScaler()\n",
    "        reply_features = self.df_replies[self.df_replies.id.isin(np.array(train.id))][[\"reply_followers\", \"reply_no_verified\", \"reply_verified\", \"time_diff\"]]\n",
    "        reply_features[['reply_followers', 'time_diff']] = scaler_replies.fit_transform(reply_features[['reply_followers', 'time_diff']])\n",
    "        reply_embeddings = np.array(self.df_replies[self.df_replies.id.isin(np.array(train.id))]['reply_embeddings_avg'].tolist())\n",
    "        x2 = np.concatenate((reply_features, reply_embeddings), axis=1)\n",
    "\n",
    "        # Test/validation data preparation\n",
    "        test_val_df_replies = pd.read_pickle(self.file_path_replies)\n",
    "        test_val_df_posts = pd.read_pickle(self.file_path_posts)\n",
    "        test_val_df_posts = test_val_df_posts[~test_val_df_posts.id.isin(train.id)]\n",
    "        test_val_df_replies = test_val_df_replies[~test_val_df_replies.id.isin(train.id)]\n",
    "\n",
    "        post_features = ['followers', 'favorite_count', 'retweet_count', 'verified', 'rumour', 'id', 'embeddings_avg']\n",
    "        test_val_reply_features = ['reply_followers', 'reply_user_id', 'reply_verified', 'time_diff', 'reply_embeddings_avg', 'id']\n",
    "        \n",
    "        test_val_df_replies['min_since_fst_post'] = round((test_val_df_replies['time'] - test_val_df_replies['time'].min())\\\n",
    "        .dt.total_seconds() / 60,2)\n",
    "        \n",
    "        test_val_df_replies = test_val_df_replies[test_val_reply_features][(test_val_df_replies.time_diff <= time_cut)&\\\n",
    "           (test_val_df_replies.min_since_fst_post <= time_cut)]\n",
    "        grouped_replies = test_val_df_replies.groupby(['id']).agg(\n",
    "            replies=('time_diff', 'count'),\n",
    "            first_time_diff=('time_diff', 'first')\n",
    "        ).reset_index()\n",
    "        \n",
    "        test_val_df_posts = test_val_df_posts[post_features].merge(grouped_replies, on=\"id\", how=\"inner\")\n",
    "        test_val_df_posts['replies'] = test_val_df_posts['replies'].fillna(0)\n",
    "        test_val_df_posts['first_time_diff'] = test_val_df_posts['first_time_diff'].fillna(0)\n",
    "        \n",
    "        # One-hot encoding for verified columns\n",
    "        test_val_df_posts['verified'] = test_val_df_posts['verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        test_val_df_posts = pd.concat([test_val_df_posts, pd.get_dummies(test_val_df_posts[\"verified\"], dtype=int)], axis=1)\n",
    "        test_val_df_posts.drop([\"verified\"], axis=1, inplace=True)\n",
    "        test_val_df_posts.rename(columns={1: 'verified', 0: 'no_verified'}, inplace=True)\n",
    "        \n",
    "        test_val_df_replies['reply_verified'] = test_val_df_replies['reply_verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        test_val_df_replies = pd.concat([test_val_df_replies, pd.get_dummies(test_val_df_replies[\"reply_verified\"], dtype=int)], axis=1)\n",
    "        test_val_df_replies.drop([\"reply_verified\"], axis=1, inplace=True)\n",
    "        test_val_df_replies.rename(columns={1: 'reply_verified', 0: 'reply_no_verified'}, inplace=True)\n",
    "        \n",
    "        test_val_df_posts = test_val_df_posts.merge(pd.concat([val,test])[['id']].reset_index(),on='id',how='left')\n",
    "        test_val_df_posts.set_index('index',drop=True,inplace=True)\n",
    "        \n",
    "        post_features = test_val_df_posts[[\"followers\", \"favorite_count\", \"retweet_count\", \"no_verified\", \"verified\", \"first_time_diff\"]]\n",
    "        \n",
    "        \n",
    "        scaled_features = scaler_posts.transform(post_features[['followers', 'favorite_count', 'retweet_count', 'first_time_diff']])\n",
    "        \n",
    "        # Convert the scaled features back to a DataFrame\n",
    "        scaled_data = pd.DataFrame(scaled_features, columns=['followers', 'favorite_count', 'retweet_count', 'first_time_diff'])\n",
    "        \n",
    "        # Add the binary features back to the scaled data\n",
    "        scaled_data['no_verified'] = np.array(post_features['no_verified'])\n",
    "        scaled_data['verified'] = np.array(post_features['verified'])\n",
    "        post_features = scaled_data\n",
    "        \n",
    "        post_embeddings = np.array(test_val_df_posts['embeddings_avg'].tolist())\n",
    "        #post_features = scaler.fit_transform(post_features)\n",
    "        x3 = np.concatenate((post_features, post_embeddings), axis=1)\n",
    "        \n",
    "        test_val_reply_features =  test_val_df_replies[[\"reply_followers\", \"reply_no_verified\",\"reply_verified\",\"time_diff\"]]\n",
    "        test_val_reply_features[['reply_followers','time_diff']] = scaler_replies.transform(test_val_reply_features[['reply_followers','time_diff']])\n",
    "        \n",
    "        test_val_reply_embeddings = np.array(test_val_df_replies['reply_embeddings_avg'].tolist())\n",
    "        x4 = np.concatenate((test_val_reply_features, test_val_reply_embeddings), axis=1)\n",
    "        \n",
    "        \n",
    "        # Mapping post ids\n",
    "        post_map = {value: i for i, value in enumerate(pd.concat([train[['id']],test_val_df_posts[['id']]])['id'].unique())}\n",
    "        df_replies_edges = pd.concat([self.df_replies[self.df_replies.id.isin(np.array(train.id))][[\"id\", \"reply_user_id\"]],\\\n",
    "                                test_val_df_replies[[\"id\", \"reply_user_id\"]]])\n",
    "        \n",
    "        df_replies_edges[\"id\"] = df_replies_edges['id'].map(post_map).astype(int)\n",
    "        \n",
    "        # Mapping reply user ids\n",
    "        reply_user_map = {value: i for i, value in enumerate(df_replies_edges['reply_user_id'].unique())}\n",
    "        df_replies_edges[\"reply_user_id\"] = df_replies_edges[\"reply_user_id\"].map(reply_user_map)\n",
    "\n",
    "        return train,test_val_df_posts,df_replies_edges,x1,x2,x3,x4\n",
    "\n",
    "\n",
    "    def create_heterodata(self,train,test_val_df_posts,df_replies_edges,x1,x2,x3,x4):\n",
    "\n",
    "        y = pd.concat([train['rumour'],test_val_df_posts['rumour']]).to_numpy()\n",
    "        edge_index = df_replies_edges.values.transpose()\n",
    "        x = np.concatenate((x1,x3))\n",
    "        x_reply = np.concatenate((x2,x4))\n",
    "            \n",
    "        num_rows = x.shape[0]\n",
    "        train_mask = np.zeros(num_rows, dtype=bool)\n",
    "        val_mask = np.zeros(num_rows, dtype=bool)\n",
    "        test_mask = np.zeros(num_rows, dtype=bool)\n",
    "        train_mask[:-x3.shape[0]]=True\n",
    "        val_mask[-x3.shape[0]:-int(x3.shape[0]/2)]=True\n",
    "        test_mask[-int(x3.shape[0]/2):]=True\n",
    "            \n",
    "        data = HeteroData()\n",
    "        data['id'].x = torch.tensor(x, dtype=torch.float32)\n",
    "        data['id'].y =  torch.from_numpy(y)\n",
    "        data['id'].train_mask = torch.tensor(train_mask)\n",
    "        data['id'].val_mask = torch.tensor(val_mask) \n",
    "        data['id'].test_mask = torch.tensor(test_mask)\n",
    "        data['reply_user_id'].x = torch.tensor(x_reply, dtype=torch.float32)\n",
    "        data['id', 'retweet', 'reply_user_id'].edge_index = torch.from_numpy(edge_index.reshape(2,len(x_reply)))\n",
    "        data = T.ToUndirected()(data)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def process(self):\n",
    "            \n",
    "        self.load_data()\n",
    "        train,test_val_df_posts,df_replies_edges,x1,x2,x3,x4 = self.process_data()\n",
    "        return self.create_heterodata(train,test_val_df_posts,df_replies_edges,x1,x2,x3,x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "102693a4-a5ad-4f77-80b8-f3fbd33ec322",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HeteroDataProcessorFilterNodeonTestV2:\n",
    "    #Different time cuts \n",
    "    def __init__(self, file_path_replies, file_path_posts, time_cut_replies=15,time_cut_posts=15):\n",
    "        self.file_path_replies = file_path_replies\n",
    "        self.file_path_posts = file_path_posts\n",
    "        self.time_cut_replies = time_cut_replies\n",
    "        self.time_cut_posts = time_cut_posts\n",
    "        self.df_replies = None\n",
    "        self.df_posts = None\n",
    "\n",
    "    def load_data(self):\n",
    "        self.df_replies = pd.read_pickle(self.file_path_replies)\n",
    "        self.df_posts = pd.read_pickle(self.file_path_posts)\n",
    "\n",
    "    def process_data(self):\n",
    "        # Define post and reply features\n",
    "        post_features = ['followers', 'favorite_count', 'retweet_count', 'verified', 'rumour', 'id', 'embeddings_avg']\n",
    "        reply_features = ['reply_followers', 'reply_user_id', 'reply_verified', 'time_diff', 'reply_embeddings_avg', 'id']\n",
    "\n",
    "        # Filter and group replies\n",
    "        self.df_replies['min_since_fst_post'] = round(\n",
    "            (self.df_replies['time'] - self.df_replies['time'].min()).dt.total_seconds() / 60, 2)\n",
    "        grouped_replies = self.df_replies.groupby(['id']).agg(\n",
    "            replies=('time_diff', 'count'),\n",
    "            first_time_diff=('time_diff', 'first')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Merge posts and replies\n",
    "        self.df_posts = self.df_posts[post_features].merge(grouped_replies, on=\"id\", how=\"inner\")\n",
    "        self.df_posts['replies'] = self.df_posts['replies'].fillna(0)\n",
    "        self.df_posts['first_time_diff'] = self.df_posts['first_time_diff'].fillna(0)\n",
    "\n",
    "        # One-hot encoding for verified columns\n",
    "        self.df_posts['verified'] = self.df_posts['verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        self.df_posts = pd.concat([self.df_posts, pd.get_dummies(self.df_posts[\"verified\"], dtype=int)], axis=1)\n",
    "        self.df_posts.drop([\"verified\"], axis=1, inplace=True)\n",
    "        self.df_posts.rename(columns={1: 'verified', 0: 'no_verified'}, inplace=True)\n",
    "\n",
    "        self.df_replies['reply_verified'] = self.df_replies['reply_verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        self.df_replies = pd.concat([self.df_replies, pd.get_dummies(self.df_replies[\"reply_verified\"], dtype=int)], axis=1)\n",
    "        self.df_replies.drop([\"reply_verified\"], axis=1, inplace=True)\n",
    "        self.df_replies.rename(columns={1: 'reply_verified', 0: 'reply_no_verified'}, inplace=True)\n",
    "\n",
    "        # Train/test split\n",
    "        train, not_train = train_test_split(self.df_posts, test_size=0.3, random_state=42, stratify=self.df_posts['rumour'])\n",
    "        val, test = train_test_split(not_train, test_size=0.5, random_state=42, stratify=not_train['rumour'])\n",
    "\n",
    "        # Post features processing\n",
    "        post_features = train[[\"followers\", \"favorite_count\", \"retweet_count\", \"no_verified\", \"verified\", \"first_time_diff\"]]\n",
    "        scaler_posts = RobustScaler()\n",
    "        scaled_features =scaler_posts.fit_transform(post_features[['followers', 'favorite_count', 'retweet_count', 'first_time_diff']])\n",
    "        scaled_data = pd.DataFrame(scaled_features, columns=['followers', 'favorite_count', 'retweet_count', 'first_time_diff'])\n",
    "        scaled_data['no_verified'] = np.array(train['no_verified'])\n",
    "        scaled_data['verified'] = np.array(train['verified'])\n",
    "        post_features = scaled_data\n",
    "        post_embeddings = np.array(train['embeddings_avg'].tolist())\n",
    "        x1 = np.concatenate((post_features, post_embeddings), axis=1)\n",
    "\n",
    "        # Reply features processing\n",
    "        scaler_replies = RobustScaler()\n",
    "        reply_features = self.df_replies[self.df_replies.id.isin(np.array(train.id))][[\"reply_followers\", \"reply_no_verified\", \"reply_verified\", \"time_diff\"]]\n",
    "        reply_features[['reply_followers', 'time_diff']] = scaler_replies.fit_transform(reply_features[['reply_followers', 'time_diff']])\n",
    "        reply_embeddings = np.array(self.df_replies[self.df_replies.id.isin(np.array(train.id))]['reply_embeddings_avg'].tolist())\n",
    "        x2 = np.concatenate((reply_features, reply_embeddings), axis=1)\n",
    "\n",
    "        # Test/validation data preparation\n",
    "        test_val_df_replies = pd.read_pickle(self.file_path_replies)\n",
    "        test_val_df_posts = pd.read_pickle(self.file_path_posts)\n",
    "        test_val_df_posts = test_val_df_posts[~test_val_df_posts.id.isin(train.id)]\n",
    "        test_val_df_replies = test_val_df_replies[~test_val_df_replies.id.isin(train.id)]\n",
    "\n",
    "        post_features = ['followers', 'favorite_count', 'retweet_count', 'verified', 'rumour', 'id', 'embeddings_avg']\n",
    "        test_val_reply_features = ['reply_followers', 'reply_user_id', 'reply_verified', 'time_diff', 'reply_embeddings_avg', 'id']\n",
    "        \n",
    "        test_val_df_replies['min_since_fst_post'] = round((test_val_df_replies['time'] - test_val_df_replies['time'].min())\\\n",
    "        .dt.total_seconds() / 60,2)\n",
    "        \n",
    "        test_val_df_replies = test_val_df_replies[test_val_reply_features][(test_val_df_replies.time_diff <= self.time_cut_replies)&\\\n",
    "           (test_val_df_replies.min_since_fst_post <= self.time_cut_posts)]\n",
    "        grouped_replies = test_val_df_replies.groupby(['id']).agg(\n",
    "            replies=('time_diff', 'count'),\n",
    "            first_time_diff=('time_diff', 'first')\n",
    "        ).reset_index()\n",
    "        \n",
    "        test_val_df_posts = test_val_df_posts[post_features].merge(grouped_replies, on=\"id\", how=\"inner\")\n",
    "        test_val_df_posts['replies'] = test_val_df_posts['replies'].fillna(0)\n",
    "        test_val_df_posts['first_time_diff'] = test_val_df_posts['first_time_diff'].fillna(0)\n",
    "        \n",
    "        # One-hot encoding for verified columns\n",
    "        test_val_df_posts['verified'] = test_val_df_posts['verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        test_val_df_posts = pd.concat([test_val_df_posts, pd.get_dummies(test_val_df_posts[\"verified\"], dtype=int)], axis=1)\n",
    "        test_val_df_posts.drop([\"verified\"], axis=1, inplace=True)\n",
    "        test_val_df_posts.rename(columns={1: 'verified', 0: 'no_verified'}, inplace=True)\n",
    "        \n",
    "        test_val_df_replies['reply_verified'] = test_val_df_replies['reply_verified'].astype(str).replace({'True': '1', 'False': '0'}).astype(int)\n",
    "        test_val_df_replies = pd.concat([test_val_df_replies, pd.get_dummies(test_val_df_replies[\"reply_verified\"], dtype=int)], axis=1)\n",
    "        test_val_df_replies.drop([\"reply_verified\"], axis=1, inplace=True)\n",
    "        test_val_df_replies.rename(columns={1: 'reply_verified', 0: 'reply_no_verified'}, inplace=True)\n",
    "        \n",
    "        test_val_df_posts = test_val_df_posts.merge(pd.concat([val,test])[['id']].reset_index(),on='id',how='left')\n",
    "        test_val_df_posts.set_index('index',drop=True,inplace=True)\n",
    "        \n",
    "        post_features = test_val_df_posts[[\"followers\", \"favorite_count\", \"retweet_count\", \"no_verified\", \"verified\", \"first_time_diff\"]]\n",
    "        \n",
    "        \n",
    "        scaled_features = scaler_posts.transform(post_features[['followers', 'favorite_count', 'retweet_count', 'first_time_diff']])\n",
    "        \n",
    "        # Convert the scaled features back to a DataFrame\n",
    "        scaled_data = pd.DataFrame(scaled_features, columns=['followers', 'favorite_count', 'retweet_count', 'first_time_diff'])\n",
    "        \n",
    "        # Add the binary features back to the scaled data\n",
    "        scaled_data['no_verified'] = np.array(post_features['no_verified'])\n",
    "        scaled_data['verified'] = np.array(post_features['verified'])\n",
    "        post_features = scaled_data\n",
    "        \n",
    "        post_embeddings = np.array(test_val_df_posts['embeddings_avg'].tolist())\n",
    "        #post_features = scaler.fit_transform(post_features)\n",
    "        x3 = np.concatenate((post_features, post_embeddings), axis=1)\n",
    "        \n",
    "        test_val_reply_features =  test_val_df_replies[[\"reply_followers\", \"reply_no_verified\",\"reply_verified\",\"time_diff\"]]\n",
    "        test_val_reply_features[['reply_followers','time_diff']] = scaler_replies.transform(test_val_reply_features[['reply_followers','time_diff']])\n",
    "        \n",
    "        test_val_reply_embeddings = np.array(test_val_df_replies['reply_embeddings_avg'].tolist())\n",
    "        x4 = np.concatenate((test_val_reply_features, test_val_reply_embeddings), axis=1)\n",
    "        \n",
    "        \n",
    "        # Mapping post ids\n",
    "        post_map = {value: i for i, value in enumerate(pd.concat([train[['id']],test_val_df_posts[['id']]])['id'].unique())}\n",
    "        df_replies_edges = pd.concat([self.df_replies[self.df_replies.id.isin(np.array(train.id))][[\"id\", \"reply_user_id\"]],\\\n",
    "                                test_val_df_replies[[\"id\", \"reply_user_id\"]]])\n",
    "        \n",
    "        df_replies_edges[\"id\"] = df_replies_edges['id'].map(post_map).astype(int)\n",
    "        \n",
    "        # Mapping reply user ids\n",
    "        reply_user_map = {value: i for i, value in enumerate(df_replies_edges['reply_user_id'].unique())}\n",
    "        df_replies_edges[\"reply_user_id\"] = df_replies_edges[\"reply_user_id\"].map(reply_user_map)\n",
    "\n",
    "        return train,test_val_df_posts,df_replies_edges,x1,x2,x3,x4\n",
    "\n",
    "\n",
    "    def create_heterodata(self,train,test_val_df_posts,df_replies_edges,x1,x2,x3,x4):\n",
    "\n",
    "        y = pd.concat([train['rumour'],test_val_df_posts['rumour']]).to_numpy()\n",
    "        edge_index = df_replies_edges.values.transpose()\n",
    "        x = np.concatenate((x1,x3))\n",
    "        x_reply = np.concatenate((x2,x4))\n",
    "            \n",
    "        num_rows = x.shape[0]\n",
    "        train_mask = np.zeros(num_rows, dtype=bool)\n",
    "        val_mask = np.zeros(num_rows, dtype=bool)\n",
    "        test_mask = np.zeros(num_rows, dtype=bool)\n",
    "        train_mask[:-x3.shape[0]]=True\n",
    "        val_mask[-x3.shape[0]:-int(x3.shape[0]/2)]=True\n",
    "        test_mask[-int(x3.shape[0]/2):]=True\n",
    "            \n",
    "        data = HeteroData()\n",
    "        data['id'].x = torch.tensor(x, dtype=torch.float32)\n",
    "        data['id'].y =  torch.from_numpy(y)\n",
    "        data['id'].train_mask = torch.tensor(train_mask)\n",
    "        data['id'].val_mask = torch.tensor(val_mask) \n",
    "        data['id'].test_mask = torch.tensor(test_mask)\n",
    "        data['reply_user_id'].x = torch.tensor(x_reply, dtype=torch.float32)\n",
    "        data['id', 'retweet', 'reply_user_id'].edge_index = torch.from_numpy(edge_index.reshape(2,len(x_reply)))\n",
    "        data = T.ToUndirected()(data)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def process(self):\n",
    "            \n",
    "        self.load_data()\n",
    "        train,test_val_df_posts,df_replies_edges,x1,x2,x3,x4 = self.process_data()\n",
    "        return self.create_heterodata(train,test_val_df_posts,df_replies_edges,x1,x2,x3,x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37248a22-d94c-4ee9-8cc9-14c7ee47df1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5639/450977190.py:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_val_reply_features[['reply_followers','time_diff']] = scaler_replies.transform(test_val_reply_features[['reply_followers','time_diff']])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usage\n",
    "file_path_replies = r\"/home/azureuser/rumour-detection-pheme/replies_charlie_hebdo.pkl\"\n",
    "file_path_posts = r\"/home/azureuser/rumour-detection-pheme/posts_charlie_hebdo.pkl\"\n",
    "time_cut =12\n",
    "\n",
    "processor = HeteroDataProcessorFilterNodeonTestV2(file_path_replies, file_path_posts, time_cut_replies=10,time_cut_posts=50)\n",
    "data = processor.process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e91c489-6070-4f7a-8903-4d9ae0884310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  id={\n",
       "    x=[1430, 106],\n",
       "    y=[1430],\n",
       "    train_mask=[1430],\n",
       "    val_mask=[1430],\n",
       "    test_mask=[1430],\n",
       "  },\n",
       "  reply_user_id={ x=[13286, 104] },\n",
       "  (id, retweet, reply_user_id)={ edge_index=[2, 13286] },\n",
       "  (reply_user_id, rev_retweet, id)={ edge_index=[2, 13286] }\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6814f02c-6ed4-42bc-9ad8-266a85903212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, to_hetero\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv((-1, -1), dim_h, add_self_loops=False)\n",
    "        self.conv2 = GATConv(dim_h, dim_h, add_self_loops=False)  # Added second GATConv layer\n",
    "        self.linear = nn.Linear(dim_h, dim_out)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index).relu()\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h, edge_index).relu()  # Pass through the second GATConv layer\n",
    "        h = self.dropout(h)\n",
    "        h = self.linear(h)\n",
    "        return h\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(mask):\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict)['id'].argmax(dim=-1)\n",
    "    acc = (pred[mask] == data['id'].y[mask]).sum() / mask.sum()\n",
    "    return float(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca895e1f-f41e-47ee-9852-98e5424ee690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  id={\n",
       "    x=[1495, 106],\n",
       "    y=[1495],\n",
       "    train_mask=[1495],\n",
       "    val_mask=[1495],\n",
       "    test_mask=[1495],\n",
       "  },\n",
       "  reply_user_id={ x=[13922, 104] },\n",
       "  (id, retweet, reply_user_id)={ edge_index=[2, 13922] },\n",
       "  (reply_user_id, rev_retweet, id)={ edge_index=[2, 13922] }\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b9c273cd-7c5d-430e-9546-8222da58c494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 | Train Loss: 0.7825 | Train Acc: 75.30% | Val Acc: 34.04%\n",
      "Epoch:  50 | Train Loss: 0.3850 | Train Acc: 82.30% | Val Acc: 44.68%\n",
      "Epoch: 100 | Train Loss: 0.3148 | Train Acc: 86.72% | Val Acc: 70.21%\n",
      "Epoch: 150 | Train Loss: 0.2533 | Train Acc: 89.01% | Val Acc: 76.60%\n",
      "Epoch: 200 | Train Loss: 0.1992 | Train Acc: 92.79% | Val Acc: 72.34%\n",
      "Epoch: 250 | Train Loss: 0.1495 | Train Acc: 94.22% | Val Acc: 72.34%\n",
      "Epoch: 300 | Train Loss: 0.1134 | Train Acc: 95.65% | Val Acc: 76.60%\n",
      "Epoch: 350 | Train Loss: 0.0967 | Train Acc: 96.93% | Val Acc: 78.72%\n",
      "Epoch: 400 | Train Loss: 0.0835 | Train Acc: 97.50% | Val Acc: 78.72%\n",
      "Epoch: 450 | Train Loss: 0.0607 | Train Acc: 98.07% | Val Acc: 78.72%\n",
      "Test accuracy: 89.36%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GAT(dim_h=64, dim_out=2)\n",
    "model = to_hetero(model, data.metadata(), aggr='sum')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data, model = data.to(device), model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x_dict, data.edge_index_dict)['id']\n",
    "    mask = data['id'].train_mask\n",
    "    loss = F.cross_entropy(out[mask], data['id'].y[mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        train_acc = test(data['id'].train_mask)\n",
    "        val_acc = test(data['id'].val_mask)\n",
    "        print(f'Epoch: {epoch:>3} | Train Loss: {loss:.4f} | Train Acc: {train_acc*100:.2f}% | Val Acc: {val_acc*100:.2f}%')\n",
    "    \n",
    "test_acc = test(data['id'].test_mask)\n",
    "print(f'Test accuracy: {test_acc*100:.2f}%')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fcc8e4ae-757b-4675-a6c4-ba647d94e623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(130)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data['id'].test_mask | data['id'].val_mask).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03d76178-0c11-4d10-86c0-fad7830b998b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7740384615384615"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mask = data['id'].test_mask | data['id'].val_mask\n",
    "pred = model(data.x_dict, data.edge_index_dict)['id'].argmax(dim=-1)\n",
    "true_labels = data['id'].y[test_mask]\n",
    "pred_labels = pred[test_mask]\n",
    "precision_score(true_labels, pred_labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b4edd5c-b8a5-4260-8108-7ff8c4e95516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7314901593252108"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(true_labels, pred_labels, average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mastering",
   "language": "python",
   "name": "mastering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
