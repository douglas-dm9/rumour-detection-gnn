{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ykffzr4H-YMP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import pickle\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9cqGPenz1TVh"
   },
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "import pandas as pd\n",
    "file_path = r\"\"\n",
    "df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'text', 'time', 'location', 'followers', 'user_id',\n",
       "       'favorite_count', 'retweet_count', 'verified', 'rumour', 'reply_id',\n",
       "       'reply_text', 'reply_time', 'reply_location', 'reply_followers',\n",
       "       'reply_user_id', 'reply_verified'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nq86ae2_7l0H"
   },
   "outputs": [],
   "source": [
    "df.time = pd.to_datetime(df.time, format='%a %b %d %H:%M:%S +0000 %Y')\n",
    "df.reply_time = pd.to_datetime(df.reply_time, format='%a %b %d %H:%M:%S +0000 %Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kiU1-cVGPzC"
   },
   "source": [
    "### Time for replies and Number of replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "B0y9FosJ8eU2"
   },
   "outputs": [],
   "source": [
    " df['time_diff']=(df.reply_time - df.time).dt.total_seconds()/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jVIvMkty9eSs"
   },
   "outputs": [],
   "source": [
    "df['reply_number'] = df.groupby('id')['time_diff'].rank(method='dense')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDFus9zKLhBg"
   },
   "source": [
    "#### Number of replies x Retweet counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "AKxtFOLASzUP"
   },
   "outputs": [],
   "source": [
    "df_posts = df[['id','text','followers','favorite_count','retweet_count','verified',\\\n",
    "  'rumour','user_id']].drop_duplicates().merge(df.groupby(['id']).agg(replies=(\\\n",
    "  'time_diff','count'),first_time_diff=('time_diff','first')).reset_index(),\\\n",
    "  on=\"id\",how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH68TYoeMgYm"
   },
   "source": [
    "#### Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-wXsNYeNuLs",
    "outputId": "540fcdf9-482d-4e1e-ccc5-83d87c85df67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# function for cleaning data\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt\n",
    "\n",
    "\n",
    "def clean_text(\n",
    "    string: str,\n",
    "    punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "    stop_words=english_stopwords) -> str:\n",
    "    \"\"\"\n",
    "    A method to clean text\n",
    "    \"\"\"\n",
    "    # Cleaning the urls\n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "\n",
    "    # Cleaning the html elements\n",
    "    string = re.sub(r'<.*?>', '', string)\n",
    "\n",
    "    # Removing the punctuations\n",
    "    for x in string.lower():\n",
    "        if x in punctuations:\n",
    "            string = string.replace(x, \"\")\n",
    "\n",
    "    # Converting the text to lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Removing stop words\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IrvGTGi9M2K2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/tmp/ipykernel_11084/1199893929.py:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  df_posts['clean_text'] = np.vectorize(remove_pattern)(df_posts['text'], \"@[\\w]*\")\n"
     ]
    }
   ],
   "source": [
    "df_posts['clean_text'] = np.vectorize(remove_pattern)(df_posts['text'], \"@[\\w]*\")\n",
    "df_posts['clean_text'] = df_posts['clean_text'].str.replace(\"[^a-zA-Z#]\", \" \").apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HnpNJdMFZ7OW"
   },
   "outputs": [],
   "source": [
    "def embedding_vocab(filepath, word_index,embedding_dim):\n",
    "    vocab_size = len(word_index) + 1\n",
    "\n",
    "\n",
    "    embedding_matrix_vocab = np.zeros((vocab_size,\n",
    "                                       embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                embedding_matrix_vocab[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "    return embedding_matrix_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 01:25:50.985424: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-11 01:25:51.214832: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-11 01:25:51.374849: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733880351.680828   11084 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733880351.769698   11084 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-11 01:25:52.410593: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "#from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df_posts['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUF6Y9hCcv1M",
    "outputId": "99084bf5-0137-44a0-b8b0-7584a21d87c2"
   },
   "outputs": [],
   "source": [
    "#import zipfile\n",
    "\n",
    "#with zipfile.ZipFile('glove.6B.zip', 'r') as zip_ref:\n",
    "#    zip_ref.extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gaWny-EfcIcN"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix_vocab = embedding_vocab(\n",
    "    'glove.6B.100d.txt', tokenizer.word_index,\n",
    "embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "# create the dictionary\n",
    "sequences = tokenizer.texts_to_sequences(df_posts['clean_text'])\n",
    "\n",
    "# Padding sequences if necessary\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Function to get embeddings for a sequence\n",
    "def get_embeddings(sequence, embedding_matrix):\n",
    "    embeddings = []\n",
    "    for idx in sequence:\n",
    "        embeddings.append(embedding_matrix[idx])\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Apply function to get embeddings for each sequence\n",
    "df_posts['embeddings'] = [get_embeddings(seq, embedding_matrix_vocab) for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pycnHmE3zX_o"
   },
   "outputs": [],
   "source": [
    "array_avg = []\n",
    "for i in df_posts.embeddings:\n",
    "  array_avg.append(np.mean(i,axis=0))\n",
    "df_posts['embeddings_avg'] = array_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reply embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/tmp/ipykernel_11084/2579928606.py:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  df['clean_reply_text'] = np.vectorize(remove_pattern)(df['reply_text'], \"@[\\w]*\")\n"
     ]
    }
   ],
   "source": [
    "df['clean_reply_text'] = np.vectorize(remove_pattern)(df['reply_text'], \"@[\\w]*\")\n",
    "df['clean_reply_text'] = df['clean_reply_text'].str.replace(\"[^a-zA-Z#]\", \" \").apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dictionary\n",
    "tokenizer =  keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df['clean_reply_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix_vocab = embedding_vocab(\n",
    "    'glove.6B.100d.txt', tokenizer.word_index,\n",
    "embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "sequences = tokenizer.texts_to_sequences(df['clean_reply_text'])\n",
    "\n",
    "# Padding sequences if necessary\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Apply function to get embeddings for each sequence\n",
    "df['reply_embeddings'] = [get_embeddings(seq, embedding_matrix_vocab) for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/miniconda3/envs/mastering/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/codespace/miniconda3/envs/mastering/lib/python3.12/site-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "array_avg = []\n",
    "for i in df.reply_embeddings:\n",
    "  array_avg.append(np.mean(i,axis=0))\n",
    "df['reply_embeddings_avg'] = array_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['reply_embeddings_avg'] = df['reply_embeddings_avg'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving cleaned csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>followers</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>verified</th>\n",
       "      <th>rumour</th>\n",
       "      <th>user_id</th>\n",
       "      <th>replies</th>\n",
       "      <th>first_time_diff</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>embeddings_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>498254340310966273</td>\n",
       "      <td>Michael Brown is the 17 yr old boy who was sho...</td>\n",
       "      <td>1034</td>\n",
       "      <td>54</td>\n",
       "      <td>293</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>552283163</td>\n",
       "      <td>8</td>\n",
       "      <td>25.733333</td>\n",
       "      <td>michael brown 17 yr old boy shot 10x amp kille...</td>\n",
       "      <td>[[0.296970009803772, 0.13338999450206757, 0.19...</td>\n",
       "      <td>[0.029599157014959736, -0.1258555807565388, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>498254929942028288</td>\n",
       "      <td>17 year old unarmed kid shot ten times by poli...</td>\n",
       "      <td>874288</td>\n",
       "      <td>50</td>\n",
       "      <td>176</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>7214972</td>\n",
       "      <td>13</td>\n",
       "      <td>13.383333</td>\n",
       "      <td>17 year old unarmed kid shot ten times police ...</td>\n",
       "      <td>[[0.44488000869750977, -0.06544700264930725, -...</td>\n",
       "      <td>[0.19377462948775953, 0.10071799257356259, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>498272309535191041</td>\n",
       "      <td>200 cops in riot gear in #ferguson because the...</td>\n",
       "      <td>605</td>\n",
       "      <td>148</td>\n",
       "      <td>486</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>15890403</td>\n",
       "      <td>8</td>\n",
       "      <td>4.766667</td>\n",
       "      <td>200 cops riot gear ferguson prayer circle conc...</td>\n",
       "      <td>[[0.5302799940109253, 0.8232499957084656, -0.0...</td>\n",
       "      <td>[0.1510988038033247, 0.2517209962010384, -0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>498280126254428160</td>\n",
       "      <td>Mike Brown was staying with his grandmother fo...</td>\n",
       "      <td>133561</td>\n",
       "      <td>46</td>\n",
       "      <td>146</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>24165761</td>\n",
       "      <td>4</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>mike brown staying grandmother summer lived co...</td>\n",
       "      <td>[[-0.1477999985218048, 0.3142299950122833, 0.3...</td>\n",
       "      <td>[0.11098087765276432, 0.10974890080979094, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>498293668655423488</td>\n",
       "      <td>Every 28 hours a black male is killed in the U...</td>\n",
       "      <td>133545</td>\n",
       "      <td>428</td>\n",
       "      <td>1755</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>24165761</td>\n",
       "      <td>34</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>every 28 hours black male killed united states...</td>\n",
       "      <td>[[-0.27726998925209045, 0.926609992980957, 0.1...</td>\n",
       "      <td>[0.1422940082848072, 0.13133273070508783, 0.15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>500427783848615936</td>\n",
       "      <td>A mother lost her child. A father lost his son...</td>\n",
       "      <td>834</td>\n",
       "      <td>285</td>\n",
       "      <td>448</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>27376958</td>\n",
       "      <td>18</td>\n",
       "      <td>3.650000</td>\n",
       "      <td>mother lost child father lost son young man lo...</td>\n",
       "      <td>[[0.6058700084686279, 0.02798900008201599, 0.0...</td>\n",
       "      <td>[0.4580300028125445, 0.21627932911117873, 0.34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>500429525558505473</td>\n",
       "      <td>Jesse Jackson leading a small group in prayer ...</td>\n",
       "      <td>7036</td>\n",
       "      <td>115</td>\n",
       "      <td>156</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>14715424</td>\n",
       "      <td>25</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>jesse jackson leading small group prayer scene...</td>\n",
       "      <td>[[-0.039987001568078995, 0.3184700012207031, -...</td>\n",
       "      <td>[0.049042703583836555, 0.1608500976115465, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>500430445830365184</td>\n",
       "      <td>There is a dangerous socioecon war occurring i...</td>\n",
       "      <td>13275</td>\n",
       "      <td>88</td>\n",
       "      <td>152</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1594679162</td>\n",
       "      <td>12</td>\n",
       "      <td>7.766667</td>\n",
       "      <td>dangerous socioecon war occurring us must use ...</td>\n",
       "      <td>[[-0.09617099910974503, -0.3849399983882904, 0...</td>\n",
       "      <td>[-0.05983392263834293, 0.2960853817371222, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>500430673099104256</td>\n",
       "      <td>Wake up. Whenever a black man is killed by pol...</td>\n",
       "      <td>172285</td>\n",
       "      <td>190</td>\n",
       "      <td>124</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>19222164</td>\n",
       "      <td>15</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>wake whenever black man killed police try make...</td>\n",
       "      <td>[[-0.08185099810361862, -0.19044999778270721, ...</td>\n",
       "      <td>[0.021059471865495047, 0.02342099944750468, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>500431179925823489</td>\n",
       "      <td>If you ever doubt the power of your voice, jus...</td>\n",
       "      <td>15525</td>\n",
       "      <td>407</td>\n",
       "      <td>692</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>55113967</td>\n",
       "      <td>17</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>ever doubt power voice look try silence police...</td>\n",
       "      <td>[[0.12039999663829803, 0.7761800289154053, 0.7...</td>\n",
       "      <td>[0.02350966880718867, 0.14279111557536656, 0.4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1010 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                               text  \\\n",
       "0     498254340310966273  Michael Brown is the 17 yr old boy who was sho...   \n",
       "1     498254929942028288  17 year old unarmed kid shot ten times by poli...   \n",
       "2     498272309535191041  200 cops in riot gear in #ferguson because the...   \n",
       "3     498280126254428160  Mike Brown was staying with his grandmother fo...   \n",
       "4     498293668655423488  Every 28 hours a black male is killed in the U...   \n",
       "...                  ...                                                ...   \n",
       "1005  500427783848615936  A mother lost her child. A father lost his son...   \n",
       "1006  500429525558505473  Jesse Jackson leading a small group in prayer ...   \n",
       "1007  500430445830365184  There is a dangerous socioecon war occurring i...   \n",
       "1008  500430673099104256  Wake up. Whenever a black man is killed by pol...   \n",
       "1009  500431179925823489  If you ever doubt the power of your voice, jus...   \n",
       "\n",
       "      followers  favorite_count  retweet_count  verified  rumour     user_id  \\\n",
       "0          1034              54            293     False       1   552283163   \n",
       "1        874288              50            176     False       1     7214972   \n",
       "2           605             148            486     False       1    15890403   \n",
       "3        133561              46            146      True       1    24165761   \n",
       "4        133545             428           1755      True       1    24165761   \n",
       "...         ...             ...            ...       ...     ...         ...   \n",
       "1005        834             285            448     False       0    27376958   \n",
       "1006       7036             115            156      True       0    14715424   \n",
       "1007      13275              88            152     False       0  1594679162   \n",
       "1008     172285             190            124      True       0    19222164   \n",
       "1009      15525             407            692     False       0    55113967   \n",
       "\n",
       "      replies  first_time_diff  \\\n",
       "0           8        25.733333   \n",
       "1          13        13.383333   \n",
       "2           8         4.766667   \n",
       "3           4         3.150000   \n",
       "4          34         0.366667   \n",
       "...       ...              ...   \n",
       "1005       18         3.650000   \n",
       "1006       25         2.833333   \n",
       "1007       12         7.766667   \n",
       "1008       15         0.700000   \n",
       "1009       17         0.700000   \n",
       "\n",
       "                                             clean_text  \\\n",
       "0     michael brown 17 yr old boy shot 10x amp kille...   \n",
       "1     17 year old unarmed kid shot ten times police ...   \n",
       "2     200 cops riot gear ferguson prayer circle conc...   \n",
       "3     mike brown staying grandmother summer lived co...   \n",
       "4     every 28 hours black male killed united states...   \n",
       "...                                                 ...   \n",
       "1005  mother lost child father lost son young man lo...   \n",
       "1006  jesse jackson leading small group prayer scene...   \n",
       "1007  dangerous socioecon war occurring us must use ...   \n",
       "1008  wake whenever black man killed police try make...   \n",
       "1009  ever doubt power voice look try silence police...   \n",
       "\n",
       "                                             embeddings  \\\n",
       "0     [[0.296970009803772, 0.13338999450206757, 0.19...   \n",
       "1     [[0.44488000869750977, -0.06544700264930725, -...   \n",
       "2     [[0.5302799940109253, 0.8232499957084656, -0.0...   \n",
       "3     [[-0.1477999985218048, 0.3142299950122833, 0.3...   \n",
       "4     [[-0.27726998925209045, 0.926609992980957, 0.1...   \n",
       "...                                                 ...   \n",
       "1005  [[0.6058700084686279, 0.02798900008201599, 0.0...   \n",
       "1006  [[-0.039987001568078995, 0.3184700012207031, -...   \n",
       "1007  [[-0.09617099910974503, -0.3849399983882904, 0...   \n",
       "1008  [[-0.08185099810361862, -0.19044999778270721, ...   \n",
       "1009  [[0.12039999663829803, 0.7761800289154053, 0.7...   \n",
       "\n",
       "                                         embeddings_avg  \n",
       "0     [0.029599157014959736, -0.1258555807565388, 0....  \n",
       "1     [0.19377462948775953, 0.10071799257356259, 0.1...  \n",
       "2     [0.1510988038033247, 0.2517209962010384, -0.08...  \n",
       "3     [0.11098087765276432, 0.10974890080979094, 0.0...  \n",
       "4     [0.1422940082848072, 0.13133273070508783, 0.15...  \n",
       "...                                                 ...  \n",
       "1005  [0.4580300028125445, 0.21627932911117873, 0.34...  \n",
       "1006  [0.049042703583836555, 0.1608500976115465, -0....  \n",
       "1007  [-0.05983392263834293, 0.2960853817371222, 0.2...  \n",
       "1008  [0.021059471865495047, 0.02342099944750468, 0....  \n",
       "1009  [0.02350966880718867, 0.14279111557536656, 0.4...  \n",
       "\n",
       "[1010 rows x 13 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('replies_ferguson.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.to_pickle(\"posts_ferguson.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44ZNwTqtKovw"
   },
   "source": [
    "#### Creat Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts = pd.read_pickle(r\"/workspaces/rumour-detection-pheme/posts_charlie_hebdo.pkl\")\n",
    "df_replies = pd.read_pickle(r\"/workspaces/rumour-detection-pheme/replies_charlie_hebdo.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yY1jnj9m3ElI"
   },
   "source": [
    "#### Torch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "xY9kjy-URKKx"
   },
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "df_posts['verified'] = df_posts['verified'].astype('str').str.\\\n",
    "             replace(' ', '').replace('True', '1').replace('False', '0')\\\n",
    "             .astype('int64')\n",
    "\n",
    "df_posts = pd.concat([df_posts, pd.get_dummies(\\\n",
    "                          df_posts[\"verified\"],dtype=int)], axis=1, join='inner')\n",
    "df_posts.drop([\"verified\"], axis=1, inplace=True)\n",
    "df_posts.rename(columns={1:'verified',0:'no_verified'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "fDw9YB9qRKB_"
   },
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "df_replies['reply_verified'] = df_replies['reply_verified'].astype('str').str.\\\n",
    "             replace(' ', '').replace('True', '1').replace('False', '0')\\\n",
    "             .astype('int64')\n",
    "\n",
    "df_replies = pd.concat([df_replies, pd.get_dummies(\\\n",
    "                          df_replies[\"reply_verified\"],dtype=int)], axis=1, join='inner')\n",
    "df_replies.drop([\"reply_verified\"], axis=1, inplace=True)\n",
    "df_replies.rename(columns={1:'reply_verified',0:'reply_no_verified'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "Wa1NAPktPoB8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get unique values from the column\n",
    "unique_values = df_posts['id'].unique()\n",
    "\n",
    "# Create a dictionary to map values to IDs\n",
    "value_to_id = {value: i for i, value in enumerate(unique_values)}\n",
    "\n",
    "# Map the values to IDs using the dictionary\n",
    "#retweets_node_features['index'] = retweets_node_features['reply_user_id'].map(value_to_id)\n",
    "post_map  = value_to_id\n",
    "\n",
    "#Only keep features\n",
    "post_features = df_posts[[\"followers\", \"favorite_count\",\"retweet_count\",\"no_verified\",\"verified\",\\\n",
    "                          \"first_time_diff\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "i3VSO_R8xKgy"
   },
   "outputs": [],
   "source": [
    "embeddings_avg = []\n",
    "for embeddings in df_posts['embeddings_avg']:\n",
    "  #embeddings= i.replace('[','').replace(']','').replace('\\n','').split(' ')\n",
    "  #embeddings =  [float(item) for item in embeddings if item != '']\n",
    "  embeddings_avg.append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize the Robust Scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Assuming data is a DataFrame containing your dataset\n",
    "scaled_features = scaler.fit_transform(post_features[['followers', 'favorite_count', 'retweet_count', 'first_time_diff']])\n",
    "\n",
    "# Convert the scaled features back to a DataFrame\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=['followers', 'favorite_count', 'retweet_count', 'first_time_diff'])\n",
    "\n",
    "# Add the binary features back to the scaled data\n",
    "scaled_data['no_verified'] = post_features['no_verified']\n",
    "scaled_data['verified'] = post_features['verified']\n",
    "post_features = scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QnaMBT0yylBn",
    "outputId": "31b11cf1-5402-4ca1-c567-f1c5abfeea86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2002, 106)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to numpy\n",
    "x1 = np.concatenate((post_features.to_numpy(),np.array(embeddings_avg)),axis=1)\n",
    "#x1 = post_features.to_numpy()\n",
    "x1.shape # [num_movie_nodes x movie_node_feature_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "dNbQMYlKzCUi"
   },
   "outputs": [],
   "source": [
    "# Select node features\n",
    "retweets_node_features = df_replies[[\"reply_followers\", \"reply_no_verified\", \"reply_verified\",\"reply_user_id\",\"time_diff\"]]\n",
    "\n",
    "# Get unique values from the column\n",
    "unique_values = retweets_node_features['reply_user_id'].unique()\n",
    "\n",
    "# Create a dictionary to map values to IDs\n",
    "value_to_id = {value: i for i, value in enumerate(unique_values)}\n",
    "\n",
    "# Map the values to IDs using the dictionary\n",
    "#retweets_node_features['index'] = retweets_node_features['reply_user_id'].map(value_to_id)\n",
    "retweets_id_mapping  = value_to_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Only keep features\n",
    "retweets_node_features = retweets_node_features[[\"reply_followers\", \"reply_no_verified\", \"reply_verified\",\"time_diff\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_avg = []\n",
    "for embeddings in df_replies['reply_embeddings_avg']:\n",
    "  #embeddings= i.replace('[','').replace(']','').replace('\\n','').split(' ')\n",
    "  #embeddings =  [float(item) for item in embeddings if item != '']\n",
    "  embeddings_avg.append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Robust Scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Assuming data is a DataFrame containing your dataset\n",
    "scaled_features = scaler.fit_transform(retweets_node_features[['reply_followers','time_diff']])\n",
    "\n",
    "# Convert the scaled features back to a DataFrame\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=['reply_followers','time_diff'])\n",
    "\n",
    "# Add the binary features back to the scaled data\n",
    "scaled_data['reply_no_verified'] = retweets_node_features['reply_no_verified']\n",
    "scaled_data['reply_verified'] = retweets_node_features['reply_verified']\n",
    "retweets_node_features = scaled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19050, 4)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to numpy\n",
    "#x2 = np.concatenate((retweets_node_features.to_numpy(),np.array(embeddings_avg)),axis=1)\n",
    "x2 = retweets_node_features.to_numpy()\n",
    "x2.shape # [num_movie_nodes x movie_node_feature_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYMUpme80rwi",
    "outputId": "fad85d16-324a-47ee-a7e4-a2afd472dffa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2002,)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract labels\n",
    "labels = df_posts.rumour\n",
    "y = labels.to_numpy()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "rsb7ZYVI04R6"
   },
   "outputs": [],
   "source": [
    "# Map post IDs\n",
    "#post_map = posts_id_mapping.reset_index().set_index(\"id\").to_dict()\n",
    "df_replies[\"id\"] = df_replies['id'].map(post_map).astype(int)\n",
    "# Map user IDs\n",
    "#retweets_map = retweets_id_mapping #retweets_id_mapping.reset_index().set_index(\"reply_user_id\").to_dict()\n",
    "df_replies[\"reply_user_id\"] = df_replies[\"reply_user_id\"].map(retweets_id_mapping)#.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zov-Q9429v1",
    "outputId": "a1a9b871-c403-434b-e5b3-45e4feea6544"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,  2001,  2001,  2001],\n",
       "       [    0,     1,     2, ...,   284,  1810, 14464]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = df_replies[[\"id\", \"reply_user_id\"]].values.transpose()\n",
    "edge_index # [2 x num_edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "I8VTjazz6G7U"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Number of rows\n",
    "num_rows = x1.shape[0]\n",
    "\n",
    "# Desired proportions\n",
    "train_proportion = 0.70\n",
    "val_proportion = 0.15\n",
    "test_proportion = 0.15\n",
    "\n",
    "# Generate a list of indices and shuffle them\n",
    "indices = np.arange(num_rows)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Calculate the split indices\n",
    "train_end = int(train_proportion * num_rows)\n",
    "val_end = train_end + int(val_proportion * num_rows)\n",
    "\n",
    "# Split the indices into train, validation, and test\n",
    "train_indices = indices[:train_end]\n",
    "val_indices = indices[train_end:val_end]\n",
    "test_indices = indices[val_end:]\n",
    "\n",
    "# Create masks\n",
    "train_mask = np.zeros(num_rows, dtype=bool)\n",
    "val_mask = np.zeros(num_rows, dtype=bool)\n",
    "test_mask = np.zeros(num_rows, dtype=bool)\n",
    "\n",
    "# Assign True to the corresponding indices\n",
    "train_mask[train_indices] = True\n",
    "val_mask[val_indices] = True\n",
    "test_mask[test_indices] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XyvzeR7IymQ",
    "outputId": "b26eb871-555d-42fd-caff-ad906a7ccf7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  id={\n",
      "    x=[2002, 106],\n",
      "    y=[2002],\n",
      "    train_mask=[2002],\n",
      "    val_mask=[2002],\n",
      "    test_mask=[2002],\n",
      "  },\n",
      "  reply_user_id={ x=[19050, 4] },\n",
      "  (id, retweet, reply_user_id)={ edge_index=[2, 19050] },\n",
      "  (reply_user_id, rev_retweet, id)={ edge_index=[2, 19050] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the HeteroData object\n",
    "data = HeteroData()\n",
    "\n",
    "# Add node features and labels for the 'paper' node type\n",
    "data['id'].x = torch.tensor(x1,dtype=torch.float32)  # Node features dtype=torch.float32\n",
    "data['id'].y = torch.from_numpy(y)  # Node labels (for example, binary classification)\n",
    "data['id'].train_mask = torch.tensor(train_mask)  # Training mask\n",
    "data['id'].val_mask =torch.tensor(val_mask)  # Validation mask\n",
    "data['id'].test_mask = torch.tensor(test_mask)  # Test mask\n",
    "\n",
    "# Add node features for the 'author' node type\n",
    "data['reply_user_id'].x = torch.tensor(x2,dtype=torch.float32) #torch.float32\n",
    "\n",
    "# Add edges for the (id, retweet, reply_D) relation\n",
    "data['id', 'retweet', 'reply_user_id'].edge_index = torch.from_numpy(edge_index.reshape(2,len(x2)))\n",
    "data = T.ToUndirected()(data)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtOzUtI7frmN",
    "outputId": "cedbbb5a-d3a3-46b2-adea-f6ed97e206cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: tensor(307) torch.Size([1401])\n"
     ]
    }
   ],
   "source": [
    "print(\"train:\",data['id'].y[train_mask].sum(),data['id'].y[train_mask].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0hD33th-gFnO",
    "outputId": "0e0f7739-7d7e-49dd-cf0f-8c8221739a32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: tensor(65) torch.Size([300])\n"
     ]
    }
   ],
   "source": [
    "print(\"val:\", data['id'].y[val_mask].sum(),data['id'].y[val_mask].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5_aX0ZogGHn",
    "outputId": "5f4934da-8242-429e-9a90-fa8f256d2b2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: tensor(75) torch.Size([301])\n"
     ]
    }
   ],
   "source": [
    "print(\"test:\", data['id'].y[test_mask].sum(),data['id'].y[test_mask].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('charlie_hebdo_graph_dataset_node_embeddings_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HeteroData object from the pkl file\n",
    "with open('charlie_hebdo_graph_dataset_node_embeddings_v2.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
